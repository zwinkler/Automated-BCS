{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image as pil_image\n",
    "import imageio\n",
    "import tensorflow\n",
    "import sklearn.metrics\n",
    "import itertools\n",
    "from keras.layers import Input\n",
    "import keras.applications\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Dropout, Conv2D, ReLU, concatenate, MaxPool2D, AvgPool2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_train, ds_info = tfds.load('cifar10', split='train', shuffle_files=True, as_supervised=True, with_info=True)\n",
    "# ds_test = tfds.load('cifar10', split='test', shuffle_files=True, as_supervised=True)\n",
    "# #tfds.show_examples(ds_test, ds_info)\n",
    "\n",
    "# print(ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.uint8), label\n",
    "\n",
    "# ds_train = ds_train.map(\n",
    "#     normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# ds_train = ds_train.cache()\n",
    "# ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "# ds_train = ds_train.batch(128)\n",
    "# ds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# ds_test = ds_test.map(\n",
    "#     normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# ds_test = ds_test.batch(128)\n",
    "# ds_test = ds_test.cache()\n",
    "# ds_test = ds_test.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 476 validated image filenames.\n",
      "Found 2346 validated image filenames.\n",
      "Found 2950 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('train_data.csv',dtype=str)\n",
    "val_df = pd.read_csv('val_data.csv',dtype=str)\n",
    "test_df = pd.read_csv('test_data.csv',dtype=str)\n",
    "\n",
    "train_df[\"class\"] = train_df[\"class\"].astype(str).astype(float)\n",
    "val_df[\"class\"] = val_df[\"class\"].astype(str).astype(float)\n",
    "test_df[\"class\"] = test_df[\"class\"].astype(str).astype(float)\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(fill_mode = \"reflect\", data_format = \"channels_last\")\n",
    "train_generator = train_datagen.flow_from_dataframe(dataframe=train_df,\\\n",
    "                                                    directory='',\\\n",
    "                                                    xcol='filename',y_col='class',\\\n",
    "                                                    target_size=(72,72), color_mode='rgb',\\\n",
    "                                                    batch_size=128, class_mode='raw',\\\n",
    "                                                    shuffle=True) #RETURN TO TRUEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "val_datagen = ImageDataGenerator(fill_mode = \"reflect\", data_format = \"channels_last\")\n",
    "val_generator = val_datagen.flow_from_dataframe(dataframe=val_df,\\\n",
    "                                                directory = '',\\\n",
    "                                                xcol='filename',ycol='class',\\\n",
    "                                                target_size=(72,72), color_mode='rgb',\\\n",
    "                                                batch_size=128, class_mode='raw',\\\n",
    "                                                shuffle=True)\n",
    "test_datagen = ImageDataGenerator(fill_mode = \"reflect\", data_format = \"channels_last\")\n",
    "test_generator = test_datagen.flow_from_dataframe(dataframe=test_df,\\\n",
    "                                                directory = '',\\\n",
    "                                                xcol='filename',ycol='class',\\\n",
    "                                                target_size=(72,72), color_mode='rgb',\\\n",
    "                                                batch_size=128, class_mode='raw',\\\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "# print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "# print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.00001\n",
    "batch_size = 32\n",
    "num_epochs = 2\n",
    "image_size = 72  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 2\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        #layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        #layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "#data_augmentation.layers[0].adapt(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(4, 4))\n",
    "# image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
    "# plt.imshow(image.astype(\"uint8\"))\n",
    "# plt.axis(\"off\")\n",
    "\n",
    "# resized_image = tf.image.resize(\n",
    "#     tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
    "# )\n",
    "# patches = Patches(patch_size)(resized_image)\n",
    "# print(f\"Image size: {image_size} X {image_size}\")\n",
    "# print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "# print(f\"Patches per image: {patches.shape[1]}\")\n",
    "# print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "\n",
    "# n = int(np.sqrt(patches.shape[1]))\n",
    "# plt.figure(figsize=(4, 4))\n",
    "# for i, patch in enumerate(patches[0]):\n",
    "#     ax = plt.subplot(n, n, i + 1)\n",
    "#     patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "#     plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "#     plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5.95, 1: 0.952, 2: 0.43272727272727274, 3: 0.7616, 4: 6.346666666666667}\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " data_augmentation (Sequential)  (None, 72, 72, 3)   0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " patches_1 (Patches)            (None, None, 108)    0           ['data_augmentation[0][0]']      \n",
      "                                                                                                  \n",
      " patch_encoder_1 (PatchEncoder)  (None, 144, 64)     16192       ['patches_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 144, 64)     128         ['patch_encoder_1[0][0]']        \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 144, 64)     66368       ['layer_normalization_5[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 144, 64)      0           ['multi_head_attention_2[0][0]', \n",
      "                                                                  'patch_encoder_1[0][0]']        \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 144, 64)     128         ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 144, 128)     8320        ['layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 144, 128)     0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 144, 64)      8256        ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 144, 64)      0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 144, 64)      0           ['dropout_8[0][0]',              \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 144, 64)     128         ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 144, 64)     66368       ['layer_normalization_7[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 144, 64)      0           ['multi_head_attention_3[0][0]', \n",
      "                                                                  'add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 144, 64)     128         ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 144, 128)     8320        ['layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 144, 128)     0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 144, 64)      8256        ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 144, 64)      0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 144, 64)      0           ['dropout_10[0][0]',             \n",
      "                                                                  'add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 144, 64)     128         ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 9216)         0           ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 9216)         0           ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 2048)         18876416    ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 2048)         0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 1024)         2098176     ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 1024)         0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 5)            5125        ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,162,437\n",
      "Trainable params: 21,162,437\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_vit_classifier()\n",
    "\n",
    "optimizer = tfa.optimizers.AdamW(\n",
    "    learning_rate=learning_rate, weight_decay=weight_decay, epsilon = 1e-7, clipnorm = 1\n",
    "    )\n",
    "\n",
    "step_size_train = np.ceil(train_generator.n/train_generator.batch_size)\n",
    "step_size_val = np.ceil(val_generator.n/val_generator.batch_size)\n",
    "\n",
    "from sklearn.utils import class_weight \n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(train_df['class']), y =train_df['class'])\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(class_weights)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "# history = model.fit(\n",
    "#     ds_train, epochs=1, verbose=1,\\\n",
    "#     validation_data=ds_test,\\\n",
    "#     validation_freq=1\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "4/4 [==============================] - 7s 1s/step - loss: nan - accuracy: 0.0105 - top-5-accuracy: 0.2689 - val_loss: nan - val_accuracy: 0.0000e+00 - val_top-5-accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "4/4 [==============================] - 3s 1s/step - loss: nan - accuracy: 0.0000e+00 - top-5-accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00 - val_top-5-accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\n",
    "        keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "        keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "    ],\n",
    "    )\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator, steps_per_epoch=step_size_train, epochs=2, verbose=1,\\\n",
    "    validation_data=val_generator, validation_steps=step_size_val,\\\n",
    "    validation_freq=1#, class_weight=class_weights #class weights break everything and for some reaosn it thinks there are still 10 classes?\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.trainable = False\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = model.layers[-2].output\n",
    "# pred = Dense(5)(x) #didn't have a softmax?\n",
    "# model_new = Model(inputs = model.input, outputs = pred)\n",
    "# model_new.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step_size_train = np.ceil(train_generator.n/train_generator.batch_size)\n",
    "# step_size_val = np.ceil(val_generator.n/val_generator.batch_size)\n",
    "\n",
    "# from sklearn.utils import class_weight \n",
    "# class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(train_df['class']), y =train_df['class'])\n",
    "# class_weights = dict(enumerate(class_weights))\n",
    "# print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_new.compile(loss='categorical_crossentropy', optimizer=optimizer,\\\n",
    "#               metrics=['categorical_accuracy'])\n",
    "\n",
    "# checkpoint_filepath = \"BCS_CNN\\\\BCS_CNN_{epoch:0d}.hdf5\"\n",
    "# checkpoint = keras.callbacks.ModelCheckpoint('BCS_CNN\\\\BCS_CNN_{epoch:0d}.hdf5', monitor='accuracy', verbose=1, save_best_only=True, mode='max', initial_value_threshold = 0.45)\n",
    "# callbacks_list = [checkpoint]\n",
    "\n",
    "    # checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    #     checkpoint_filepath,\n",
    "    #     monitor=\"val_accuracy\",\n",
    "    #     save_best_only=True,\n",
    "    #     save_weights_only=True,\n",
    "    # )\n",
    "# model_new.compile(\n",
    "#     optimizer=optimizer,\n",
    "#     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), #had from_logits=True\n",
    "#     metrics=[\n",
    "#         keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "#         #keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "#     ],\n",
    "#     )\n",
    "\n",
    "\n",
    "# history = model_new.fit(\n",
    "#     train_generator, steps_per_epoch=step_size_train, epochs=1, verbose=1,\\\n",
    "#     validation_data=val_generator, validation_steps=step_size_val,\\\n",
    "#     validation_freq=1, class_weight=class_weights #class weights break everything and for some reaosn it thinks there are still 10 classes?\n",
    "#     )\n",
    "\n",
    "    #model.load_weights(checkpoint_filepath)\n",
    "    # Y_test_hat=model.predict(test_generator)\n",
    "    # y_test_hat=Y_test_hat.argmax(axis=-1)+2\n",
    "    # y_test = test_df['class']\n",
    "    # y_test = [int(i) for i in y_test]\n",
    "\n",
    "# _, accuracy, top_5_accuracy = model.evaluate(test_generator)\n",
    "# print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "# print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_Xtrain(X_train):\n",
    "    mn = []\n",
    "    mx = []\n",
    "    Xn_train = np.zeros(np.shape(X_train))\n",
    "    X_norm = np.zeros(np.shape(X_train))\n",
    "    for i in range(len(X_train[0, ::])):\n",
    "        mn.append(np.min(X_train[::, i]))\n",
    "        Xn_train[::, i] = X_train[::, i] - mn[i]\n",
    "    for i in range(len(X_train[0, ::])):\n",
    "        mx.append(np.max(Xn_train[::, i]))\n",
    "        if mx[i] == 0:\n",
    "            X_norm[::, i] = 0\n",
    "        if mx[i] != 0:\n",
    "            X_norm[::, i] = Xn_train[::, i]/mx[i]\n",
    "            \n",
    "    return X_norm, mx, mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "The accuracy of SqueezeNet is:  0.16983050847457626\n",
      "dict_keys(['loss', 'accuracy', 'top-5-accuracy', 'val_loss', 'val_accuracy', 'val_top-5-accuracy'])\n",
      "Model precision is:  0.03396610169491525\n",
      "Model f1_score is:  0.05807012460156476\n",
      "Model recall is:  0.2\n",
      "Model MSE is:  5.1084745762711865\n",
      "Confusion Matrix\n",
      "[[ 501    0    0    0    0]\n",
      " [ 204    0    0    0    0]\n",
      " [1485    0    0    0    0]\n",
      " [ 462    0    0    0    0]\n",
      " [ 298    0    0    0    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "Y_test_hat=model.predict(test_generator)\n",
    "y_test_hat=Y_test_hat.argmax(axis=-1)+2\n",
    "\n",
    "print(y_test_hat[:200])\n",
    "\n",
    "\n",
    "y_test = test_df['class']\n",
    "y_test = [int(i) for i in y_test]\n",
    "print(y_test[:200])\n",
    "\n",
    "con_matrix = sklearn.metrics.confusion_matrix(y_test,y_test_hat)\n",
    "acc=np.diag(con_matrix).sum().astype(float)/con_matrix.sum()\n",
    "# acc = tensorflow.keras.metrics.Accuracy()\n",
    "# acc.reset_state()\n",
    "# acc.update_state(y_test, y_test_hat)\n",
    "\n",
    "\n",
    "print('The accuracy of SqueezeNet is: ', acc)\n",
    "\n",
    "min = np.min(con_matrix)\n",
    "max = np.max(con_matrix)\n",
    "temp_mat = con_matrix - min\n",
    "temp_mat = con_matrix/max\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(normalize_Xtrain(temp_mat)[0], cmap='gray')\n",
    "plt.title('Confusion Matrix for SqueezeNet')\n",
    "plt.xticks(list(range(len(['2', '3', '4', '5', '6']))), ['2', '3', '4', '5', '6'])\n",
    "plt.yticks(list(range(len(['2', '3', '4', '5', '6']))), ['2', '3', '4', '5', '6'])\n",
    "plt.ylabel('True Labels')\n",
    "plt.xlabel('Predicted Labels')\n",
    "\n",
    "print(history.history.keys())\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.plot(np.arange(0, len(history.history['categorical_accuracy'])), history.history['categorical_accuracy'])\n",
    "# plt.plot(np.arange(0, len(history.history['val_categorical_accuracy'])), history.history['val_categorical_accuracy'])\n",
    "# plt.legend(('Training Accuracy', 'Validation Accuracy'))\n",
    "\n",
    "#Metrics\n",
    "precision = sklearn.metrics.precision_score(y_test,y_test_hat, average='macro')\n",
    "print('Model precision is: ', precision)\n",
    "\n",
    "f1_score = sklearn.metrics.f1_score(y_test,y_test_hat, average='macro')\n",
    "print('Model f1_score is: ', f1_score)\n",
    "\n",
    "recall = sklearn.metrics.recall_score(y_test,y_test_hat, average='macro')\n",
    "print('Model recall is: ', recall)\n",
    "\n",
    "MSE = sklearn.metrics.mean_squared_error(y_test,y_test_hat)\n",
    "print('Model MSE is: ', MSE)\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(con_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "The accuracy of the network model2 is:  0.23898305084745763\n",
      "dict_keys(['loss', 'accuracy', 'top-5-accuracy', 'val_loss', 'val_accuracy', 'val_top-5-accuracy'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'categorical_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-be45f553ea60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'categorical_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'categorical_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_categorical_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_categorical_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training Accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Validation Accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'categorical_accuracy'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_within_1 = []\n",
    "for i in range(0, len(y_test)):\n",
    "    if ((y_test_hat[i] == y_test[i]+1) or (y_test_hat[i] == y_test[i]-1) or (y_test_hat[i] == y_test[i])):\n",
    "        y_within_1.append(y_test[i])\n",
    "    else:\n",
    "        y_within_1.append(y_test_hat[i])\n",
    "\n",
    "print(y_within_1)\n",
    "print(type(y_within_1[20]))\n",
    "print(type(y_test[0]))\n",
    "\n",
    "con_matrix = sklearn.metrics.confusion_matrix(y_test,y_within_1)\n",
    "acc=np.diag(con_matrix).sum().astype(float)/con_matrix.sum()\n",
    "\n",
    "\n",
    "print('The accuracy of the network model2 is: ', acc)\n",
    "\n",
    "min = np.min(con_matrix)\n",
    "max = np.max(con_matrix)\n",
    "temp_mat = con_matrix - min\n",
    "temp_mat = con_matrix/max\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(normalize_Xtrain(con_matrix)[0], cmap='gray')\n",
    "plt.title('Confusion Matrix for SqueezeNet')\n",
    "plt.xticks(list(range(len(['2', '3', '4', '5', '6']))), ['2', '3', '4', '5', '6'])\n",
    "plt.yticks(list(range(len(['2', '3', '4', '5', '6']))), ['2', '3', '4', '5', '6'])\n",
    "plt.ylabel('True Labels')\n",
    "plt.xlabel('Predicted Labels')\n",
    "\n",
    "\n",
    "print(history.history.keys())\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(np.arange(0, len(history.history['categorical_accuracy'])), history.history['categorical_accuracy'])\n",
    "plt.plot(np.arange(0, len(history.history['val_categorical_accuracy'])), history.history['val_categorical_accuracy'])\n",
    "plt.legend(('Training Accuracy', 'Validation Accuracy'))\n",
    "\n",
    "#Metrics\n",
    "precision = sklearn.metrics.precision_score(y_test,y_within_1, average='macro')\n",
    "print('Model unweighted precision is: ', precision)\n",
    "precision = sklearn.metrics.precision_score(y_test,y_within_1, average='weighted')\n",
    "print('Model weighted precision is: ', precision)\n",
    "print()\n",
    "\n",
    "#print('Avg :', sklearn.metrics.accuracy_score(y_test, y_test_hat, normalize=False))\n",
    "\n",
    "recall = sklearn.metrics.recall_score(y_test,y_within_1, average='macro')\n",
    "print('Model unweighted recall is: ', recall)\n",
    "recall = sklearn.metrics.recall_score(y_test,y_within_1, average='weighted')\n",
    "print('Model weighted recall is: ', recall)\n",
    "print()\n",
    "\n",
    "f1_score = sklearn.metrics.f1_score(y_test,y_within_1, average='macro')\n",
    "print('Model unweighted f1_score is: ', f1_score)\n",
    "f1_score = sklearn.metrics.f1_score(y_test,y_within_1, average='weighted')\n",
    "print('Model weighted f1_score is: ', f1_score)\n",
    "print()\n",
    "\n",
    "MSE = sklearn.metrics.mean_squared_error(y_test,y_test_hat)\n",
    "print('Model MSE is: ', MSE)\n",
    "\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(con_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ML_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "851288773b5edbc54f631fdf579cbc9e2fc2ac5bd43b2d347da3c29f51b44049"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
