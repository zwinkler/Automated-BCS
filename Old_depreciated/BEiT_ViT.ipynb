{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 476/476 [00:00<00:00, 238017.25it/s]\n",
      "Using custom data configuration DGE_training-1a4881cf19cc1a09\n",
      "Reusing dataset imagefolder (C:\\Users\\zacha\\.cache\\huggingface\\datasets\\imagefolder\\DGE_training-1a4881cf19cc1a09\\0.0.0\\0fc50c79b681877cc46b23245a6ef5333d036f48db40d53765a68034bc48faff)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "transformer_name = \"microsoft/beit-base-patch16-224\"\n",
    "\n",
    "\n",
    "train_dataset = load_dataset(\"DGE_training\", data_dir=\"*/\", split=\"train\")#.shuffle()\n",
    "#train_dataset = load_dataset(\"DGE_val\", data_dir=\"*/\", split='test')\n",
    "#train_dataset = load_dataset(\"DGE_test\", data_dir=\"*/\", split='test')\n",
    "#train_dataset = train_dataset.train_test_split(test_size=0.2)\n",
    "#print(train_dataset[\"train\"][0])\n",
    "labels = train_dataset.features[\"label\"].names\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# food = load_dataset(\"food101\", split=\"train[:5000]\")\n",
    "# food = food.train_test_split(test_size=0.2)\n",
    "# food[\"train\"][0]\n",
    "# labels = food[\"train\"].features[\"label\"].names\n",
    "# label2id, id2label = dict(), dict()\n",
    "# for i, label in enumerate(labels):\n",
    "#     label2id[label] = str(i)\n",
    "#     id2label[str(i)] = label\n",
    "# id2label[str(79)]\n",
    "\n",
    "from transformers import AutoFeatureExtractor, BeitFeatureExtractor\n",
    "\n",
    "feature_extractor = BeitFeatureExtractor.from_pretrained(transformer_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.min(train_dataset[\"train\"][:]['label']))\n",
    "# print(np.max(train_dataset[\"train\"][:]['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(food)\n",
    "# print(food['train'])\n",
    "# print(train_dataset)\n",
    "# print(train_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "\n",
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "_transforms = Compose([RandomResizedCrop(feature_extractor.size), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "\n",
    "#food = food.with_transform(transforms)\n",
    "train_dataset = train_dataset.with_transform(transforms)\n",
    "\n",
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BeitForImageClassification(\n",
       "  (beit): BeitModel(\n",
       "    (embeddings): BeitEmbeddings(\n",
       "      (patch_embeddings): BeitPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): BeitEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (relative_position_bias): BeitRelativePositionBias()\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): Identity()\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (relative_position_bias): BeitRelativePositionBias()\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.00909090880304575)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (relative_position_bias): BeitRelativePositionBias()\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.0181818176060915)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (relative_position_bias): BeitRelativePositionBias()\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.027272727340459824)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (relative_position_bias): BeitRelativePositionBias()\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.036363635212183)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (relative_position_bias): BeitRelativePositionBias()\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.045454543083906174)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (relative_position_bias): BeitRelativePositionBias()\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.054545458406209946)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (relative_position_bias): BeitRelativePositionBias()\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.06363636255264282)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (relative_position_bias): BeitRelativePositionBias()\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.0727272778749466)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (relative_position_bias): BeitRelativePositionBias()\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.08181818574666977)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (relative_position_bias): BeitRelativePositionBias()\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.09090909361839294)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (relative_position_bias): BeitRelativePositionBias()\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.10000000149011612)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): Identity()\n",
       "    (pooler): BeitPooler(\n",
       "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer, BeitForImageClassification\n",
    "from datasets import load_metric\n",
    "\n",
    "model = BeitForImageClassification.from_pretrained(\n",
    "    transformer_name,\n",
    "    num_labels=5,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    # id2label=id2label,\n",
    "    # label2id=label2id,\n",
    ")\n",
    "\n",
    "device = 'cuda'\n",
    "model = model.to(device)\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     metrics = [\"accuracy\", \"recall\", \"precision\", \"f1\"] #List of metrics to return\n",
    "#     metric={}\n",
    "#     for met in metrics:\n",
    "#        metric[met] = load_metric(met)\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     metric_res={}\n",
    "#     for met in metrics:\n",
    "#        metric_res[met]=metric[met].compute('macro', predictions=predictions, references=labels)[met]\n",
    "#     return metric_res\n",
    "\n",
    "from datasets import load_metric\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.utils import class_weight\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        train_df = pd.read_csv('train_data.csv',dtype=str)\n",
    "        class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(train_df['class']), y =train_df['class'])\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.cuda.FloatTensor(class_weights))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.95       0.952      0.43272727 0.7616     6.34666667]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_df = pd.read_csv('train_data.csv',dtype=str)\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(train_df['class']), y =train_df['class'])\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# summary(model, (3, 224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "#training_dataset_pt = datasets.ImageFolder(root=\"DGE_training\", transform=transforms)\n",
    "dataset_size = len(train_dataset[:]['label'])\n",
    "train_count = int(dataset_size * 0.7)\n",
    "val_count = dataset_size - train_count\n",
    "train_dataset, valid_dataset = torch.utils.data.dataset.random_split(train_dataset, [train_count, val_count])\n",
    "\n",
    "#print(train_dataset[0])\n",
    "import numpy as np \n",
    "\n",
    "#y_train_indices = train_dataset[\"train\"].indices\n",
    "\n",
    "y_train = train_dataset[:]['label']\n",
    "\n",
    "class_sample_count = np.array(\n",
    "    [len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n",
    "\n",
    "weight = 1. / class_sample_count\n",
    "samples_weight = np.array([weight[t] for t in y_train])\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, sampler=sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 3, 2, 3, 2, 2, 2, 3, 3, 0, 3, 3, 2, 1, 1, 2, 1, 2, 2, 2, 3, 2, 1, 2, 1, 2, 2, 1, 2, 3, 3, 3, 2, 3, 3, 1, 1, 3, 1, 3, 3, 3, 2, 3, 3, 2, 0, 1, 2, 3, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 3, 2, 2, 2, 2, 1, 2, 1, 3, 1, 3, 3, 0, 3, 2, 3, 1, 2, 1, 0, 1, 1, 2, 2, 2, 2, 2, 3, 3, 2, 1, 4, 3, 2, 1, 1, 1, 2, 2, 2, 1, 2, 3, 3, 3, 3, 2, 2, 3, 1, 3, 2, 3, 1, 3, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 3, 4, 2, 2, 1, 1, 1, 2, 1, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 4, 2, 1, 3, 1, 1, 2, 3, 3, 2, 1, 2, 2, 3, 3, 2, 0, 2, 2, 3, 3, 2, 3, 3, 1, 2, 1, 1, 2, 2, 2, 1, 3, 2, 2, 1, 3, 3, 2, 2, 2, 2, 2, 2, 3, 4, 3, 1, 2, 3, 3, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 1, 3, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 3, 3, 1, 2, 3, 2, 2, 2, 2, 3, 2, 3, 2, 1, 2, 1, 3, 2, 3, 2, 2, 1, 3, 1, 1, 3, 2, 2, 4, 2, 2, 3, 3, 2, 1, 0, 2, 2, 0, 3, 2, 2, 1, 2, 1, 2, 0, 3, 2, 2, 2, 2, 4, 3, 2, 1, 2, 2, 3, 4, 0, 3, 1, 1, 3, 3, 2, 2, 2, 3, 3, 0, 2, 3, 1, 4, 2, 2, 3, 1, 1, 2, 2, 4, 3, 3, 2, 2, 2, 2, 2, 2, 1, 2, 3, 2, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[:]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zacha\\anaconda3\\envs\\ML_env_pytorch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'accuracy': 0.14414414414414414}\n",
      "1\n",
      "{'accuracy': 0.14114114114114115}\n",
      "2\n",
      "{'accuracy': 0.15915915915915915}\n",
      "3\n",
      "{'accuracy': 0.19519519519519518}\n",
      "4\n",
      "{'accuracy': 0.15315315315315314}\n",
      "5\n",
      "{'accuracy': 0.16516516516516516}\n",
      "6\n",
      "{'accuracy': 0.12912912912912913}\n",
      "7\n",
      "{'accuracy': 0.16216216216216217}\n",
      "8\n",
      "{'accuracy': 0.16516516516516516}\n",
      "9\n",
      "{'accuracy': 0.15915915915915915}\n",
      "10\n",
      "{'accuracy': 0.16216216216216217}\n",
      "11\n",
      "{'accuracy': 0.15315315315315314}\n",
      "12\n",
      "{'accuracy': 0.13513513513513514}\n",
      "13\n",
      "{'accuracy': 0.15615615615615616}\n",
      "14\n",
      "{'accuracy': 0.14414414414414414}\n",
      "15\n",
      "{'accuracy': 0.17417417417417416}\n",
      "16\n",
      "{'accuracy': 0.15915915915915915}\n",
      "17\n",
      "{'accuracy': 0.16516516516516516}\n",
      "18\n",
      "{'accuracy': 0.17417417417417416}\n",
      "19\n",
      "{'accuracy': 0.14714714714714713}\n",
      "20\n",
      "{'accuracy': 0.15615615615615616}\n",
      "21\n",
      "{'accuracy': 0.14714714714714713}\n",
      "22\n",
      "{'accuracy': 0.16816816816816818}\n",
      "23\n",
      "{'accuracy': 0.14414414414414414}\n",
      "24\n",
      "{'accuracy': 0.13513513513513514}\n",
      "25\n",
      "{'accuracy': 0.14714714714714713}\n",
      "26\n",
      "{'accuracy': 0.13513513513513514}\n",
      "27\n",
      "{'accuracy': 0.14114114114114115}\n",
      "28\n",
      "{'accuracy': 0.15615615615615616}\n",
      "29\n",
      "{'accuracy': 0.18618618618618618}\n",
      "30\n",
      "{'accuracy': 0.17417417417417416}\n",
      "31\n",
      "{'accuracy': 0.15315315315315314}\n",
      "32\n",
      "{'accuracy': 0.15315315315315314}\n",
      "33\n",
      "{'accuracy': 0.16816816816816818}\n",
      "34\n",
      "{'accuracy': 0.15015015015015015}\n",
      "35\n",
      "{'accuracy': 0.14414414414414414}\n",
      "36\n",
      "{'accuracy': 0.17117117117117117}\n",
      "37\n",
      "{'accuracy': 0.16816816816816818}\n",
      "38\n",
      "{'accuracy': 0.15615615615615616}\n",
      "39\n",
      "{'accuracy': 0.12012012012012012}\n",
      "40\n",
      "{'accuracy': 0.1831831831831832}\n",
      "41\n",
      "{'accuracy': 0.17417417417417416}\n",
      "42\n",
      "{'accuracy': 0.15615615615615616}\n",
      "43\n",
      "{'accuracy': 0.16216216216216217}\n",
      "44\n",
      "{'accuracy': 0.16516516516516516}\n",
      "45\n",
      "{'accuracy': 0.16216216216216217}\n",
      "46\n",
      "{'accuracy': 0.16816816816816818}\n",
      "47\n",
      "{'accuracy': 0.15015015015015015}\n",
      "48\n",
      "{'accuracy': 0.15615615615615616}\n",
      "49\n",
      "{'accuracy': 0.13813813813813813}\n",
      "50\n",
      "{'accuracy': 0.16816816816816818}\n",
      "51\n",
      "{'accuracy': 0.16216216216216217}\n",
      "52\n",
      "{'accuracy': 0.15315315315315314}\n",
      "53\n",
      "{'accuracy': 0.15615615615615616}\n",
      "54\n",
      "{'accuracy': 0.12912912912912913}\n",
      "55\n",
      "{'accuracy': 0.17117117117117117}\n",
      "56\n",
      "{'accuracy': 0.15015015015015015}\n",
      "57\n",
      "{'accuracy': 0.1921921921921922}\n",
      "58\n",
      "{'accuracy': 0.15915915915915915}\n",
      "59\n",
      "{'accuracy': 0.12612612612612611}\n",
      "60\n",
      "{'accuracy': 0.17417417417417416}\n",
      "61\n",
      "{'accuracy': 0.16216216216216217}\n",
      "62\n",
      "{'accuracy': 0.15915915915915915}\n",
      "63\n",
      "{'accuracy': 0.13513513513513514}\n",
      "64\n",
      "{'accuracy': 0.2012012012012012}\n",
      "65\n",
      "{'accuracy': 0.15315315315315314}\n",
      "66\n",
      "{'accuracy': 0.15315315315315314}\n",
      "67\n",
      "{'accuracy': 0.16516516516516516}\n",
      "68\n",
      "{'accuracy': 0.0990990990990991}\n",
      "69\n",
      "{'accuracy': 0.18018018018018017}\n",
      "70\n",
      "{'accuracy': 0.17117117117117117}\n",
      "71\n",
      "{'accuracy': 0.12912912912912913}\n",
      "72\n",
      "{'accuracy': 0.17117117117117117}\n",
      "73\n",
      "{'accuracy': 0.17717717717717718}\n",
      "74\n",
      "{'accuracy': 0.15015015015015015}\n",
      "75\n",
      "{'accuracy': 0.17117117117117117}\n",
      "76\n",
      "{'accuracy': 0.13513513513513514}\n",
      "77\n",
      "{'accuracy': 0.17417417417417416}\n",
      "78\n",
      "{'accuracy': 0.16216216216216217}\n",
      "79\n",
      "{'accuracy': 0.17417417417417416}\n",
      "80\n",
      "{'accuracy': 0.14714714714714713}\n",
      "81\n",
      "{'accuracy': 0.10810810810810811}\n",
      "82\n",
      "{'accuracy': 0.14114114114114115}\n",
      "83\n",
      "{'accuracy': 0.1831831831831832}\n",
      "84\n",
      "{'accuracy': 0.1831831831831832}\n",
      "85\n",
      "{'accuracy': 0.16516516516516516}\n",
      "86\n",
      "{'accuracy': 0.14414414414414414}\n",
      "87\n",
      "{'accuracy': 0.16216216216216217}\n",
      "88\n",
      "{'accuracy': 0.15315315315315314}\n",
      "89\n",
      "{'accuracy': 0.17117117117117117}\n",
      "90\n",
      "{'accuracy': 0.13813813813813813}\n",
      "91\n",
      "{'accuracy': 0.15015015015015015}\n",
      "92\n",
      "{'accuracy': 0.15015015015015015}\n",
      "93\n",
      "{'accuracy': 0.16516516516516516}\n",
      "94\n",
      "{'accuracy': 0.15015015015015015}\n",
      "95\n",
      "{'accuracy': 0.14414414414414414}\n",
      "96\n",
      "{'accuracy': 0.12312312312312312}\n",
      "97\n",
      "{'accuracy': 0.15615615615615616}\n",
      "98\n",
      "{'accuracy': 0.16816816816816818}\n",
      "99\n",
      "{'accuracy': 0.15915915915915915}\n",
      "100\n",
      "{'accuracy': 0.17717717717717718}\n",
      "101\n",
      "{'accuracy': 0.17117117117117117}\n",
      "102\n",
      "{'accuracy': 0.15315315315315314}\n",
      "103\n",
      "{'accuracy': 0.2132132132132132}\n",
      "104\n",
      "{'accuracy': 0.15915915915915915}\n",
      "105\n",
      "{'accuracy': 0.1831831831831832}\n",
      "106\n",
      "{'accuracy': 0.18018018018018017}\n",
      "107\n",
      "{'accuracy': 0.12312312312312312}\n",
      "108\n",
      "{'accuracy': 0.1891891891891892}\n",
      "109\n",
      "{'accuracy': 0.13813813813813813}\n",
      "110\n",
      "{'accuracy': 0.16516516516516516}\n",
      "111\n",
      "{'accuracy': 0.15615615615615616}\n",
      "112\n",
      "{'accuracy': 0.18018018018018017}\n",
      "113\n",
      "{'accuracy': 0.21921921921921922}\n",
      "114\n",
      "{'accuracy': 0.14414414414414414}\n",
      "115\n",
      "{'accuracy': 0.15315315315315314}\n",
      "116\n",
      "{'accuracy': 0.13813813813813813}\n",
      "117\n",
      "{'accuracy': 0.14414414414414414}\n",
      "118\n",
      "{'accuracy': 0.16816816816816818}\n",
      "119\n",
      "{'accuracy': 0.15615615615615616}\n",
      "120\n",
      "{'accuracy': 0.17117117117117117}\n",
      "121\n",
      "{'accuracy': 0.18618618618618618}\n",
      "122\n",
      "{'accuracy': 0.1921921921921922}\n",
      "123\n",
      "{'accuracy': 0.15015015015015015}\n",
      "124\n",
      "{'accuracy': 0.13513513513513514}\n",
      "125\n",
      "{'accuracy': 0.15915915915915915}\n",
      "126\n",
      "{'accuracy': 0.18018018018018017}\n",
      "127\n",
      "{'accuracy': 0.15315315315315314}\n",
      "128\n",
      "{'accuracy': 0.2012012012012012}\n",
      "129\n",
      "{'accuracy': 0.17117117117117117}\n",
      "130\n",
      "{'accuracy': 0.17117117117117117}\n",
      "131\n",
      "{'accuracy': 0.16516516516516516}\n",
      "132\n",
      "{'accuracy': 0.18018018018018017}\n",
      "133\n",
      "{'accuracy': 0.16216216216216217}\n",
      "134\n",
      "{'accuracy': 0.15315315315315314}\n",
      "135\n",
      "{'accuracy': 0.16516516516516516}\n",
      "136\n",
      "{'accuracy': 0.16816816816816818}\n",
      "137\n",
      "{'accuracy': 0.16816816816816818}\n",
      "138\n",
      "{'accuracy': 0.16516516516516516}\n",
      "139\n",
      "{'accuracy': 0.13513513513513514}\n",
      "140\n",
      "{'accuracy': 0.14114114114114115}\n",
      "141\n",
      "{'accuracy': 0.14114114114114115}\n",
      "142\n",
      "{'accuracy': 0.18618618618618618}\n",
      "143\n",
      "{'accuracy': 0.17717717717717718}\n",
      "144\n",
      "{'accuracy': 0.16516516516516516}\n",
      "145\n",
      "{'accuracy': 0.15315315315315314}\n",
      "146\n",
      "{'accuracy': 0.15615615615615616}\n",
      "147\n",
      "{'accuracy': 0.13213213213213212}\n",
      "148\n",
      "{'accuracy': 0.16816816816816818}\n",
      "149\n",
      "{'accuracy': 0.17717717717717718}\n",
      "150\n",
      "{'accuracy': 0.16816816816816818}\n",
      "151\n",
      "{'accuracy': 0.16216216216216217}\n",
      "152\n",
      "{'accuracy': 0.17417417417417416}\n",
      "153\n",
      "{'accuracy': 0.17117117117117117}\n",
      "154\n",
      "{'accuracy': 0.13513513513513514}\n",
      "155\n",
      "{'accuracy': 0.12912912912912913}\n",
      "156\n",
      "{'accuracy': 0.12612612612612611}\n",
      "157\n",
      "{'accuracy': 0.19519519519519518}\n",
      "158\n",
      "{'accuracy': 0.15315315315315314}\n",
      "159\n",
      "{'accuracy': 0.16516516516516516}\n",
      "160\n",
      "{'accuracy': 0.15015015015015015}\n",
      "161\n",
      "{'accuracy': 0.16216216216216217}\n",
      "162\n",
      "{'accuracy': 0.18018018018018017}\n",
      "163\n",
      "{'accuracy': 0.15915915915915915}\n",
      "164\n",
      "{'accuracy': 0.16216216216216217}\n",
      "165\n",
      "{'accuracy': 0.1831831831831832}\n",
      "166\n",
      "{'accuracy': 0.13513513513513514}\n",
      "167\n",
      "{'accuracy': 0.2012012012012012}\n",
      "168\n",
      "{'accuracy': 0.16516516516516516}\n",
      "169\n",
      "{'accuracy': 0.18618618618618618}\n",
      "170\n",
      "{'accuracy': 0.16816816816816818}\n",
      "171\n",
      "{'accuracy': 0.19519519519519518}\n",
      "172\n",
      "{'accuracy': 0.14714714714714713}\n",
      "173\n",
      "{'accuracy': 0.15615615615615616}\n",
      "174\n",
      "{'accuracy': 0.13813813813813813}\n",
      "175\n",
      "{'accuracy': 0.1111111111111111}\n",
      "176\n",
      "{'accuracy': 0.13213213213213212}\n",
      "177\n",
      "{'accuracy': 0.18618618618618618}\n",
      "178\n",
      "{'accuracy': 0.12612612612612611}\n",
      "179\n",
      "{'accuracy': 0.13513513513513514}\n",
      "180\n",
      "{'accuracy': 0.18018018018018017}\n",
      "181\n",
      "{'accuracy': 0.18018018018018017}\n",
      "182\n",
      "{'accuracy': 0.15915915915915915}\n",
      "183\n",
      "{'accuracy': 0.17417417417417416}\n",
      "184\n",
      "{'accuracy': 0.1831831831831832}\n",
      "185\n",
      "{'accuracy': 0.1111111111111111}\n",
      "186\n",
      "{'accuracy': 0.2042042042042042}\n",
      "187\n",
      "{'accuracy': 0.1891891891891892}\n",
      "188\n",
      "{'accuracy': 0.21021021021021022}\n",
      "189\n",
      "{'accuracy': 0.16216216216216217}\n",
      "190\n",
      "{'accuracy': 0.1921921921921922}\n",
      "191\n",
      "{'accuracy': 0.15315315315315314}\n",
      "192\n",
      "{'accuracy': 0.12612612612612611}\n",
      "193\n",
      "{'accuracy': 0.13813813813813813}\n",
      "194\n",
      "{'accuracy': 0.17417417417417416}\n",
      "195\n",
      "{'accuracy': 0.1921921921921922}\n",
      "196\n",
      "{'accuracy': 0.16216216216216217}\n",
      "197\n",
      "{'accuracy': 0.16216216216216217}\n",
      "198\n",
      "{'accuracy': 0.16216216216216217}\n",
      "199\n",
      "{'accuracy': 0.14114114114114115}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from datasets import load_metric\n",
    "\n",
    "\n",
    "model.train()\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "size = len(train_dataloader.dataset)\n",
    "\n",
    "train_df = pd.read_csv('train_data.csv',dtype=str)\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(train_df['class']), y =train_df['class'])\n",
    "loss_fct = nn.CrossEntropyLoss(weight=torch.cuda.FloatTensor(class_weights))\n",
    "#loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "\n",
    "# acc_metric = torchmetrics.Accuracy(num_classes=5, average='macro')\n",
    "# precision_metric = torchmetrics.Precision(num_classes=5, average='macro')\n",
    "# recall_metric = torchmetrics.Recall(num_classes=5, average='macro')\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(200):\n",
    "    metric = load_metric('accuracy')\n",
    "    for batch in train_dataloader:\n",
    "        optim.zero_grad()\n",
    "        #input_ids = batch['input_ids'].to(device)\n",
    "        #attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(batch['pixel_values'].type('torch.cuda.FloatTensor'), labels=labels)\n",
    "        loss = loss_fct(outputs[1], labels)\n",
    "        #loss = loss_fct(outputs[1].view(-1, model.config.num_labels), labels.view(-1)) #difference between this and the above line?\n",
    "        #loss = outputs[0]\n",
    "        #loss.backward()                              #Do I need this loss.backward? Does my loss function do that already?\n",
    "        optim.step()\n",
    "        # if batch % 100 == 0:\n",
    "        #     loss, current = loss.item(), batch * len(X)\n",
    "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        metric.add_batch(predictions=np.argmax(outputs[1].cpu().detach().numpy(), axis=-1), references=labels)\n",
    "    print(epoch)\n",
    "    print(metric.compute())\n",
    "    #print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4633,  0.4159,  1.0280,  0.3456,  0.8690],\n",
      "        [-0.5470,  0.6106,  1.5428,  0.0613,  0.8497],\n",
      "        [-0.7136,  0.6064,  0.9738,  1.0512,  1.3467],\n",
      "        [-0.6677,  0.3204,  1.5986, -0.6328,  1.5374],\n",
      "        [-0.9205,  0.8382,  1.3102,  0.1464,  1.3542],\n",
      "        [-0.9995,  0.3725,  0.6428,  0.4840,  1.3246],\n",
      "        [-1.1717, -0.4074,  0.7591, -0.5021,  0.7506],\n",
      "        [-1.3107,  0.7272,  1.9333, -0.9098,  0.5535],\n",
      "        [ 0.2494,  1.0322,  0.7350, -0.4661,  0.1094],\n",
      "        [-0.4558, -0.2620,  1.7390, -1.6217,  0.9895],\n",
      "        [-0.7108, -0.5093,  1.3565, -0.3074,  0.9590],\n",
      "        [-0.9799,  0.1571,  1.1919,  0.7901,  1.0418],\n",
      "        [-0.2123,  0.9098,  1.3497, -0.4022,  0.4971]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[2 2 4 2 4 4 2 2 1 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(outputs[1])\n",
    "# print(labels)\n",
    "# print(model.config.num_labels)\n",
    "# print(outputs[1].view(-1, model.config.num_labels))\n",
    "\n",
    "# print(outputs[1])\n",
    "# print(batch)\n",
    "\n",
    "pred = np.argmax(outputs[1].cpu().detach().numpy(), axis=-1) \n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-dfcb90c2c1de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mdata_collator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"test\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zacha\\anaconda3\\envs\\ML_env_pytorch\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=50,\n",
    "    fp16=False,\n",
    "    #save_steps=10,\n",
    "    #eval_steps=10,\n",
    "    #logging_steps=10,\n",
    "    learning_rate=2e-7,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "#cudnn.benchmark = True\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset[\"train\"],\n",
    "    eval_dataset=train_dataset[\"test\"],\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       50.0\n",
      "  train_loss               =     1.3301\n",
      "  train_runtime            = 0:04:17.95\n",
      "  train_samples_per_second =     73.656\n",
      "  train_steps_per_second   =      4.652\n"
     ]
    }
   ],
   "source": [
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 2950/2950 [00:00<00:00, 20624.64it/s]\n",
      "Using custom data configuration DGE_test-14758c97f2c96edf\n",
      "Reusing dataset imagefolder (C:\\Users\\zacha\\.cache\\huggingface\\datasets\\imagefolder\\DGE_test-14758c97f2c96edf\\0.0.0\\0fc50c79b681877cc46b23245a6ef5333d036f48db40d53765a68034bc48faff)\n",
      "Loading cached split indices for dataset at C:\\Users\\zacha\\.cache\\huggingface\\datasets\\imagefolder\\DGE_test-14758c97f2c96edf\\0.0.0\\0fc50c79b681877cc46b23245a6ef5333d036f48db40d53765a68034bc48faff\\cache-554718d9831d2184.arrow and C:\\Users\\zacha\\.cache\\huggingface\\datasets\\imagefolder\\DGE_test-14758c97f2c96edf\\0.0.0\\0fc50c79b681877cc46b23245a6ef5333d036f48db40d53765a68034bc48faff\\cache-98c7a26f94069e47.arrow\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset(\"DGE_test\", data_dir=\"*/\", split=\"train\")\n",
    "#train_dataset = load_dataset(\"DGE_val\", data_dir=\"*/\", split='test')\n",
    "#train_dataset = load_dataset(\"DGE_test\", data_dir=\"*/\", split='test')\n",
    "test_dataset = test_dataset.train_test_split(test_size=0.01)\n",
    "test_dataset[\"train\"][0]\n",
    "labels = test_dataset[\"train\"].features[\"label\"].names\n",
    "test_dataset = test_dataset.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = trainer.evaluate(test_dataset[\"train\"])\n",
    "# trainer.log_metrics(\"eval\", metrics)\n",
    "# trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "# from transformers import ImageClassificationPipeline\n",
    "\n",
    "# classifier = ImageClassificationPipeline(model=model, tokenizer=feature_extractor, framework=\"pt\", num_workers=10, batch_size=16, device=0)\n",
    "\n",
    "# classifier(test_dataset, batch_size=16)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2920\n",
      "  Batch size = 8\n",
      "100%|██████████| 365/365 [00:21<00:00, 17.16it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(test_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=array([[-0.9374621 ,  0.7268996 ,  0.86397487, -0.6530805 , -0.48267034],\n",
      "       [ 0.09381264,  0.14224368, -0.40154248, -1.0393232 ,  0.5632388 ],\n",
      "       [ 0.11771351,  0.94170356,  0.36834213, -1.3381771 ,  0.4872938 ],\n",
      "       ...,\n",
      "       [ 0.6386661 ,  0.60770094,  0.30931827,  0.85920656, -0.22804642],\n",
      "       [-0.96552956,  1.1937306 ,  0.36758405, -0.9231657 , -0.46966907],\n",
      "       [ 0.4315107 ,  1.0482173 ,  1.2298714 , -0.6966719 ,  0.2938264 ]],\n",
      "      dtype=float32), label_ids=array([3, 2, 2, ..., 2, 2, 2], dtype=int64), metrics={'test_loss': 1.6232883930206299, 'test_accuracy': 0.16712328767123288, 'test_runtime': 21.5008, 'test_samples_per_second': 135.809, 'test_steps_per_second': 16.976})\n",
      "[2 4 1 ... 3 1 2]\n",
      "[3 2 2 ... 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)\n",
    "y_pred = np.argmax(predictions.predictions, axis=-1)\n",
    "y_true = predictions.label_ids\n",
    "print(y_pred)\n",
    "print(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAI/CAYAAABd3iKmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA3O0lEQVR4nO3dd3xUZdrG8etJoYUinVAUBFSUqtJEEEEFUYpgQ8CGsLqyyrvuuurqWlgRG4iIJaCANEWlFwERpFdBpHcQQZAmJBRJ5nn/ILJEUkAz55xn8vvuZz7MzDmZuWc2Jneup4yx1goAACBIovwuAAAA4PdoUAAAQODQoAAAgMChQQEAAIFDgwIAAAKHBgUAAAROTLifoHqp+qxjDrM8Ubn8LiFHSEo57ncJEW/L4Z/8LiHi5YmO9buEHOFg4ibj5fOd3LfFs9+1scUu9uS1kaAAAIDACXuCAgAAwiyU4ncF2Y4EBQAABA4JCgAArrMhvyvIdiQoAAAgcGhQAABA4DDEAwCA60IM8QAAAIQdCQoAAI6zTJIFAAAIPxIUAABcxxwUAACA8CNBAQDAdcxBAQAACD8SFAAAXMeHBQIAAIQfCQoAAK5jDgoAAED4kaAAAOA69kEBAAAIPxIUAAAcx2fxAAAAeIAGBQAABA5DPAAAuI5JsgAAAOFHggIAgOuYJAsAABB+JCgAALiODwsEAAAIPxIUAABcxxwUAACA8CNBAQDAdeyDAgAAEH4kKAAAuI45KAAAAOFHggIAgOuYgwIAABB+JCgAADjOWnaSBQAACDsaFAAAEDgM8QAA4DqWGQMAAIQfCQoAAK5jmTEAAED4kaAAAOA65qAAAACEHwkKAACuC7FRGwAAQNiRoAAA4DrmoESWqKgofTp9iPoNfSPDc66oWUXLf5yrG2+93sPKIsfYRZ9oxIxBGjZ9oIZM+eCs442aNdDwrz46fbxGnWo+VOm+qKgoff7Vx+o/7M2zjt3SrplGzxym0TOHadjEAbr08so+VOi2999/Xdu3L9PSpdPSPX733W20ePGXWrz4S82cOVrVqlXxuEL39Xv3FW3YukjzF09O9/jfHn9Is+eP1+z54zV/8WTt+2W9LihcyOMq4aUcnaB06HKntm7cprgCcekej4qK0v89+1fNn7XI48oiyyN3dNcvB35J99iSOd9q9tR5kqRKVS5Wzw9e0J2N7vWyvIjQqctd2pLB9/KP23fp/jaP6PAvR3Rtk/p64c2n1P7mzj5U6a6hQz/T++8P0cCBvdM9vm3bD7rppjt16NBh3XRTY/Xv/4oaNWrjbZGOGzl8tAZ8MEzvD3g93eP9+g5Uv74DJUnNb26iR7o9oEMH0/+5kiOxD0rkKBlfXI1uaKDRw8dneM49ne/Q9EmzdGDfQQ8ry1mOHT12+nrefHllrY/FOKpkfAk1urGBvhg+Lt3jK5Z+r8O/HJEkrVy2SiXjS3hZXkSYN2+xDhw4lOHxhQuX6dChw5KkxYu/VZky8R5VFjnmz1uigwcPndO57e64VV98NjG8BcF3WSYoxpjLJLWWVEaSlbRL0nhr7dow1xZWT/bort493lFc/nzpHi9RqriatLhOD7Xrpqo1iWv/MCv1G/mGrLUaM3SCxg6fcNYpjZs31F+f6aLCRQvr7/c+5UORbnuqx//pzZcy/l4+U9t7WmnO1ws8qCrnuv/+uzV16iy/y4hYefPmUdMbGumfT7zodynBEoFzUDJtUIwx/5LUXtInkhan3l1W0khjzCfW2l5hri8sGt3YQAf2HdTalet19TW10j3nyR7d9VaP/gpFYGzmpYdaP6p9e/arcNEL9M4nb2r7pu1avmhlmnNmfTlHs76co1p1q+svTz6obnc94VO17rnuxgY6sO+A1qxcp9rXXJnpuXUaXKW297RUp1ZdPaou52nUqL7uu+8uNW3azu9SIlbzFk20aOG3DO/kAFklKJ0lXWGtPXnmncaY3pJWS0q3QTHGdJXUVZLKFKigIvlKZkOp2adm7epqfFNDXdv0GuXOnUtx+ePU853n9Uy3/3XkV9S4TK9+0EOSVLhIITVsWl/JySma+eVsv8p20r49+yVJB/cf0qwv5+jyWlXOalB+s3zRSpW9qIwKFSmU4ZwVpFWrTg01btZIDZteo9x5cisuf5x69X9BTz36QprzLrm8kl7s/Ywebt9dvxw87E+xEa5q1cv03nuvqnXr+zIdDsKf0/b2W/XFZ2cnsTleBP4xnVWDEpJUWtL2390fn3osXdbaBEkJklS9VP3AzSp4u+d7ervne5Kkq6+ppfse6ZCmOZGkm+v87y+gHn2f1ezp82hOzlOevHkUFWV0NOmY8uTNo7rX1dbA3kPSnFO2fBnt3PajJOnSapUVExtDc3Ie3nr5Xb318ruSpNrXXKn7/9rhrOYkvkxJ9f2ol55+9AVt3/KDD1VGvnLlSuuTTz5Q587/p02btvpdTsQqWDC/GjSoo790JmXNCbJqULpLmmGM2Sjpt59sF0qqJKlbGOvyxR333iZJ+uzjMT5XEhmKFC+s1z/8ryQpOiZaU8d8pYWzFqttp1aSpNFDx6vJLY3U4vZmSk5O1oljv+rfjzCunB3uTP1eHvXxGD38RGcVKlxIz736pCQpOTlFdzW738fq3DNkyNtq2LC+ihUrrE2bFqpHjz6KjT3143PgwOF6+unHVaRIYb311qnUNTk5Rdde29LPkp0zcFAfNWhYV0WLFtaq9XPV6+W+p9/jQR+OlCTd0vImzfx6ro6eMbkekcvYLJZNGGOiJNXRqUmyRtJOSUustee0r24QE5RIkycql98l5AhJKcf9LiHibTn8k98lRLw80bF+l5AjHEzcZLx8vuNzhnr2uzZPw06evLYsV/FYa0OSFnpQCwAAgKQcvlEbAACR4BwHNZySYzdqAwAAwUWCAgCA6yJwmTEJCgAACBwSFAAAXBeBW92ToAAAgMAhQQEAwHXMQQEAAAg/EhQAAFzHHBQAAIDwI0EBAMB1AZmDYozJI2m2pNw61WN8bq193hhTRNKnkspL2ibpTmvtwcweiwQFAABklxOSmlhra0iqKam5MaaepKckzbDWVpY0I/V2pkhQAABwXUDmoFhrraTE1JuxqRcrqbWkxqn3D5E0S9K/MnssEhQAAJBtjDHRxpgVkvZKmm6tXSSppLV2tySl/lsiq8ehQQEAAOfMGNPVGLP0jEvXM49ba1OstTUllZVUxxhT9Y88D0M8AAC4zsNJstbaBEkJ53DeIWPMLEnNJe0xxsRba3cbY+J1Kl3JFAkKAADIFsaY4saYC1Kv55V0g6R1ksZLui/1tPskjcvqsUhQAABwXUCWGUuKlzTEGBOtUyHIKGvtRGPMAkmjjDGdJe2QdEdWD0SDAgAAsoW1dqWkWuncv19S0/N5LBoUAABcF5BlxtmJOSgAACBwSFAAAHBdcOagZBsSFAAAEDgkKAAAuI45KAAAAOFHggIAgOuYgwIAABB+JCgAALiOOSgAAADhR4ICAIDrmIMCAAAQfjQoAAAgcBjiAQDAdQzxAAAAhB8JCgAArrPW7wqyHQkKAAAIHBIUAABcxxwUAACA8CNBAQDAdSQoAAAA4UeCAgCA6/iwQAAAgPAjQQEAwHXMQQEAAAg/EhQAAFzHTrIAAADhR4ICAIDrmIMCAAAQfiQoAAC4LgITlLA3KGsO7Aj3U+R4hfPm97uEHCHx1+N+lxDxYqOi/S4BQEAwxAMAAAKHIR4AAFzHVvcAAADhR4ICAIDjbIiN2gAAAMKOBAUAANdF4DJjEhQAABA4JCgAALiOVTwAAADhR4ICAIDrWMUDAAAQfiQoAAC4jlU8AAAA4UeCAgCA60hQAAAAwo8EBQAA11lW8QAAAIQdDQoAAAgchngAAHAdk2QBAADCjwQFAADXsdU9AABA+JGgAADgOsscFAAAgLAjQQEAwHXMQQEAAAg/EhQAABxn2QcFAAAg/EhQAABwHXNQAAAAwo8EBQAA17EPCgAAQPiRoAAA4DrmoAAAAIQfDQoAAAgchngAAHAdG7UBAACEHwkKAACuY5IsAABA+NGgAADgOhvy7pIJY0w5Y8xMY8xaY8xqY8zjqfe/YIz50RizIvXSIquXxBAPAADILsmSnrDWfmuMKSBpmTFmeuqxPtbaN871gWhQAABwXUDmoFhrd0vanXr9iDFmraQyf+SxGOIBAADZzhhTXlItSYtS7+pmjFlpjPnIGFM4q6+nQQEAwHE2FPLsYozpaoxZesal6+/rMcbkl/SFpO7W2sOS3pNUUVJNnUpY3szqNTHEAwAAzpm1NkFSQkbHjTGxOtWcDLfWjk79mj1nHB8gaWJWz0ODAgCA6wIyB8UYYyR9KGmttbb3GffHp85PkaTbJK3K6rFoUAAAQHZpIKmTpO+NMStS73tGUntjTE1JVtI2SX/J6oFoUAAAcF1AEhRr7VxJJp1Dk8/3sXLUJNlmNzXW6lWztW7NXD35z0czPO/qq2roxLEdatv2ltP3DUh4U7t2fqcVy2d4UWpEKF2mlMZM+FjzFk/WnIUT1fXhe886p90dLTVr3njNmjdek6aN1BVVL/WhUre9//7r2r59mZYunZbu8UsuqahZs8bo0KEN6t79rLlsOAf933tVm7ct1sIlU9I9fm3Duvph13eau2Ci5i6YqH899TePK3Rfv3df0YatizR/cfq/xwoWzK+RoxI0Z8EEzV8yRfd0bOdxhfBajmlQoqKi9Hbfl3Vry46qVuN63XVXG1WpUjnd817p+W9NmzYrzf0ffzxKt9zawaNqI0NKcoqef7aXGtRpoeY33KUHu9yjSy6tmOacHdt3qvUtHdW4QSv1fu09vdm3h0/Vumvo0M/UuvV9GR4/ePCQnnjieb311gAPq4osw4d9rrZtHsj0nAXzl+ja+rfq2vq36tVe/TyqLHKMHD5at7d5MMPjD3XtpPXrNqph/ZZqeXMH/bfn04qNjfWwwoALyE6y2SnHNCh1atfS5s3btHXrDp08eVKjRo1Tq5bNzjqv26MPavSYSdr78/4098+Zu0gHDh7yqNrIsGfPz1r53RpJUlJikjas36L40iXTnLNk8XL9cuiwJGnp0hUqXbqU53W6bt68xTpw4FCGx3/+eb+WLVupkydPeldUhJk/b4kOZvIe48+bP2+JDmbyM9Zaq/wF8kuS4uLy6eDBX5ScnOxRdfBDjmlQSpcppR927jp9e+ePu8/6ZVi6dCm1ad1cHyQM9bq8iFfuwjKqVr2Kli39LsNzOnS6XTO+mu1hVUD2qVOnluYtnKQvxnyky9JJZ/HnDPhgqC65tKLWbpqveYsm6ekne8jaYMy7QHj84UmyxpgHrLWDsrOYcDq18imt339z937zRT39TE+FQt5FWDlBXFw+DRr6tp59uqcSjySle06DhnXVodPturXZPR5XB/x5361YrSuqNFRS0lHd1KyxRn7ygWrVaOJ3WRGlyQ0N9f3KtWrVoqMqXHyRxowfrAXzl+rIkUS/SwuGgEySzU5/JkF5MaMDZ+4yFwql/wvJaz/u3K1yZUufvl22TLx2796T5pyrrqyu4cPe1aYNC9Wu7S165+2eatXq7GEgnLuYmBgNGvq2Ph81QZMmTE/3nMuvuFR9+v1Xndr/NdOIFwiqI0cSlZR0VJI0beosxcTGqEjRLHfyxnno0LGdJo6fKknaumW7tm/fqcqXXOxzVQinTBMUY8zKjA5JKpnBsTS7zMXkKhOItm7J0hWqVKmCypcvpx9//El33tlane5Nu5Kn8qX1T1//cGAfTZr8lcan/geBP+atd17WhvVb9H7/wekeL1M2XoOH9dOjXZ/Uls3bPK0NyC4lShbT3j37JElXXVVdUVFROrD/oM9VRZadO3epUeNrtGD+UhUvUVSVKlfQtm0/+F1WYNgITFCyGuIpKamZpN//l2YkzQ9LRWGSkpKix7s/q8mTRig6KkqDh3yqNWs2qGuXTpKkhAGZzzsZNrS/rmtUX8WKFdG2LUv14ktvaNDgT7wo3Vl1612lu9q30epV6zVzzlhJ0ssv9VaZcqeSrCEffaJ//OtRFS5ygV5783lJUnJKim5szPLB8zFkyNtq2LC+ihUrrE2bFqpHjz6KjT31n/bAgcNVsmRxzZs3QQUK5FcoFFK3bg+qVq0biMbPw0eD++rahnVVtGhhrd0wTz3/2/f0e/zRhyPUps3N6vxQByWnpOj4seN64L7HfK7YPQMH9VGD1Pd41fq56vXy/97jQR+O1Ou9+qv/B69p3qJJMsboxedepwmMcCazSUbGmA8lDUrdeOX3x0ZYa7OcMBCUBCWSFc6b3+8ScoTEX4/7XULEi42K9ruEiBdtcszaCF8dTNyU3mZlYXPksVs9+11b4O2Jnry2TBMUa23nTI4xmxEAAIQFW90DAOC6CFx9StYHAAAChwQFAADXReAqHhIUAAAQOCQoAAC4jgQFAAAg/EhQAABwXCR+cCIJCgAACBwSFAAAXMccFAAAgPCjQQEAAIHDEA8AAK5jiAcAACD8SFAAAHCcJUEBAAAIPxIUAABcR4ICAAAQfiQoAAC4LuR3AdmPBAUAAAQOCQoAAI5jFQ8AAIAHSFAAAHAdCQoAAED4kaAAAOA6VvEAAACEHwkKAACOYxUPAACAB2hQAABA4DDEAwCA65gkCwAAEH4kKAAAOI5JsgAAAB4gQQEAwHXMQQEAAAg/EhQAABxnSVAAAADCjwQFAADXkaAAAACEHwkKAACOYw4KAACAB0hQAABwHQkKAABA+JGgAADgOOagAAAAeIAGBQAABA5DPAAAOI4hHgAAAA+QoAAA4DgSFAAAAA+QoAAA4Dpr/K4g24W9QSlToGi4nyLH27R+rN8l5AhNa3Txu4SIt/fkYb9LiHiJycf8LgE4JyQoAAA4jjkoAAAAHiBBAQDAcTYUeXNQSFAAAEDgkKAAAOA45qAAAAB4gAQFAADH2QjcB4UEBQAABA4JCgAAjmMOCgAAQAaMMeWMMTONMWuNMauNMY+n3l/EGDPdGLMx9d/CWT0WDQoAAMguyZKesNZWkVRP0qPGmMslPSVphrW2sqQZqbczxRAPAACOC8pGbdba3ZJ2p14/YoxZK6mMpNaSGqeeNkTSLEn/yuyxSFAAAEC2M8aUl1RL0iJJJVObl9+amBJZfT0JCgAAjrPWu+cyxnSV1PWMuxKstQm/Oye/pC8kdbfWHjbm/BMeGhQAAHDOUpuRhIyOG2Nidao5GW6tHZ169x5jTLy1drcxJl7S3qyehyEeAAAcZ0PGs0tmzKmo5ENJa621vc84NF7SfanX75M0LqvXRIICAACySwNJnSR9b4xZkXrfM5J6SRpljOksaYekO7J6IBoUAAAcF6BVPHMlZVRM0/N5LIZ4AABA4JCgAADgOC9X8XiFBAUAAAQOCQoAAI4LyhyU7ESCAgAAAocEBQAAx1lLggIAABB2JCgAADjOhvyuIPuRoAAAgMChQQEAAIHDEA8AAI4LMUkWAAAg/EhQAABwHMuMAQAAPECCAgCA49jqHgAAwAMkKAAAOM5avyvIfiQoAAAgcEhQAABwHHNQAAAAPECCAgCA49hJFgAAwAMkKAAAOI6dZAEAADxAggIAgOPYBwUAAMADNCgAACBwGOIBAMBxLDMGAADwAAkKAACOi8Rlxjm2QcmdO5dGTRykXLlyKSYmWpPHf6U+r76b5pyKlcvrjX49dEX1Knrj5X5K6D/Ep2rddeLEr7rv0X/q15MnlZKcohuvv1bdHuqk/h8O0xfjv1ThCwpJkh7/y31qdE0dn6uNHPkLxunJN/6hCpeWl6xVryfe0Opla/wuy0m5cufSsHEJypU7VtHRMZo2cYb6vZaQ5pyChQro5b7P6cLyZXXi+K/6d/ce2rhus08Vuym+TCn1fbenipcsplAopBFDPteHHwxLc06hQgX1Zr8euqhCOZ04fkJPPPac1q/d5FPFCLcc26CcOPGr2rd5SEeTjikmJkafTx6iWTPmavnSlafPOXTwsJ5/upeatWjiY6Vuy5UrVh+93Uv58uXVyeRk3fvIP9Sw3tWSpE53tdED99zuc4WR6bGXumnRzCX6T9cXFRMbozx5c/tdkrN+PfGr7m/3SOrPimgNnzBQs2fM13fLVp0+5y/dH9C6VRv0t/ufVIVKF+k/vf6lB27/q49VuyclOVkvPfe6Vq1cq7j8+TTl61GaPWu+Nq7fcvqcv/29i1avWqeH7n1cFStX0Muv/Vt33/aQj1UHB8uMI8zRpGOSpJjYGMXGxMj+7v/h/fsOaOXy1Tp5MtmP8iKCMUb58uWVJCUnJys5OVnGRF4UGST58udTjbrVNGnkZElS8slkJR5O8rkqt535syIm9uyfFRUvqaAFc5ZIkrZu2q4yF8araPEintfpsr179mnVyrWSpKTEo9q4YYtKxZdMc07lSytq7jcLJUmbN25V2QvLqFjxop7XCm9k2aAYYy4zxjQ1xuT/3f3Nw1eWN6KiojR51ih9u26W5nyzQCuWfe93SREpJSVF7e57VI1uba/6tWup+hWXSZJGfjFBt937iJ7t2Vu/HD7ic5WRo/RF8Tq0/xc93edJDZz6vp58/QnlyZvH77KcFhUVpTFfD9e8NdM0/5tFWvnt6jTH16/eqJtuuV6SVK3W5SpdtpRKxZfwo9SIULZcaVWtXkXLl61Mc/+aVet1c8sbJEk1r6yqsuXiFV+6ZHoPkeOErPHs4pVMGxRjzGOSxkn6m6RVxpjWZxzuGc7CvBAKhdSi8Z2qV+1G1axVVZdcVsnvkiJSdHS0vhjSXzPGDNX3azZo45Ztuuu2WzRl1Ef6YnB/FS9aRK+/M8DvMiNGdHS0KlerrLEfj9dDzR7W8aPH1aHb3X6X5bRQKKTbmnRQ4xq3qHqtK1T5soppjie8PUQFCxXUmK+Hq+NDd2nt9xuUnJLiU7VuyxeXVwlD+uiFZ15V4pG0yV//vgNV6IKCmvrN53qgSwetWrlOycm8z5EqqzkoXSRdZa1NNMaUl/S5Maa8tbavpAzbKGNMV0ldJalIvjLKnyfYUefhw0e0YN5SNW7aQBvWMeEqXAoWyK/aV1bX3IVL08w9ub3VzXr0n8/7WFlk+Xn3z/p5989au3ydJGnWpNk0KNnkyOFELZ6/TA2b1E8zCTYpMUnPPP7S6dszlo7Tzu27/CjRaTExMUoY8pbGfD5JUyZ+ddbxxCNJeqLbc6dvL1gxVT/s2OlliYEViat4shriibbWJkqStXabpMaSbjbG9FYmDYq1NsFae7W19uqgNidFihZWwYIFJEm58+TWtdfV06aNW32uKvIcOHhIh48kSpKOnzihhUuWq8JF5fTzvgOnz5nxzXxVuvgiv0qMOAd+Pqi9u35WuYplJUlXXVtL2zZs97kqdxUueoEKFDw1wp07T27Vb1RHWzZuS3NOgYL5FRt76u+9Ozq20ZKFy5WUyLyf8/XG2y9p04YtGvDux+keL1iwwOn3+Z5722nR/GVnpSyIHFklKD8ZY2paa1dIUmqScqukjyRVC3dx4VSiZDH17v9fRUVHKyoqShPHTtXX02arw/13SJKGD/5MxUsU1YQZnyh/gTiFQiE9+HBH3XBNG/6DOA8/7z+of//3DaWEQrIhq2ZNGqpxg7p66qXXtX7jFslIZUqV1PNPPuZ3qRGl73P99Fy/ZxQbG6tdO3brlb+/5ndJzipesph69XtB0dFRMiZKX47/SrOmz9Vd97WVJH06ZLQqXlJBvd55QaGUkDZt2Kpnu/fwuWr31K5bS7ff3UprV2/Q1G8+lyS92qOvSpeNlyQNGzxKlS69WH3f7amUlBRtXL9F/3jsP36WHCiRuJOs+f1s9DQHjSkrKdla+1M6xxpYa+dl9QQXFa0egYufgmXT+rF+l5AjNK3Rxe8SIt7ek4f9LiHiJSYf87uEHGHngVWedgyLSrf17Hdt3V2jPXltmSYo1toMB/fOpTkBAADhF4lJQI7eBwUAAARTjt1JFgCASBGJc1BIUAAAQOCQoAAA4LicuA8KAACA52hQAABA4DDEAwCA40J+FxAGJCgAACBwSFAAAHCczfjj8ZxFggIAAAKHBAUAAMeFInCvexIUAAAQOCQoAAA4LsQcFAAAgPAjQQEAwHGs4gEAAPAACQoAAI5jJ1kAAAAPkKAAAOA45qAAAAB4gAQFAADHMQcFAADAAzQoAAAgcBjiAQDAcQzxAAAAeIAEBQAAx7HMGAAAwAMkKAAAOC4UeQEKCQoAAAgeEhQAABwXYg4KAABA+NGgAADgOOvhJSvGmI+MMXuNMavOuO8FY8yPxpgVqZcWWT0ODQoAAMhOgyU1T+f+PtbamqmXyVk9CHNQAABwXJB2krXWzjbGlP+zj0OCAgAAvNDNGLMydQiocFYn06AAAOC4kDGeXYwxXY0xS8+4dD2HEt+TVFFSTUm7Jb2Z1RcwxAMAAM6ZtTZBUsJ5fs2e364bYwZImpjV15CgAADguCCt4kmPMSb+jJu3SVqV0bm/IUEBAADZxhgzUlJjScWMMTslPS+psTGmpk71ONsk/SWrx6FBAQAA2cZa2z6duz8838ehQQEAwHFBWmacXZiDAgAAAocEBQAAx4Ui77MCSVAAAEDwkKAAAOC4kCIvQiFBAQAAgUOCAgCA4/7oBmpBRoICAAAChwQFAADHReIqnrA3KD8e2R/up8jx6la71+8ScoS8Ubn8LiHiHTl51O8SIt6JlJN+lwCcExIUAAAcx06yAAAAHiBBAQDAcaziAQAA8AAJCgAAjovEVTwkKAAAIHBoUAAAQOAwxAMAgONYZgwAAOABEhQAABxHggIAAOABEhQAABxnWWYMAAAQfiQoAAA4jjkoAAAAHiBBAQDAcSQoAAAAHiBBAQDAcdbvAsKABAUAAAQOCQoAAI4LsQ8KAABA+JGgAADgOFbxAAAAeIAGBQAABA5DPAAAOI4hHgAAAA+QoAAA4Dg2agMAAPAACQoAAI5jozYAAAAPkKAAAOA4VvEAAAB4gAQFAADHsYoHAADAAyQoAAA4LhSBGQoJCgAACBwSFAAAHMcqHgAAAA+QoAAA4LjIm4FCggIAAAKIBgUAAAQOQzwAADiOSbIAAAAeIEEBAMBxIeN3BdmPBAUAAAQOCQoAAI5jq3sAAAAPkKAAAOC4yMtPSFAAAEAA5agEpdlNjdW790uKjorSR4NG6rXX+6c53rLlTXrxhX8qFLJKTk7WE088r3nzl0iSHn+six58sL2stVq1ap06P/R3nThxwo+X4ZyoqCgN+3Kgfv7pZz1+77/SHCtQqICe7/O0yl1UWidO/KoX/+8VbV6/1adK3fT5whE6mnhUoVBIKckp6tzikTTH73n4Lt3UtqkkKTo6WhdVvlC3VG+rI4eO+FGuc0qXKaW+772i4iWKKhSyGj7kM334wbA05zz8twfU9o5bJUnRMdGqfMnFql6poQ4d+sWPkiNG6TKl9O4Hr6lEyeIKhUL6ePCnSnjvY7/LCqRI3AfFWBveYCgmV5lAJE9RUVFau3qOmrdor507d2vhgsnq2OmvWrt24+lz4uLyKSnpqCSpWrUqGjnifVWtdp1Kly6lb2aOUbUa1+v48eMaOeJ9TZnytT4eOsqvl5NG9aIV/C4hUx3+cpcur36Z8hfId1aD0v25v+po0jEl9B6k8pUu1FM9/66H7+zuT6FZyBuVy+8S0vX5whHqfPPD+uXg4SzPbXBjfd3V5XY9ducTHlR2/rYd3eN3CWcpUbKYSpQsrlUr1youfz59OfMzPdjxMW1cvznd829s3lhdHrlXd7Z+0ONKz82JlJN+l3DOSpYsrpKlimvld2uUP3+cZswerU7t/6oNGbz3QbLv8AZPF/4+Xf4ez37XvrJthCevLccM8dSpXUubN2/T1q07dPLkSY0aNU6tWjZLc85vzYkkxeXLpzObt5iYGOXNm0fR0dHKlzevdu/+ybPaXVYivrgaNq2vsSMmpHu8wiXltXjuMknStk07FF8uXkWKFfayxBzlhtZNNH3s136X4ZS9e/Zp1cq1kqSkxKPauGGLSsWXyPD81u1aaOwXk70qL6Lt2fOzVn63RpKUmJikDes3K750SZ+rCqaQrGcXr2TZoBhj6hhjaqdev9wY83djTIvwl5a9SpcppR927jp9e+ePu1W6dKmzzmvdurlWff+Nxo8boi5dTv2VuWvXT+rd531t3bxYO3cs1y+HD2v6V7M9q91l/3jpMfX973sKhdL/pt64ZpOatGgkSbqiZhXFly2pkqUz/uGPs1lr1Wfk6/pwyvtq1eGWDM/LnSe36jWurVmT+d79o8qWK62q1ato+bKV6R7PkzePGje9VpPHT/e4sshX7sIyqlb9ci1b+p3fpcAjmTYoxpjnJb0t6T1jzCuS3pGUX9JTxph/e1BftjHm7EQqveGtceO+VNVq16nd7Z314gv/lCRdcEEhtWrZTJUuqadyF12puLh8uueetmGv2XUNb7hGB/Yd0tqV6zM8Z1C/YSpYqIBGTh+kuzu30/pVG5WcnOJhle57pM1jerD5X/REx6fU9v42qlG3errnXXtTfa1cupq5J39Qvrh8GvDxW3r+6V5KPJKU7jk3NW+spYuWM/ckm8XF5dPgof3076d6Zvje53TWw4tXspoke7ukmpJyS/pJUllr7WFjzOuSFkl6Ob0vMsZ0ldRVkkx0IUVFxWVbwX/Ujzt3q1zZ0qdvly0Tr927Mx7vnjN3kS6++CIVLVpYjRs30NZtO7Rv3wFJ0pixU1S/3tUaMWJ02Ot2WY061XTdTQ10bdN6ypU7l+IKxOm/7zynZ7v1OH1OUuJRvfB/r5y+PXHxZ9q1Y1d6D4cM7NuzX5J0aP8hzZ4yV5fXvEzfLTr7L/ymrZroq7EzvC4vIsTExGjAkLc05rNJmjLxqwzPa9X2ZoZ3sllMTIwGDeunz0dN0KQJ0/wuBx7Kaogn2VqbYq09KmmztfawJFlrjymTScPW2gRr7dXW2quD0JxI0pKlK1SpUgWVL19OsbGxuvPO1powMe03e8WK5U9fr1WzqnLlitX+/Qf1w44fVbfulcqbN48kqcn112rduo1C5t7p+YFuvqqtbq1zh55++AUtnbssTXMiSfkL5ldM7Kk++bYOLfXtwu+UlHg0vYdDOvLkzaN8cXlPX69z3dXaks4qqLgCcapVr7rmTJ3vdYkR4c1+L2nThi1KeHdIhucUKJhf9RrU1tTJzPHJTn3799SG9Zv1Xv9BfpcSaCEPL17JKkH51RiTL7VBueq3O40xheTYqqaUlBQ93v1ZTZ40QtFRURo85FOtWbNBXbt0kiQlDBiqtre1UMeOt+vkyWQdP3Zc93Q4tVxz8ZLlGj16kpYsnqrk5GStWLFaAwYO9/PlOK3dva0lSV98PE4XV75IL739rFJCIW3dsE0v/r2Xz9W5pUjxwur54UuSpJjoaE0bO0OLZi1Rm04tJUljh56anHzdzddq8eylOn7suG+1uqp2vSt1+92ttWb1ek2b/YUkqVePt1SmbLwkaeigU6v5br7lBs2eOU/Hjh7zrdZIU7feVbqrfRutXrVOM+eOkyS9/FJvfTXtG58rgxcyXWZsjMltrT1rsw9jTDFJ8dba77N6gqAsM45kQV9mHCmCusw4kgRxmXGkcWmZscu8Xmb8j/LtPftd+8a2kZ68tkwTlPSak9T790naF5aKAADAeeHDAgEAADxAgwIAgOOCtMzYGPORMWavMWbVGfcVMcZMN8ZsTP03yx05aVAAAEB2Giyp+e/ue0rSDGttZUkzUm9nigYFAADHBWmZsbV2tqQDv7u7taTf1ukPkdQmq8ehQQEAAOFW0lq7W5JS/83yM02y2gcFAAAEnPVwFc+Zu8WnSrDWJmT389CgAACAc5bajJxvQ7LHGBNvrd1tjImXtDerL2CIBwAAxwVpDkoGxku6L/X6fZLGZfUFNCgAACDbGGNGSlog6VJjzE5jTGdJvSTdaIzZKOnG1NuZYogHAADHBWknWWtt+wwONT2fxyFBAQAAgUOCAgCA44KTn2QfEhQAABA4JCgAADguSHNQsgsJCgAACBwaFAAAEDgM8QAA4Lg/sYFaYJGgAACAwCFBAQDAcV5+WKBXSFAAAEDgkKAAAOA45qAAAAB4gAQFAADHMQcFAADAAyQoAAA4jjkoAAAAHiBBAQDAcSHLHBQAAICwI0EBAMBxkZefkKAAAIAAIkEBAMBxoQjMUEhQAABA4NCgAACAwGGIBwAAx7HVPQAAgAdIUAAAcBxb3QMAAHiABAUAAMexzBgAAMADJCgAADiOVTwAAAAeIEEBAMBxrOIBAADwAAkKAACOs5Y5KAAAAGFHggIAgOPYBwUAAMADJCgAADiOVTwAAAAeIEGJAL+cPOp3CTnC3tAhv0uIeEdPnvC7hIh3MpTidwnAOaFBAQDAcWx1DwAA4AESFAAAHMcyYwAAAA+QoAAA4Di2ugcAAPAACQoAAI5jozYAAAAPkKAAAOA49kEBAADwAAkKAACOYx8UAAAAD5CgAADgOPZBAQAA8AAJCgAAjmMOCgAAgAdIUAAAcBz7oAAAAHiABgUAAAQOQzwAADguxDJjAACA8CNBAQDAcZGXn5CgAACAACJBAQDAcWzUBgAA4AESFAAAHEeCAgAA4AESFAAAHGfZBwUAACD8SFAAAHBcJM5BoUEBAADZxhizTdIRSSmSkq21V/+Rx6FBAQDAcTZ4Ccr11tp9f+YBmIMCAAAChwQFAADHBWwVj5U0zRhjJX1grU34Iw9CgwIAAM6ZMaarpK5n3JXwuyakgbV2lzGmhKTpxph11trZ5/s8NCgAAOCcpTYjGaYi1tpdqf/uNcaMkVRH0nk3KMxBAQDAcSFZzy6ZMcbEGWMK/HZd0k2SVv2R10SCAgAAsktJSWOMMdKpHmOEtfbLP/JANCgAADguKJNkrbVbJNXIjsdiiAcAAAQOCQoAAI6LxK3uSVAAAEDgkKAAAOC4AG51/6eRoAAAgMAhQQEAwHGhgKziyU4kKAAAIHBIUAAAcBxzUAAAADyQoxqUZjc11upVs7VuzVw9+c9HzzresuVN+nbZdC1dMk0LF0xWg2tqnz72+GNd9N2Kr7Vi+QwNG9pfuXPn9rJ0J+XKnUujp32sibM+0ZS5n+nxfz2c7nl1G1ylCTNHasrczzRi/ACPq3RbfJlSGjXuI81cOF4z5o9V5790POucQoUKauDHfTV9zmhNnD5Sl1ap5EOlbuv37ivasHWR5i+enO7xggXza+SoBM1ZMEHzl0zRPR3beVyh+959/1Vt3bZEi5dkviv6lVdV1y9HNqlNm5s9qswNIWs9u3jFhHt73JhcZQKRO0VFRWnt6jlq3qK9du7crYULJqtjp79q7dqNp8+Ji8unpKSjkqRq1apo5Ij3VbXadSpdupS+mTlG1Wpcr+PHj2vkiPc1ZcrX+njoKL9eThoXFSzpdwkZyheXV0eTjikmJkafTvpQPZ55QyuWfX/6eIGC+fXZlMF64M5u2v3jTyparLD27zvoY8UZOxH61e8SzlKiZDGVKFlcq1auVVz+fJry9Sh17vSYNq7fcvqcZ198QklJR9XntfdUsXIFvfzav3X3bQ/5WHXGkn497ncJ6bqmQW0lJh7V+wNe1zV1Wpx1/O//eEQFC+bXC/95XUWLFdGSb6fp0or1dfLkSR+qzdzJUIrfJaSrQYM6SkxK0oABb6pO7ebpnhMVFaUJE4fq+IkTGjrkM40dO8XjKs9d4tGtxsvnq1Kijme/a9fuXezJa8sxCUqd2rW0efM2bd26QydPntSoUePUqmWzNOf81pxIUly+fGk+2yAmJkZ58+ZRdHS08uXNq927f/KsdpcdTTomSYqJjVFMbMxZnxfRqt3Nmjbxa+3+8dT7GdTmJKj27tmnVSvXSpKSEo9q44YtKhWftmGtfGlFzf1moSRp88atKnthGRUrXtTzWl02f94SHTx4KMPj1lrlL5Bf0qk/dA4e/EXJyckeVRcZ5s1brIMHDmV6zsOP3Kdx477Uz3v3e1OUQ6yH//NKjmlQSpcppR927jp9e+ePu1W6dKmzzmvdurlWff+Nxo8boi5dnpAk7dr1k3r3eV9bNy/Wzh3L9cvhw5r+1WzPandZVFSUJswcqcVrv9K8WYv03bdpP3W7QsWLVPCCgho+LkHjZgzXbXfe4lOl7itbrrSqVq+i5ctWprl/zar1urnlDZKkmldWVdly8YovHdzUzUUDPhiqSy6tqLWb5mveokl6+skegfnwtkgRX7qkWrVqpoEDhvtdCjxy3g2KMebjcBQSbqkf/ZxGej9Axo37UlWrXad2t3fWiy/8U5J0wQWF1KplM1W6pJ7KXXSl4uLy6Z572oa95kgQCoXU8vr2alC9uWpceYUuuaximuPRMdGqWqOKHmr/mO6/41F1+0cXla94oU/VuitfXF4lDOmjF555VYlHktIc6993oApdUFBTv/lcD3TpoFUr1yk5OZgxv6ua3NBQ369cqyqVrlGja1rptTefV4HURAXZ47XX/qPnnu2lUCjkdynwSKbLjI0x439/l6TrjTEXSJK1tlUGX9dVUldJMtGFFBUV9+cr/ZN+3Llb5cqWPn27bJl47d69J8Pz58xdpIsvvkhFixZW48YNtHXbDu3bd0CSNGbsFNWvd7VGjBgd9rojxZHDiVo4b5kaNb1GG9ZtPn3/T7v26OCBQzp29LiOHT2uxfO/VZUrLtG2zTt8rNYtMTExShjylsZ8PklTJn511vHEI0l6ottzp28vWDFVP+zY6WWJEa9Dx3Z6q/cHkqStW7Zr+/adqnzJxfr2d2kW/rhaV1bT4I/7SZKKFi2sZs0aKzklWRMnTPe5smDIiRu1lZV0WFJvSW+mXo6ccT1d1toEa+3V1tqrg9CcSNKSpStUqVIFlS9fTrGxsbrzztaaMHFamnMqVix/+nqtmlWVK1es9u8/qB92/Ki6da9U3rx5JElNrr9W69ZtFDJXpOgFKlDw1F+RufPkVoNGdbV547Y053w15RvVrldL0dHRypM3j2peVVWbN2z1oVp3vfH2S9q0YYsGvJt+uFmwYAHFxp76W+See9tp0fxlZ6Us+HN27tylRo2vkSQVL1FUlSpX0LZtP/hcVWSpenkjXVGloa6o0lBjx0zR/3X/D81JhMtqo7arJT0u6d+S/mmtXWGMOWat/Sb8pWWvlJQUPd79WU2eNELRUVEaPORTrVmzQV27dJIkJQwYqra3tVDHjrfr5MlkHT92XPd0eESStHjJco0ePUlLFk9VcnKyVqxYrQEDGQfNSvGSxfX6Oy8qOjpaUVFGk8ZN18xpc9T+/lNLMEcO/kKbN27V7K/na9LsT2VDIX06bGyahAWZq123lm6/u5XWrt6gqd98Lkl6tUdflS4bL0kaNniUKl16sfq+21MpKSnauH6L/vHYf/ws2UkDB/VRg4Z1VbRoYa1aP1e9Xu57uukb9OFIvd6rv/p/8JrmLZokY4xefO51HdjPhO/zMWhwXzVsVE9FixbW+o3z9fJ/3zr9Hn84cITP1QVfJG7Udk7LjI0xZSX1kbRHUitr7TlPEgjKMuNIFuRlxpEkiMuMI01QlxlHkqAuM440Xi8zrlz8Ks9+1278eZknr+2ctrq31u6UdIcx5hadGvIBAAABEYlzUM7rs3istZMkTQpTLQAAAJL4sEAAAJwXiXNQcsxGbQAAwB0kKAAAOM7ayNvAjgQFAAAEDgkKAACOCzEHBQAAIPxIUAAAcFwkfno2CQoAAAgcEhQAABzHHBQAAAAP0KAAAIDAYYgHAADHMUkWAADAAyQoAAA4LkSCAgAAEH4kKAAAOM6yzBgAACD8SFAAAHAcq3gAAAA8QIICAIDj2OoeAADAAyQoAAA4jjkoAAAAHiBBAQDAcewkCwAA4AESFAAAHMccFAAAAA/QoAAAgMBhiAcAAMexURsAAIAHSFAAAHAck2QBAAA8QIICAIDj2KgNAADAAyQoAAA4zrKKBwAAIPxIUAAAcBxzUAAAADxAggIAgOPYBwUAAMADJCgAADiOVTwAAAAeIEEBAMBxzEEBAADwAA0KAAAIHIZ4AABwHEM8AAAAHiBBAQDAcZGXn5CgAACAADKROG71ZxljulprE/yuI5LxHocf77E3eJ/Dj/c4ZyJBSV9XvwvIAXiPw4/32Bu8z+HHe5wD0aAAAIDAoUEBAACBQ4OSPsY6w4/3OPx4j73B+xx+vMc5EJNkAQBA4JCgAACAwKFBOYMxprkxZr0xZpMx5im/64lExpiPjDF7jTGr/K4lUhljyhljZhpj1hpjVhtjHve7pkhjjMljjFlsjPku9T1+0e+aIpUxJtoYs9wYM9HvWuAtGpRUxphoSf0l3SzpckntjTGX+1tVRBosqbnfRUS4ZElPWGurSKon6VG+l7PdCUlNrLU1JNWU1NwYU8/fkiLW45LW+l0EvEeD8j91JG2y1m6x1v4q6RNJrX2uKeJYa2dLOuB3HZHMWrvbWvtt6vUjOvXDvYy/VUUWe0pi6s3Y1AsT+rKZMaaspFskDfS7FniPBuV/ykj64YzbO8UPdTjOGFNeUi1Ji3wuJeKkDj2skLRX0nRrLe9x9ntL0pOSQj7XAR/QoPyPSec+/iKCs4wx+SV9Iam7tfaw3/VEGmttirW2pqSykuoYY6r6XFJEMcbcKmmvtXaZ37XAHzQo/7NTUrkzbpeVtMunWoA/xRgTq1PNyXBr7Wi/64lk1tpDkmaJuVXZrYGkVsaYbTo15N7EGDPM35LgJRqU/1kiqbIxpoIxJpekuyWN97km4LwZY4ykDyWttdb29rueSGSMKW6MuSD1el5JN0ha52tREcZa+7S1tqy1trxO/Tz+2lrb0eey4CEalFTW2mRJ3SRN1alJhaOstav9rSryGGNGSlog6VJjzE5jTGe/a4pADSR10qm/OFekXlr4XVSEiZc00xizUqf+uJlurWUZLJCN2EkWAAAEDgkKAAAIHBoUAAAQODQoAAAgcGhQAABA4NCgAACAwKFBAQAAgUODAgAAAocGBQAABM7/A8EXkWRqBWWBAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *100, index = range(0, 5),\n",
    "                     columns = range(0, 5))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model precision is:  0.2480342783103116\n",
      "Model f1_score is:  0.18742417181992377\n",
      "Model recall is:  0.20167054867149292\n",
      "Model MSE is:  2.2856164383561643\n",
      "Confusion Matrix\n",
      "[[ 129  154  122   34   51]\n",
      " [  12   64   33   43   52]\n",
      " [  91 1011  175  114   85]\n",
      " [  11  142  166   79   59]\n",
      " [  11  104   85   52   41]]\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "#metrics\n",
    "precision = sklearn.metrics.precision_score(y_true,y_pred, average='macro')\n",
    "print('Model precision is: ', precision)\n",
    "\n",
    "f1_score = sklearn.metrics.f1_score(y_true,y_pred, average='macro')\n",
    "print('Model f1_score is: ', f1_score)\n",
    "\n",
    "recall = sklearn.metrics.recall_score(y_true,y_pred, average='macro')\n",
    "print('Model recall is: ', recall)\n",
    "\n",
    "MSE = sklearn.metrics.mean_squared_error(y_true,y_pred)\n",
    "print('Model MSE is: ', MSE)\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(cf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ML_env_pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2107e4a676a391af164907120f93cd287ed2c419d0c1d2ead0dc1b99f4b7ffc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
