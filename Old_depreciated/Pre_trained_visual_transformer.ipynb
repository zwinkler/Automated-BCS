{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image as pil_image\n",
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import AutoFeatureExtractor\n",
    "from transformers import TFAutoModelForTokenClassification\n",
    "from transformers import TFAutoModelForImageClassification\n",
    "from keras.layers import Input\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing TFViTForImageClassification: ['vit/pooler/dense/kernel:0', 'vit/pooler/dense/bias:0']\n",
      "- This IS expected if you are initializing TFViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, TFAutoModelForImageClassification, AutoFeatureExtractor\n",
    "\n",
    "# Download model and configuration from huggingface.co and cache.\n",
    "config = AutoConfig.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=5)\n",
    "model = TFAutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=5)\n",
    "#feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=5)\n",
    "\n",
    "# image = imageio.imread('DGE_test\\\\2\\Cow_11\\\\100_DGE.tif')\n",
    "# inputs = feature_extractor(images=image, return_tensors=\"tf\")\n",
    "\n",
    "# def preprocessing_func():\n",
    "#     return feature_extractor(images=image, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import skimage.transform\n",
    "import PIL\n",
    "\n",
    "class ViTDataGen(Sequence):\n",
    "    # The input to the data generator will be the dataframe and which columns to use\n",
    "    def __init__(self, df, X_col, y_col,\n",
    "                 directory,\n",
    "                 batch_size,\n",
    "                 input_size=(224, 224, 3), #changed from (3,224,224)\n",
    "                 target_size=None,\n",
    "                 bitdepth=None,\n",
    "                 shuffle=True):\n",
    "        \n",
    "        self.df = df.copy() # dataframe\n",
    "        self.X_col = X_col # column for X data (filename)\n",
    "        self.y_col = y_col # column for y data (class label)\n",
    "        self.directory = directory # base directory for data\n",
    "        self.batch_size = batch_size # batch size\n",
    "        self.input_size = input_size # size expected by network (224,224,3) for VGG\n",
    "        self.target_size = target_size # resized image for spatial res sims\n",
    "        self.bitdepth = bitdepth # quantized image for bitdepth sims\n",
    "        self.shuffle = shuffle # whether to shuffle batches\n",
    "        \n",
    "        self.n = len(self.df) # number of data points\n",
    "        self.nclasses = df[y_col].nunique() # number of classes\n",
    "            \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    def __get_input(self, path, directory, input_size, target_size, bitdepth):\n",
    "        \n",
    "        img = imageio.imread(directory+path)\n",
    "    \n",
    "        # with fits.open(directory+path) as img: # read in fits image\n",
    "        #     img.verify('silentfix')\n",
    "        #     img = img[0].data\n",
    "            \n",
    "        #img = np.expand_dims(img,axis=2) # copy single channel to three to create rgb dimensioned image\n",
    "        #img = np.tile(img,(1,1,3))\n",
    "        \n",
    "        # scale to target_size\n",
    "        if target_size is not None:\n",
    "            img = skimage.transform.resize(img, (target_size[0],target_size[1]), order=1, mode='reflect',\\\n",
    "                                           clip=True, preserve_range=True, anti_aliasing=True)\n",
    "        \n",
    "        # scale to input_size (expected dimensions for input to network)\n",
    "        img = skimage.transform.resize(img, (input_size[0],input_size[1]), order=1, mode='reflect',\\\n",
    "                                       clip=True, preserve_range=True, anti_aliasing=True)\n",
    "        \n",
    "        # put bitdepth stuff here eventually\n",
    "        \n",
    "        # scale intensities to range [0,255] as expected by VGG preprocessing function\n",
    "        # can cheat a bit here and treat each channel the same since these are grayscale images\n",
    "        # img = img + 5978.7 # -5978.7 is minimum of entire magnetogram dataset\n",
    "        # img = img/(2*5978.7)*255 # +5978.7 is maximum of entire magnetogram dataset\n",
    "        # img[img<-2550] = -2550\n",
    "        # img = img + 2550 # -5978.7 is minimum of entire magnetogram dataset\n",
    "        # img = img/(5100)*255 # +5978.7 is maximum of entire magnetogram dataset\n",
    "        \n",
    "        \n",
    "        \n",
    "        img = feature_extractor(img, return_tensor=\"tf\") # preprocess according to ViT expectations\n",
    "\n",
    "        return img\n",
    "    \n",
    "    \n",
    "    def __get_output(self, label, num_classes):\n",
    "        #print(label)\n",
    "        #print(label, num_classes)\n",
    "        return tensorflow.keras.utils.to_categorical(label, num_classes=num_classes)\n",
    "    \n",
    "    def __get_data(self, batches):\n",
    "        # Generates data containing batch_size samples\n",
    "\n",
    "        path_batch = batches[self.X_col]\n",
    "        \n",
    "        label_batch = batches[self.y_col]\n",
    "\n",
    "        X_batch = np.asarray([self.__get_input(x, self.directory, self.input_size, self.target_size, self.bitdepth)\\\n",
    "                              for x in path_batch])\n",
    "\n",
    "        y_batch = np.asarray([self.__get_output(y, self.nclasses) for y in label_batch])\n",
    "        #print(y_batch)\n",
    "        \n",
    "        return X_batch, y_batch\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        batches = self.df[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        X, y = self.__get_data(batches)        \n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_vi_t_for_image_classification_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vit (TFViTMainLayer)        multiple                  85798656  \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  3845      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85,802,501\n",
      "Trainable params: 85,802,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('train_data.csv',dtype=str)\n",
    "# val_df = pd.read_csv('val_data.csv',dtype=str)\n",
    "# test_df = pd.read_csv('test_data.csv',dtype=str)\n",
    "\n",
    "# train_df = np.expand_dims(np.asarray(train_df['class']).astype(int), axis=1)-2\n",
    "# val_df = np.expand_dims(np.asarray(val_df['class']).astype(int), axis=1)-2\n",
    "# test_df = np.expand_dims(np.asarray(test_df['class']).astype(int), axis=1)-2\n",
    "\n",
    "# train_generator = ViTDataGen(train_df, X_col='filename', y_col='class', directory='',\\\n",
    "#                               batch_size=128, input_size=(224,224,3),\\\n",
    "#                            target_size=None, bitdepth=None, shuffle=True)\n",
    "# val_generator = ViTDataGen(val_df, X_col='filename', y_col='class', directory='',\\\n",
    "#                             batch_size=128, input_size=(224,224,3),\\\n",
    "#                            target_size=None, bitdepth=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-d277bce06358>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mtrain_datagen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfill_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"reflect\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"channels_last\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m train_generator = train_datagen.flow_from_dataframe(dataframe=train_df,\\\n\u001b[0m\u001b[0;32m     17\u001b[0m                                                     \u001b[0mdirectory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocessing_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                                                     \u001b[0mxcol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'filename'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36mflow_from_dataframe\u001b[1;34m(self, dataframe, directory, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, subset, interpolation, validate_filenames, **kwargs)\u001b[0m\n\u001b[0;32m   1115\u001b[0m           DeprecationWarning)\n\u001b[0;32m   1116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1117\u001b[1;33m     return DataFrameIterator(\n\u001b[0m\u001b[0;32m   1118\u001b[0m         \u001b[0mdataframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataframe, directory, image_data_generator, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, subset, interpolation, dtype, validate_filenames)\u001b[0m\n\u001b[0;32m    569\u001b[0m       \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m       validate_filenames=True):\n\u001b[1;32m--> 571\u001b[1;33m     super(DataFrameIterator, self).__init__(\n\u001b[0m\u001b[0;32m    572\u001b[0m         \u001b[0mdataframe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras_preprocessing\\image\\dataframe_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataframe, directory, image_data_generator, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, subset, interpolation, dtype, validate_filenames)\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;31m# check that inputs match the required class_mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidate_filenames\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# check which image files are valid and keep them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filter_valid_filepaths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_col\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras_preprocessing\\image\\dataframe_iterator.py\u001b[0m in \u001b[0;36m_check_params\u001b[1;34m(self, df, x_col, y_col, weight_col, classes)\u001b[0m\n\u001b[0;32m    188\u001b[0m             )\n\u001b[0;32m    189\u001b[0m         \u001b[1;31m# check that filenames/filepaths column values are all strings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_col\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m             raise TypeError('All values in column x_col={} must be strings.'\n\u001b[0;32m    192\u001b[0m                             .format(x_col))\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# train_df = pd.read_csv('train_data.csv',dtype=str)\n",
    "# val_df = pd.read_csv('val_data.csv',dtype=str)\n",
    "# test_df = pd.read_csv('test_data.csv',dtype=str)\n",
    "\n",
    "# # train_df[\"class\"] = train_df[\"class\"].astype(str).astype(int)\n",
    "# # val_df[\"class\"] = val_df[\"class\"].astype(str).astype(int)\n",
    "# # test_df[\"class\"] = test_df[\"class\"].astype(str).astype(int)\n",
    "\n",
    "# train_df = np.expand_dims(np.asarray(train_df['class']).astype(int), axis=1)-2\n",
    "# val_df = np.expand_dims(np.asarray(val_df['class']).astype(int), axis=1)-2\n",
    "# test_df = np.expand_dims(np.asarray(test_df['class']).astype(int), axis=1)-2\n",
    "\n",
    "# train_datagen = ImageDataGenerator(fill_mode = \"reflect\", data_format = \"channels_last\")\n",
    "# train_generator = train_datagen.flow_from_dataframe(dataframe=train_df,\\\n",
    "#                                                     directory='', preprocessing_function=feature_extractor,\\\n",
    "#                                                     xcol='filename',y_col='class',\\\n",
    "#                                                     target_size=(224,224), color_mode='rgb',\\\n",
    "#                                                     batch_size=8, class_mode='raw',\\\n",
    "#                                                     shuffle=True)\n",
    "# val_datagen = ImageDataGenerator(fill_mode = \"reflect\", data_format = \"channels_last\")\n",
    "# val_generator = val_datagen.flow_from_dataframe(dataframe=val_df,\\\n",
    "#                                                 directory = '', preprocessing_function=feature_extractor,\\\n",
    "#                                                 xcol='filename',ycol='class',\\\n",
    "#                                                 target_size=(224,224), color_mode='rgb',\\\n",
    "#                                                 batch_size=8, class_mode='raw',\\\n",
    "#                                                 shuffle=True)\n",
    "# test_datagen = ImageDataGenerator(fill_mode = \"reflect\", data_format = \"channels_last\")\n",
    "# test_generator = test_datagen.flow_from_dataframe(dataframe=test_df,\\\n",
    "#                                                 directory = '', preprocessing_function=feature_extractor,\\\n",
    "#                                                 xcol='filename',ycol='class',\\\n",
    "#                                                 target_size=(224,224), color_mode='rgb',\\\n",
    "#                                                 batch_size=8, class_mode='raw',\\\n",
    "#                                                 shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.transform\n",
    "\n",
    "train_df = pd.read_csv('train_data.csv',dtype=str)\n",
    "val_df = pd.read_csv('val_data.csv',dtype=str)\n",
    "test_df = pd.read_csv('test_data.csv',dtype=str)\n",
    "\n",
    "def read_data(data_frame):\n",
    "    num_imgs_train = len(train_df['class'])\n",
    "    x_train = np.zeros((num_imgs_train, 224, 224, 3))\n",
    "\n",
    "    img_names = list(train_df['filename'])\n",
    "    y_train = np.expand_dims(np.asarray(train_df['class']).astype(int), axis=1)-2\n",
    "\n",
    "\n",
    "    for i in range(0,num_imgs_train):\n",
    "        img = imageio.imread(img_names[i])\n",
    "        x_train[i, :, :, :] = skimage.transform.resize(img, (224, 224, 3))\n",
    "    return x_train, y_train\n",
    "\n",
    "x_train, y_train = read_data(train_df)\n",
    "x_val, y_val = read_data(val_df)\n",
    "x_test, y_test = read_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 996, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"tf_vi_t_for_image_classification_3\" (type TFViTForImageClassification).\n    \n    in user code:\n    \n        File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 759, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\transformers\\models\\vit\\modeling_tf_vit.py\", line 789, in call  *\n            outputs = self.vit(\n        File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        ValueError: Exception encountered when calling layer \"vit\" (type TFViTMainLayer).\n        \n        in user code:\n        \n            File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 759, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\transformers\\models\\vit\\modeling_tf_vit.py\", line 494, in call  *\n                embedding_output = self.embeddings(\n            File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n        \n            ValueError: Exception encountered when calling layer \"embeddings\" (type TFViTEmbeddings).\n            \n            in user code:\n            \n                File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\transformers\\models\\vit\\modeling_tf_vit.py\", line 123, in call  *\n                    embeddings = self.patch_embeddings(\n                File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n            \n                ValueError: Exception encountered when calling layer \"patch_embeddings\" (type TFPatchEmbeddings).\n                \n                in user code:\n                \n                    File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\transformers\\models\\vit\\modeling_tf_vit.py\", line 189, in call  *\n                        projection = self.projection(pixel_values)\n                    File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n                        raise e.with_traceback(filtered_tb) from None\n                    File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 247, in assert_input_compatibility\n                        raise ValueError(\n                \n                    ValueError: Input 0 of layer \"projection\" is incompatible with the layer: expected axis -1of input shape to have value 3, but received input with shape (None, 224, 3, 224)\n                \n                \n                Call arguments received:\n                  • pixel_values=tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)\n                  • interpolate_pos_encoding=None\n                  • training=True\n            \n            \n            Call arguments received:\n              • pixel_values=tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)\n              • interpolate_pos_encoding=None\n              • training=True\n        \n        \n        Call arguments received:\n          • self=tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)\n          • pixel_values=None\n          • head_mask=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • interpolate_pos_encoding=None\n          • return_dict=True\n          • training=True\n    \n    \n    Call arguments received:\n      • self=tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)\n      • pixel_values=None\n      • head_mask=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • interpolate_pos_encoding=None\n      • return_dict=None\n      • labels=None\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-5d7762d5088f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m history = model.fit(\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 996, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"tf_vi_t_for_image_classification_3\" (type TFViTForImageClassification).\n    \n    in user code:\n    \n        File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 759, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\transformers\\models\\vit\\modeling_tf_vit.py\", line 789, in call  *\n            outputs = self.vit(\n        File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        ValueError: Exception encountered when calling layer \"vit\" (type TFViTMainLayer).\n        \n        in user code:\n        \n            File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 759, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\transformers\\models\\vit\\modeling_tf_vit.py\", line 494, in call  *\n                embedding_output = self.embeddings(\n            File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n        \n            ValueError: Exception encountered when calling layer \"embeddings\" (type TFViTEmbeddings).\n            \n            in user code:\n            \n                File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\transformers\\models\\vit\\modeling_tf_vit.py\", line 123, in call  *\n                    embeddings = self.patch_embeddings(\n                File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n            \n                ValueError: Exception encountered when calling layer \"patch_embeddings\" (type TFPatchEmbeddings).\n                \n                in user code:\n                \n                    File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\transformers\\models\\vit\\modeling_tf_vit.py\", line 189, in call  *\n                        projection = self.projection(pixel_values)\n                    File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n                        raise e.with_traceback(filtered_tb) from None\n                    File \"c:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 247, in assert_input_compatibility\n                        raise ValueError(\n                \n                    ValueError: Input 0 of layer \"projection\" is incompatible with the layer: expected axis -1of input shape to have value 3, but received input with shape (None, 224, 3, 224)\n                \n                \n                Call arguments received:\n                  • pixel_values=tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)\n                  • interpolate_pos_encoding=None\n                  • training=True\n            \n            \n            Call arguments received:\n              • pixel_values=tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)\n              • interpolate_pos_encoding=None\n              • training=True\n        \n        \n        Call arguments received:\n          • self=tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)\n          • pixel_values=None\n          • head_mask=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • interpolate_pos_encoding=None\n          • return_dict=True\n          • training=True\n    \n    \n    Call arguments received:\n      • self=tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)\n      • pixel_values=None\n      • head_mask=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • interpolate_pos_encoding=None\n      • return_dict=None\n      • labels=None\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.utils import class_weight \n",
    "# class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(train_df['class']), y =train_df['class'])\n",
    "# class_weights = dict(enumerate(class_weights))\n",
    "# print(class_weights)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-7),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), #had from_logits=True but im not sure it actually needs that? #was sparse\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy(), #was sparse\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        steps_per_epoch = np.ceil(len(y_train)/batch_size),\n",
    "        batch_size=batch_size,\n",
    "        epochs=2,\n",
    "        validation_data = (x_val, y_val),\n",
    "        validation_steps = np.ceil(len(y_val)/batch_size),\n",
    "        validation_freq=1\n",
    "    )\n",
    "\n",
    "\n",
    "# step_size_train = np.ceil(train_generator.n/train_generator.batch_size)\n",
    "# step_size_val = np.ceil(val_generator.n/val_generator.batch_size)\n",
    "\n",
    "\n",
    "# history = model.fit(\n",
    "#     train_generator, steps_per_epoch=step_size_train, epochs=2, verbose=1,\\\n",
    "#     validation_data=val_generator, validation_steps=step_size_val,\\\n",
    "#     validation_freq=1 #, class_weight=class_weights #class weights break everything and for some reaosn it thinks there are still 10 classes?\n",
    "#     )\n",
    "# history = model.fit(\n",
    "#     train_generator, epochs=1, verbose=1,\\\n",
    "#     validation_data=val_generator\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import requests\n",
    "# from transformers import CLIPProcessor, TFCLIPVisionModel\n",
    "\n",
    "# model = TFCLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ML_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "851288773b5edbc54f631fdf579cbc9e2fc2ac5bd43b2d347da3c29f51b44049"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
