{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG Transfer Learning for Solar Flare Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image as pil_image\n",
    "import imageio\n",
    "import tensorflow\n",
    "import sklearn.metrics\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "#plt.rcParams.update({'font.size': 32})\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_params(model):\n",
    "    total_params = 0 # initialize counter for total params\n",
    "    trainable_params = 0 # initialize counter for trainable params\n",
    "    print('Layer Name\\t\\tType\\t\\tFilter shape\\t\\t# Parameters\\tTrainable') # print column headings\n",
    "    for layer in model.layers: # loop over layers\n",
    "        lname = layer.name # grab layer name\n",
    "        ltype = type(layer).__name__ # grab layer type\n",
    "        ltype[ltype.find('/'):] # parse for only the last part of the string\n",
    "        if ltype=='Conv2D': # print for convolutional layers\n",
    "            weights = layer.get_weights()\n",
    "            print(lname+'\\t\\t'+ltype+'\\t\\t'+str(weights[0].shape)+'\\t\\t'+\\\n",
    "                  str(layer.count_params())+'\\t'+str(layer.trainable))\n",
    "            if layer.trainable:\n",
    "                trainable_params += layer.count_params()\n",
    "            total_params += layer.count_params() # update number of params\n",
    "        elif ltype=='MaxPooling2D': # print for max pool layers\n",
    "            weights = layer.get_weights()\n",
    "            print(lname+'\\t\\t'+ltype+'\\t---------------\\t\\t---')\n",
    "        elif ltype=='Flatten': # print for flatten layers\n",
    "            print(lname+'\\t\\t'+ltype+'\\t\\t---------------\\t\\t---')\n",
    "        elif ltype=='Dense': # print for dense layers\n",
    "            weights = layer.get_weights()\n",
    "            print(lname+'\\t\\t\\t'+ltype+'\\t\\t'+str(weights[0].shape)+'\\t\\t'+\\\n",
    "                  str(layer.count_params())+'\\t'+str(layer.trainable))\n",
    "            if layer.trainable:\n",
    "                trainable_params += layer.count_params()\n",
    "            total_params += layer.count_params() # update number of params\n",
    "        elif ltype=='Dropout': # print for dropout layers\n",
    "            print(lname+'\\t\\t'+ltype+'\\t\\t------------------\\t---')\n",
    "    print('---------------')\n",
    "    print('Total trainable parameters: '+str(trainable_params)) # print total params\n",
    "    print('Total untrainable parameters: '+str(total_params-trainable_params))\n",
    "    print('Total parameters: '+str(total_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shapes(model):\n",
    "    print('Layer Name\\t\\tType\\t\\tInput Shape\\t\\tOutput Shape\\tTrainable')# print column headings\n",
    "    for layer in model.layers:  # loop over layers\n",
    "        lname = layer.name # grab layer name\n",
    "        ltype = type(layer).__name__ # grab layer type\n",
    "        ltype[ltype.find('/'):] # parse for only the last part of the string\n",
    "        if ltype=='Conv2D': # print for convolutional layers\n",
    "            print(lname+'\\t\\t'+ltype+'\\t\\t'+str(layer.input_shape)+'\\t'+\\\n",
    "                  str(layer.output_shape)+'\\t'+str(layer.trainable))\n",
    "        elif ltype=='MaxPooling2D': # print for maxpool layers\n",
    "            print(lname+'\\t\\t'+ltype+'\\t'+str(layer.input_shape)+'\\t'+\\\n",
    "                  str(layer.output_shape))\n",
    "        elif ltype=='Flatten': # print for flatten layers\n",
    "            print(lname+'\\t\\t'+ltype+'\\t\\t'+str(layer.input_shape)+'\\t'+\\\n",
    "                  str(layer.output_shape))\n",
    "        elif ltype=='Dense': # print for dense layers\n",
    "            print(lname+'\\t\\t\\t'+ltype+'\\t\\t'+str(layer.input_shape)+'\\t\\t'+\\\n",
    "                  str(layer.output_shape)+'\\t'+str(layer.trainable))\n",
    "        elif ltype=='Dropout': # print for dropout layers\n",
    "            print(lname+'\\t\\t'+ltype+'\\t\\t'+str(layer.input_shape)+'\\t'+\\\n",
    "                  str(layer.output_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input\n",
    "from astropy.io import fits\n",
    "import skimage.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input\n",
    "import keras.applications\n",
    "input_tensor = Input(shape=(224,224,3))\n",
    "model1 = keras.applications.vgg16.VGG16(include_top=False,weights='imagenet',input_tensor=input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "new_output = model1.output # take the output as currently defined\n",
    "new_output = Flatten()(new_output)\n",
    "#new_output = Dropout(0.5)(new_output)\n",
    "new_output = Dense(256,activation='relu')(new_output)\n",
    "new_output = Dropout(0.5)(new_output)\n",
    "new_output = Dense(256,activation='relu')(new_output)\n",
    "new_output = Dropout(0.5)(new_output)\n",
    "#new_output = Dense(5,activation='softmax')(new_output) # this is for classification\n",
    "new_output = Dense(1,activation='linear')(new_output)\n",
    "model2= Model(inputs=model1.input,outputs=new_output) # define a new model with the new output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name\t\tType\t\tFilter shape\t\t# Parameters\tTrainable\n",
      "block1_conv1\t\tConv2D\t\t(3, 3, 3, 64)\t\t1792\tTrue\n",
      "block1_conv2\t\tConv2D\t\t(3, 3, 64, 64)\t\t36928\tTrue\n",
      "block1_pool\t\tMaxPooling2D\t---------------\t\t---\n",
      "block2_conv1\t\tConv2D\t\t(3, 3, 64, 128)\t\t73856\tTrue\n",
      "block2_conv2\t\tConv2D\t\t(3, 3, 128, 128)\t\t147584\tTrue\n",
      "block2_pool\t\tMaxPooling2D\t---------------\t\t---\n",
      "block3_conv1\t\tConv2D\t\t(3, 3, 128, 256)\t\t295168\tTrue\n",
      "block3_conv2\t\tConv2D\t\t(3, 3, 256, 256)\t\t590080\tTrue\n",
      "block3_conv3\t\tConv2D\t\t(3, 3, 256, 256)\t\t590080\tTrue\n",
      "block3_pool\t\tMaxPooling2D\t---------------\t\t---\n",
      "block4_conv1\t\tConv2D\t\t(3, 3, 256, 512)\t\t1180160\tTrue\n",
      "block4_conv2\t\tConv2D\t\t(3, 3, 512, 512)\t\t2359808\tTrue\n",
      "block4_conv3\t\tConv2D\t\t(3, 3, 512, 512)\t\t2359808\tTrue\n",
      "block4_pool\t\tMaxPooling2D\t---------------\t\t---\n",
      "block5_conv1\t\tConv2D\t\t(3, 3, 512, 512)\t\t2359808\tTrue\n",
      "block5_conv2\t\tConv2D\t\t(3, 3, 512, 512)\t\t2359808\tTrue\n",
      "block5_conv3\t\tConv2D\t\t(3, 3, 512, 512)\t\t2359808\tTrue\n",
      "block5_pool\t\tMaxPooling2D\t---------------\t\t---\n",
      "flatten\t\tFlatten\t\t---------------\t\t---\n",
      "dense\t\t\tDense\t\t(25088, 256)\t\t6422784\tTrue\n",
      "dropout\t\tDropout\t\t------------------\t---\n",
      "dense_1\t\t\tDense\t\t(256, 256)\t\t65792\tTrue\n",
      "dropout_1\t\tDropout\t\t------------------\t---\n",
      "dense_2\t\t\tDense\t\t(256, 1)\t\t257\tTrue\n",
      "---------------\n",
      "Total trainable parameters: 21203521\n",
      "Total untrainable parameters: 0\n",
      "Total parameters: 21203521\n"
     ]
    }
   ],
   "source": [
    "print_params(model2)\n",
    "#print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model2.layers[:-10]:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name\t\tType\t\tFilter shape\t\t# Parameters\tTrainable\n",
      "block1_conv1\t\tConv2D\t\t(3, 3, 3, 64)\t\t1792\tFalse\n",
      "block1_conv2\t\tConv2D\t\t(3, 3, 64, 64)\t\t36928\tFalse\n",
      "block1_pool\t\tMaxPooling2D\t---------------\t\t---\n",
      "block2_conv1\t\tConv2D\t\t(3, 3, 64, 128)\t\t73856\tFalse\n",
      "block2_conv2\t\tConv2D\t\t(3, 3, 128, 128)\t\t147584\tFalse\n",
      "block2_pool\t\tMaxPooling2D\t---------------\t\t---\n",
      "block3_conv1\t\tConv2D\t\t(3, 3, 128, 256)\t\t295168\tFalse\n",
      "block3_conv2\t\tConv2D\t\t(3, 3, 256, 256)\t\t590080\tFalse\n",
      "block3_conv3\t\tConv2D\t\t(3, 3, 256, 256)\t\t590080\tFalse\n",
      "block3_pool\t\tMaxPooling2D\t---------------\t\t---\n",
      "block4_conv1\t\tConv2D\t\t(3, 3, 256, 512)\t\t1180160\tFalse\n",
      "block4_conv2\t\tConv2D\t\t(3, 3, 512, 512)\t\t2359808\tFalse\n",
      "block4_conv3\t\tConv2D\t\t(3, 3, 512, 512)\t\t2359808\tFalse\n",
      "block4_pool\t\tMaxPooling2D\t---------------\t\t---\n",
      "block5_conv1\t\tConv2D\t\t(3, 3, 512, 512)\t\t2359808\tTrue\n",
      "block5_conv2\t\tConv2D\t\t(3, 3, 512, 512)\t\t2359808\tTrue\n",
      "block5_conv3\t\tConv2D\t\t(3, 3, 512, 512)\t\t2359808\tTrue\n",
      "block5_pool\t\tMaxPooling2D\t---------------\t\t---\n",
      "flatten\t\tFlatten\t\t---------------\t\t---\n",
      "dense\t\t\tDense\t\t(25088, 256)\t\t6422784\tTrue\n",
      "dropout\t\tDropout\t\t------------------\t---\n",
      "dense_1\t\t\tDense\t\t(256, 256)\t\t65792\tTrue\n",
      "dropout_1\t\tDropout\t\t------------------\t---\n",
      "dense_2\t\t\tDense\t\t(256, 1)\t\t257\tTrue\n",
      "---------------\n",
      "Total trainable parameters: 13568257\n",
      "Total untrainable parameters: 7635264\n",
      "Total parameters: 21203521\n"
     ]
    }
   ],
   "source": [
    "print_params(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Data Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach taken from https://medium.com/analytics-vidhya/write-your-own-custom-data-generator-for-tensorflow-keras-1252b64e41c3\n",
    "\n",
    "From https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence: \"Sequence are a safer way to do multiprocessing. This structure guarantees that the network will only train once on each sample per epoch which is not the case with generators.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "from astropy.io import fits\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import skimage.transform\n",
    "class FitsDataGen(Sequence):\n",
    "    # The input to the data generator will be the dataframe and which columns to use\n",
    "    def __init__(self, df, X_col, y_col,\n",
    "                 directory,\n",
    "                 batch_size,\n",
    "                 input_size=(224, 224, 3),\n",
    "                 target_size=None,\n",
    "                 bitdepth=None,\n",
    "                 shuffle=True):\n",
    "        \n",
    "        self.df = df.copy() # dataframe\n",
    "        self.X_col = X_col # column for X data (filename)\n",
    "        self.y_col = y_col # column for y data (class label)\n",
    "        self.directory = directory # base directory for data\n",
    "        self.batch_size = batch_size # batch size\n",
    "        self.input_size = input_size # size expected by network (224,224,3) for VGG\n",
    "        self.target_size = target_size # resized image for spatial res sims\n",
    "        self.bitdepth = bitdepth # quantized image for bitdepth sims\n",
    "        self.shuffle = shuffle # whether to shuffle batches\n",
    "        \n",
    "        self.n = len(self.df) # number of data points\n",
    "        self.nclasses = df[y_col].nunique() # number of classes\n",
    "            \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    def __get_input(self, path, directory, input_size, target_size, bitdepth):\n",
    "    \n",
    "        with fits.open(directory+path) as img: # read in fits image\n",
    "            img.verify('silentfix')\n",
    "            img = img[0].data\n",
    "            \n",
    "        #img = np.expand_dims(img,axis=2) # copy single channel to three to create rgb dimensioned image\n",
    "        #img = np.tile(img,(1,1,3))\n",
    "        \n",
    "        # scale to target_size\n",
    "        if target_size is not None:\n",
    "            img = skimage.transform.resize(img, (target_size[0],target_size[1]), order=1, mode='reflect',\\\n",
    "                                           clip=True, preserve_range=True, anti_aliasing=True)\n",
    "        \n",
    "        # scale to input_size (expected dimensions for input to network)\n",
    "        img = skimage.transform.resize(img, (input_size[0],input_size[1]), order=1, mode='reflect',\\\n",
    "                                       clip=True, preserve_range=True, anti_aliasing=True)\n",
    "        \n",
    "        # put bitdepth stuff here eventually\n",
    "        \n",
    "        # scale intensities to range [0,255] as expected by VGG preprocessing function\n",
    "        # can cheat a bit here and treat each channel the same since these are grayscale images\n",
    "        # img = img + 5978.7 # -5978.7 is minimum of entire magnetogram dataset\n",
    "        # img = img/(2*5978.7)*255 # +5978.7 is maximum of entire magnetogram dataset\n",
    "        # img[img<-2550] = -2550\n",
    "        # img = img + 2550 # -5978.7 is minimum of entire magnetogram dataset\n",
    "        # img = img/(5100)*255 # +5978.7 is maximum of entire magnetogram dataset\n",
    "        img = img/(3500)*255 #changed to 3500 because some pixels need clipping\n",
    "        img[img>255] = 255\n",
    "        \n",
    "        \n",
    "        \n",
    "        img = preprocess_input(img) # preprocess according to VGG expectations\n",
    "\n",
    "        return img\n",
    "    \n",
    "    \n",
    "    def __get_output(self, label, num_classes):\n",
    "        #print(label)\n",
    "        print(label, num_classes)\n",
    "        return tensorflow.keras.utils.to_categorical(label, num_classes=num_classes)\n",
    "    \n",
    "    def __get_data(self, batches):\n",
    "        # Generates data containing batch_size samples\n",
    "\n",
    "        path_batch = batches[self.X_col]\n",
    "        \n",
    "        label_batch = batches[self.y_col]\n",
    "\n",
    "        X_batch = np.asarray([self.__get_input(x, self.directory, self.input_size, self.target_size, self.bitdepth)\\\n",
    "                              for x in path_batch])\n",
    "\n",
    "        y_batch = np.asarray([self.__get_output(y, self.nclasses) for y in label_batch])\n",
    "        print(y_batch)\n",
    "        \n",
    "        return X_batch, y_batch\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        batches = self.df[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        X, y = self.__get_data(batches)        \n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('train_data.csv',dtype=str)\n",
    "# val_df = pd.read_csv('val_data.csv',dtype=str)\n",
    "# test_df = pd.read_csv('test_data.csv',dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "# val_df = val_df.sample(frac=1).reset_index(drop=True)\n",
    "# test_df = test_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_generator = FitsDataGen(train_df, X_col='filename', y_col='class', directory='',\\\n",
    "#                               batch_size=128, input_size=(224,224,3),\\\n",
    "#                            target_size=None, bitdepth=None, shuffle=True)\n",
    "# val_generator = FitsDataGen(val_df, X_col='filename', y_col='class', directory='',\\\n",
    "#                             batch_size=128, input_size=(224,224,3),\\\n",
    "#                            target_size=None, bitdepth=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('classifier_VGG/Train_Data_by_AR_sr600x600.csv',dtype=str)\n",
    "# val_df = pd.read_csv('classifier_VGG/Validation_data_by_AR_sr600x600.csv',dtype=str)\n",
    "# test_df = pd.read_csv('classifier_VGG/Test_Data_by_AR_sr600x600.csv',dtype=str)\n",
    "\n",
    "train_df = pd.read_csv('train_data.csv',dtype=str)\n",
    "val_df = pd.read_csv('val_data.csv',dtype=str)\n",
    "test_df = pd.read_csv('test_data.csv',dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3893 validated image filenames belonging to 5 classes.\n",
      "Found 971 validated image filenames belonging to 5 classes.\n",
      "Found 444 validated image filenames belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "train_generator = train_datagen.flow_from_dataframe(dataframe=train_df,\\\n",
    "                                                    directory='',\\\n",
    "                                                    xcol='filename',y_col='class',\\\n",
    "                                                    target_size=(224,224), color_mode='rgb',\\\n",
    "                                                    batch_size=128, class_mode='categorical',\\\n",
    "                                                    shuffle=True) #RETURN TO TRUEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "val_generator = val_datagen.flow_from_dataframe(dataframe=val_df,\\\n",
    "                                                directory = '',\\\n",
    "                                                xcol='filename',ycol='class',\\\n",
    "                                                target_size=(224,224), color_mode='rgb',\\\n",
    "                                                batch_size=128, class_mode='categorical',\\\n",
    "                                                shuffle=True)\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_generator = test_datagen.flow_from_dataframe(dataframe=test_df,\\\n",
    "                                                directory = '',\\\n",
    "                                                xcol='filename',ycol='class',\\\n",
    "                                                target_size=(224,224), color_mode='rgb',\\\n",
    "                                                batch_size=128, class_mode='categorical',\\\n",
    "                                                shuffle=False) #SHOULD STAY FALSEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "# from keras.applications.vgg16 import preprocess_input\n",
    "# from keras.applications.vgg16 import decode_predictions\n",
    "# train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "# train_generator = train_datagen.flow_from_directory('BCS_sorted_HDV\\\\',\\\n",
    "#                                                     target_size=(224,224), color_mode='rgb',\\\n",
    "#                                                     batch_size=128, class_mode='categorical',\\\n",
    "#                                                     shuffle=True)\n",
    "# val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "# val_generator = val_datagen.flow_from_directory('BCS_sorted_HDV\\\\',\\\n",
    "#                                                     target_size=(224,224), color_mode='rgb',\\\n",
    "#                                                     batch_size=128, class_mode='categorical',\\\n",
    "#                                                     shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_tnr(y_true,y_pred):\n",
    "    import keras.backend as K\n",
    "    y_true = K.argmax(y_true)\n",
    "    y_pred = K.argmax(y_pred)\n",
    "    neg_y_true = 1 - y_true\n",
    "    neg_y_pred = 1 - y_pred\n",
    "    fp = K.cast(K.sum(neg_y_true * y_pred),'float32')\n",
    "    tn = K.cast(K.sum(neg_y_true * neg_y_pred),'float32')\n",
    "    tnr = tn / (tn + fp + K.epsilon())\n",
    "    return tnr\n",
    "\n",
    "def categorical_tpr(y_true,y_pred):\n",
    "    import keras.backend as K\n",
    "    y_true = K.argmax(y_true)\n",
    "    y_pred = K.argmax(y_pred)\n",
    "    neg_y_pred = 1 - y_pred\n",
    "    fn = K.cast(K.sum(y_true * neg_y_pred),'float32')\n",
    "    tp = K.cast(K.sum(y_true * y_pred),'float32')\n",
    "    tpr = tp / (tp + fn + K.epsilon())\n",
    "    return tpr\n",
    "\n",
    "def categorical_tss(y_true,y_pred):\n",
    "    import keras.backend as K\n",
    "    tpr = categorical_tpr(y_true,y_pred)\n",
    "    tnr = categorical_tnr(y_true,y_pred)\n",
    "    tss = tpr + tnr - 1\n",
    "    return tss\n",
    "\n",
    "def save_con_mat(y_true, y_pred):\n",
    "    import tensorflow as tf\n",
    "    print((y_true.shape))\n",
    "    print((y_pred.shape))\n",
    "    con_matrix = tf.math.confusion_matrix(y_true, y_pred)\n",
    "    # acc=np.diag(con_matrix).sum().astype(float)/con_matrix.sum()\n",
    "    return con_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "adam_opt = Adam(learning_rate=0.001)\n",
    "# model2.compile(loss='categorical_crossentropy', optimizer=adam_opt,\\\n",
    "#                metrics=['categorical_accuracy'])# save_con_mat]) #,categorical_tnr,categorical_tpr,categorical_tss])\n",
    "#for classification\n",
    "\n",
    "model2.compile(loss='mean_absolute_percentage_error', optimizer=adam_opt,\\\n",
    "               metrics=['categorical_accuracy'])# save_con_mat]) #,categorical_tnr,categorical_tpr,categorical_tss])\n",
    "               # for regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.0\n",
      "Batch size:  128\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "filepath = 'BCS_CNN\\\\BCS_CNN.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_tss', verbose=1, save_best_only=False, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_categorical_tss', min_delta=0.001, patience=5, verbose=1, mode='max')\n",
    "#callbacks_list = [checkpoint, early_stop]\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "step_size_train = np.ceil(train_generator.n/train_generator.batch_size)\n",
    "print(step_size_train)\n",
    "print('Batch size: ', train_generator.batch_size)\n",
    "#step_size_train = 2373 # to help debug val accuracy issue\n",
    "step_size_val = np.ceil(val_generator.n/val_generator.batch_size)\n",
    "print(step_size_val)\n",
    "#step_size_val = 303 # to help debug val accuracy issue\n",
    "\n",
    "# the following assumes that 0 is the majority class\n",
    "# class_weights = {0: 1.,\n",
    "#                  1: (np.asarray(train_generator.classes)==0).sum()/(np.asarray(train_generator.classes)==1).sum()}\n",
    "# the following taken from https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n",
    "#class_weights = {0: (1/(train_generator.classes==0).sum())*len(train_generator.classes)/2,\n",
    "#                 1: (1/(train_generator.classes==1).sum())*len(train_generator.classes)/2}\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 6.}\n",
    "\n",
    "from sklearn.utils import class_weight \n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(train_df['class']), y =train_df['class'])\n",
    "class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1.351736111111111, 1: 1.5857433808553971, 2: 0.4137088204038257, 3: 0.9032482598607888, 4: 9.495121951219513}\n"
     ]
    }
   ],
   "source": [
    "print(class_weights)\n",
    "# class_weights[0] = 2\n",
    "# class_weights[1] = 1.5\n",
    "# class_weights[2] = 0.2\n",
    "# class_weights[3] = 1.3\n",
    "# class_weights[4] = 7\n",
    "# print(class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 114252.7266 - categorical_accuracy: 0.1480\n",
      "Epoch 00001: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 279ms/step - loss: 114252.7266 - categorical_accuracy: 0.1480 - val_loss: 286404.0000 - val_categorical_accuracy: 0.1483\n",
      "Epoch 2/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 183739.5781 - categorical_accuracy: 0.1480\n",
      "Epoch 00002: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 281ms/step - loss: 183739.5781 - categorical_accuracy: 0.1480 - val_loss: 32433.4258 - val_categorical_accuracy: 0.1483\n",
      "Epoch 3/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 153618.9688 - categorical_accuracy: 0.1480\n",
      "Epoch 00003: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 276ms/step - loss: 153618.9688 - categorical_accuracy: 0.1480 - val_loss: 158689.0781 - val_categorical_accuracy: 0.1483\n",
      "Epoch 4/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 174650.0625 - categorical_accuracy: 0.1480\n",
      "Epoch 00004: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 174650.0625 - categorical_accuracy: 0.1480 - val_loss: 81344.3047 - val_categorical_accuracy: 0.1483\n",
      "Epoch 5/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 121412.2891 - categorical_accuracy: 0.1480\n",
      "Epoch 00005: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 282ms/step - loss: 121412.2891 - categorical_accuracy: 0.1480 - val_loss: 274759.7188 - val_categorical_accuracy: 0.1483\n",
      "Epoch 6/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 164395.0469 - categorical_accuracy: 0.1480\n",
      "Epoch 00006: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 164395.0469 - categorical_accuracy: 0.1480 - val_loss: 405735.0000 - val_categorical_accuracy: 0.1483\n",
      "Epoch 7/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 107740.2656 - categorical_accuracy: 0.1480\n",
      "Epoch 00007: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 277ms/step - loss: 107740.2656 - categorical_accuracy: 0.1480 - val_loss: 52036.6055 - val_categorical_accuracy: 0.1483\n",
      "Epoch 8/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 53985.0078 - categorical_accuracy: 0.1480\n",
      "Epoch 00008: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 273ms/step - loss: 53985.0078 - categorical_accuracy: 0.1480 - val_loss: 69132.9688 - val_categorical_accuracy: 0.1483\n",
      "Epoch 9/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 63942.3047 - categorical_accuracy: 0.1480\n",
      "Epoch 00009: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 63942.3047 - categorical_accuracy: 0.1480 - val_loss: 145742.6875 - val_categorical_accuracy: 0.1483\n",
      "Epoch 10/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 77877.4609 - categorical_accuracy: 0.1480\n",
      "Epoch 00010: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 77877.4609 - categorical_accuracy: 0.1480 - val_loss: 28760.4199 - val_categorical_accuracy: 0.1483\n",
      "Epoch 11/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 64072.2109 - categorical_accuracy: 0.1480\n",
      "Epoch 00011: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 64072.2109 - categorical_accuracy: 0.1480 - val_loss: 76979.2891 - val_categorical_accuracy: 0.1483\n",
      "Epoch 12/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 127471.1328 - categorical_accuracy: 0.1480\n",
      "Epoch 00012: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 127471.1328 - categorical_accuracy: 0.1480 - val_loss: 300904.0312 - val_categorical_accuracy: 0.1483\n",
      "Epoch 13/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 88391.1406 - categorical_accuracy: 0.1480\n",
      "Epoch 00013: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 88391.1406 - categorical_accuracy: 0.1480 - val_loss: 87032.5547 - val_categorical_accuracy: 0.1483\n",
      "Epoch 14/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 66483.8984 - categorical_accuracy: 0.1480\n",
      "Epoch 00014: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 66483.8984 - categorical_accuracy: 0.1480 - val_loss: 196580.2812 - val_categorical_accuracy: 0.1483\n",
      "Epoch 15/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 121916.9219 - categorical_accuracy: 0.1480\n",
      "Epoch 00015: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 121916.9219 - categorical_accuracy: 0.1480 - val_loss: 158714.1562 - val_categorical_accuracy: 0.1483\n",
      "Epoch 16/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 127432.6406 - categorical_accuracy: 0.1480\n",
      "Epoch 00016: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 277ms/step - loss: 127432.6406 - categorical_accuracy: 0.1480 - val_loss: 162905.3906 - val_categorical_accuracy: 0.1483\n",
      "Epoch 17/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 155628.5625 - categorical_accuracy: 0.1480\n",
      "Epoch 00017: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 155628.5625 - categorical_accuracy: 0.1480 - val_loss: 204819.7500 - val_categorical_accuracy: 0.1483\n",
      "Epoch 18/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 293221.8750 - categorical_accuracy: 0.1480\n",
      "Epoch 00018: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 293221.8750 - categorical_accuracy: 0.1480 - val_loss: 414439.9375 - val_categorical_accuracy: 0.1483\n",
      "Epoch 19/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 313986.3750 - categorical_accuracy: 0.1480\n",
      "Epoch 00019: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 277ms/step - loss: 313986.3750 - categorical_accuracy: 0.1480 - val_loss: 303965.4688 - val_categorical_accuracy: 0.1483\n",
      "Epoch 20/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 213942.8594 - categorical_accuracy: 0.1480\n",
      "Epoch 00020: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 213942.8594 - categorical_accuracy: 0.1480 - val_loss: 144937.7500 - val_categorical_accuracy: 0.1483\n",
      "Epoch 21/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 104353.8281 - categorical_accuracy: 0.1480\n",
      "Epoch 00021: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 104353.8281 - categorical_accuracy: 0.1480 - val_loss: 331343.6250 - val_categorical_accuracy: 0.1483\n",
      "Epoch 22/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 188812.8281 - categorical_accuracy: 0.1480\n",
      "Epoch 00022: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 188812.8281 - categorical_accuracy: 0.1480 - val_loss: 227524.0000 - val_categorical_accuracy: 0.1483\n",
      "Epoch 23/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 180386.4531 - categorical_accuracy: 0.1480\n",
      "Epoch 00023: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 180386.4531 - categorical_accuracy: 0.1480 - val_loss: 70594.7266 - val_categorical_accuracy: 0.1483\n",
      "Epoch 24/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 90174.7500 - categorical_accuracy: 0.1480\n",
      "Epoch 00024: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 277ms/step - loss: 90174.7500 - categorical_accuracy: 0.1480 - val_loss: 135656.4844 - val_categorical_accuracy: 0.1483\n",
      "Epoch 25/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 86892.4766 - categorical_accuracy: 0.1480\n",
      "Epoch 00025: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 86892.4766 - categorical_accuracy: 0.1480 - val_loss: 132549.3125 - val_categorical_accuracy: 0.1483\n",
      "Epoch 26/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 96595.1719 - categorical_accuracy: 0.1480\n",
      "Epoch 00026: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 96595.1719 - categorical_accuracy: 0.1480 - val_loss: 27112.8789 - val_categorical_accuracy: 0.1483\n",
      "Epoch 27/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 61248.9023 - categorical_accuracy: 0.1480\n",
      "Epoch 00027: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 276ms/step - loss: 61248.9023 - categorical_accuracy: 0.1480 - val_loss: 98075.6406 - val_categorical_accuracy: 0.1483\n",
      "Epoch 28/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 114264.5859 - categorical_accuracy: 0.1480\n",
      "Epoch 00028: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 114264.5859 - categorical_accuracy: 0.1480 - val_loss: 35928.6289 - val_categorical_accuracy: 0.1483\n",
      "Epoch 29/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 74299.4062 - categorical_accuracy: 0.1480\n",
      "Epoch 00029: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 74299.4062 - categorical_accuracy: 0.1480 - val_loss: 189395.2500 - val_categorical_accuracy: 0.1483\n",
      "Epoch 30/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 113687.7969 - categorical_accuracy: 0.1480\n",
      "Epoch 00030: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 290ms/step - loss: 113687.7969 - categorical_accuracy: 0.1480 - val_loss: 74400.5938 - val_categorical_accuracy: 0.1483\n",
      "Epoch 31/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 100746.9375 - categorical_accuracy: 0.1480\n",
      "Epoch 00031: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 10s 306ms/step - loss: 100746.9375 - categorical_accuracy: 0.1480 - val_loss: 100174.6406 - val_categorical_accuracy: 0.1483\n",
      "Epoch 32/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 64438.0898 - categorical_accuracy: 0.1480\n",
      "Epoch 00032: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 8s 241ms/step - loss: 64438.0898 - categorical_accuracy: 0.1480 - val_loss: 96676.7344 - val_categorical_accuracy: 0.1483\n",
      "Epoch 33/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 99162.5312 - categorical_accuracy: 0.1480 \n",
      "Epoch 00033: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 99162.5312 - categorical_accuracy: 0.1480 - val_loss: 109568.1641 - val_categorical_accuracy: 0.1483\n",
      "Epoch 34/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 83650.2734 - categorical_accuracy: 0.1480\n",
      "Epoch 00034: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 83650.2734 - categorical_accuracy: 0.1480 - val_loss: 159121.4531 - val_categorical_accuracy: 0.1483\n",
      "Epoch 35/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 60187.5820 - categorical_accuracy: 0.1480\n",
      "Epoch 00035: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 60187.5820 - categorical_accuracy: 0.1480 - val_loss: 44556.5820 - val_categorical_accuracy: 0.1483\n",
      "Epoch 36/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 59934.4609 - categorical_accuracy: 0.1480\n",
      "Epoch 00036: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 59934.4609 - categorical_accuracy: 0.1480 - val_loss: 3457.3167 - val_categorical_accuracy: 0.1483\n",
      "Epoch 37/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 115026.3828 - categorical_accuracy: 0.1480\n",
      "Epoch 00037: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 115026.3828 - categorical_accuracy: 0.1480 - val_loss: 69103.9453 - val_categorical_accuracy: 0.1483\n",
      "Epoch 38/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 82215.5547 - categorical_accuracy: 0.1480\n",
      "Epoch 00038: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 82215.5547 - categorical_accuracy: 0.1480 - val_loss: 76929.8594 - val_categorical_accuracy: 0.1483\n",
      "Epoch 39/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 146099.6250 - categorical_accuracy: 0.1480\n",
      "Epoch 00039: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 276ms/step - loss: 146099.6250 - categorical_accuracy: 0.1480 - val_loss: 83654.8203 - val_categorical_accuracy: 0.1483\n",
      "Epoch 40/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 166584.2969 - categorical_accuracy: 0.1480\n",
      "Epoch 00040: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 8s 272ms/step - loss: 166584.2969 - categorical_accuracy: 0.1480 - val_loss: 187537.9844 - val_categorical_accuracy: 0.1483\n",
      "Epoch 41/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 94811.3203 - categorical_accuracy: 0.1480\n",
      "Epoch 00041: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 273ms/step - loss: 94811.3203 - categorical_accuracy: 0.1480 - val_loss: 125364.7734 - val_categorical_accuracy: 0.1483\n",
      "Epoch 42/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 51655.3867 - categorical_accuracy: 0.1480\n",
      "Epoch 00042: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 272ms/step - loss: 51655.3867 - categorical_accuracy: 0.1480 - val_loss: 132888.3594 - val_categorical_accuracy: 0.1483\n",
      "Epoch 43/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 141359.3750 - categorical_accuracy: 0.1480\n",
      "Epoch 00043: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 141359.3750 - categorical_accuracy: 0.1480 - val_loss: 48441.9648 - val_categorical_accuracy: 0.1483\n",
      "Epoch 44/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 80285.7578 - categorical_accuracy: 0.1480\n",
      "Epoch 00044: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 273ms/step - loss: 80285.7578 - categorical_accuracy: 0.1480 - val_loss: 60623.5977 - val_categorical_accuracy: 0.1483\n",
      "Epoch 45/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 51799.4023 - categorical_accuracy: 0.1480\n",
      "Epoch 00045: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 8s 272ms/step - loss: 51799.4023 - categorical_accuracy: 0.1480 - val_loss: 33417.3086 - val_categorical_accuracy: 0.1483\n",
      "Epoch 46/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 104720.3750 - categorical_accuracy: 0.1480\n",
      "Epoch 00046: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 104720.3750 - categorical_accuracy: 0.1480 - val_loss: 50652.9102 - val_categorical_accuracy: 0.1483\n",
      "Epoch 47/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 63203.0156 - categorical_accuracy: 0.1480\n",
      "Epoch 00047: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 63203.0156 - categorical_accuracy: 0.1480 - val_loss: 11633.3477 - val_categorical_accuracy: 0.1483\n",
      "Epoch 48/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 103585.3984 - categorical_accuracy: 0.1480\n",
      "Epoch 00048: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 103585.3984 - categorical_accuracy: 0.1480 - val_loss: 142868.7031 - val_categorical_accuracy: 0.1483\n",
      "Epoch 49/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 159206.0000 - categorical_accuracy: 0.1480\n",
      "Epoch 00049: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 273ms/step - loss: 159206.0000 - categorical_accuracy: 0.1480 - val_loss: 55526.5352 - val_categorical_accuracy: 0.1483\n",
      "Epoch 50/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 113389.4297 - categorical_accuracy: 0.1480\n",
      "Epoch 00050: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 273ms/step - loss: 113389.4297 - categorical_accuracy: 0.1480 - val_loss: 77562.0312 - val_categorical_accuracy: 0.1483\n",
      "Epoch 51/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 102563.9844 - categorical_accuracy: 0.1480\n",
      "Epoch 00051: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 102563.9844 - categorical_accuracy: 0.1480 - val_loss: 360638.0312 - val_categorical_accuracy: 0.1483\n",
      "Epoch 52/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 145695.7812 - categorical_accuracy: 0.1480\n",
      "Epoch 00052: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 8s 272ms/step - loss: 145695.7812 - categorical_accuracy: 0.1480 - val_loss: 26139.2227 - val_categorical_accuracy: 0.1483\n",
      "Epoch 53/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 74056.0938 - categorical_accuracy: 0.1480\n",
      "Epoch 00053: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 273ms/step - loss: 74056.0938 - categorical_accuracy: 0.1480 - val_loss: 173553.7344 - val_categorical_accuracy: 0.1483\n",
      "Epoch 54/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 74859.5938 - categorical_accuracy: 0.1480\n",
      "Epoch 00054: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 273ms/step - loss: 74859.5938 - categorical_accuracy: 0.1480 - val_loss: 12343.1094 - val_categorical_accuracy: 0.1483\n",
      "Epoch 55/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 55928.3477 - categorical_accuracy: 0.1480\n",
      "Epoch 00055: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 55928.3477 - categorical_accuracy: 0.1480 - val_loss: 154981.8438 - val_categorical_accuracy: 0.1483\n",
      "Epoch 56/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 91761.1875 - categorical_accuracy: 0.1480\n",
      "Epoch 00056: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 91761.1875 - categorical_accuracy: 0.1480 - val_loss: 218348.1250 - val_categorical_accuracy: 0.1483\n",
      "Epoch 57/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 140268.1719 - categorical_accuracy: 0.1480\n",
      "Epoch 00057: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 140268.1719 - categorical_accuracy: 0.1480 - val_loss: 18737.5625 - val_categorical_accuracy: 0.1483\n",
      "Epoch 58/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 66388.8203 - categorical_accuracy: 0.1480\n",
      "Epoch 00058: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 273ms/step - loss: 66388.8203 - categorical_accuracy: 0.1480 - val_loss: 112872.8125 - val_categorical_accuracy: 0.1483\n",
      "Epoch 59/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 113320.8984 - categorical_accuracy: 0.1480\n",
      "Epoch 00059: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 273ms/step - loss: 113320.8984 - categorical_accuracy: 0.1480 - val_loss: 99549.5703 - val_categorical_accuracy: 0.1483\n",
      "Epoch 60/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 146606.3125 - categorical_accuracy: 0.1480\n",
      "Epoch 00060: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 146606.3125 - categorical_accuracy: 0.1480 - val_loss: 23408.2344 - val_categorical_accuracy: 0.1483\n",
      "Epoch 61/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 168257.5469 - categorical_accuracy: 0.1480\n",
      "Epoch 00061: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 168257.5469 - categorical_accuracy: 0.1480 - val_loss: 196193.7344 - val_categorical_accuracy: 0.1483\n",
      "Epoch 62/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 104786.7031 - categorical_accuracy: 0.1480\n",
      "Epoch 00062: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 276ms/step - loss: 104786.7031 - categorical_accuracy: 0.1480 - val_loss: 21339.4746 - val_categorical_accuracy: 0.1483\n",
      "Epoch 63/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 144533.3750 - categorical_accuracy: 0.1480\n",
      "Epoch 00063: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 276ms/step - loss: 144533.3750 - categorical_accuracy: 0.1480 - val_loss: 156891.1250 - val_categorical_accuracy: 0.1483\n",
      "Epoch 64/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 165674.8594 - categorical_accuracy: 0.1480\n",
      "Epoch 00064: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 165674.8594 - categorical_accuracy: 0.1480 - val_loss: 23845.4570 - val_categorical_accuracy: 0.1483\n",
      "Epoch 65/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 48487.0000 - categorical_accuracy: 0.1480\n",
      "Epoch 00065: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 48487.0000 - categorical_accuracy: 0.1480 - val_loss: 41995.5781 - val_categorical_accuracy: 0.1483\n",
      "Epoch 66/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 98085.7656 - categorical_accuracy: 0.1480\n",
      "Epoch 00066: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 98085.7656 - categorical_accuracy: 0.1480 - val_loss: 80590.9141 - val_categorical_accuracy: 0.1483\n",
      "Epoch 67/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 181418.8594 - categorical_accuracy: 0.1480\n",
      "Epoch 00067: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 273ms/step - loss: 181418.8594 - categorical_accuracy: 0.1480 - val_loss: 98119.8750 - val_categorical_accuracy: 0.1483\n",
      "Epoch 68/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 162610.5469 - categorical_accuracy: 0.1480\n",
      "Epoch 00068: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 273ms/step - loss: 162610.5469 - categorical_accuracy: 0.1480 - val_loss: 86470.2812 - val_categorical_accuracy: 0.1483\n",
      "Epoch 69/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 224056.2656 - categorical_accuracy: 0.1480\n",
      "Epoch 00069: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 224056.2656 - categorical_accuracy: 0.1480 - val_loss: 158319.6719 - val_categorical_accuracy: 0.1483\n",
      "Epoch 70/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 186492.1875 - categorical_accuracy: 0.1480\n",
      "Epoch 00070: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 186492.1875 - categorical_accuracy: 0.1480 - val_loss: 377719.5312 - val_categorical_accuracy: 0.1483\n",
      "Epoch 71/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 222212.2969 - categorical_accuracy: 0.1480\n",
      "Epoch 00071: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 272ms/step - loss: 222212.2969 - categorical_accuracy: 0.1480 - val_loss: 151550.1406 - val_categorical_accuracy: 0.1483\n",
      "Epoch 72/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 81580.5234 - categorical_accuracy: 0.1480\n",
      "Epoch 00072: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 272ms/step - loss: 81580.5234 - categorical_accuracy: 0.1480 - val_loss: 33285.4688 - val_categorical_accuracy: 0.1483\n",
      "Epoch 73/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 99580.9141 - categorical_accuracy: 0.1480 \n",
      "Epoch 00073: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 273ms/step - loss: 99580.9141 - categorical_accuracy: 0.1480 - val_loss: 140452.2656 - val_categorical_accuracy: 0.1483\n",
      "Epoch 74/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 178837.7812 - categorical_accuracy: 0.1480\n",
      "Epoch 00074: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 276ms/step - loss: 178837.7812 - categorical_accuracy: 0.1480 - val_loss: 189296.7188 - val_categorical_accuracy: 0.1483\n",
      "Epoch 75/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 162558.0469 - categorical_accuracy: 0.1480\n",
      "Epoch 00075: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 277ms/step - loss: 162558.0469 - categorical_accuracy: 0.1480 - val_loss: 100223.7422 - val_categorical_accuracy: 0.1483\n",
      "Epoch 76/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 126829.8438 - categorical_accuracy: 0.1480\n",
      "Epoch 00076: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 280ms/step - loss: 126829.8438 - categorical_accuracy: 0.1480 - val_loss: 374032.0938 - val_categorical_accuracy: 0.1483\n",
      "Epoch 77/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 329253.6562 - categorical_accuracy: 0.1480\n",
      "Epoch 00077: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 329253.6562 - categorical_accuracy: 0.1480 - val_loss: 191697.3438 - val_categorical_accuracy: 0.1483\n",
      "Epoch 78/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 146699.7812 - categorical_accuracy: 0.1480\n",
      "Epoch 00078: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 146699.7812 - categorical_accuracy: 0.1480 - val_loss: 82140.3672 - val_categorical_accuracy: 0.1483\n",
      "Epoch 79/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 59214.8789 - categorical_accuracy: 0.1480\n",
      "Epoch 00079: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 59214.8789 - categorical_accuracy: 0.1480 - val_loss: 29144.6367 - val_categorical_accuracy: 0.1483\n",
      "Epoch 80/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 132243.2500 - categorical_accuracy: 0.1480\n",
      "Epoch 00080: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 132243.2500 - categorical_accuracy: 0.1480 - val_loss: 231526.8594 - val_categorical_accuracy: 0.1483\n",
      "Epoch 81/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 191412.7969 - categorical_accuracy: 0.1480\n",
      "Epoch 00081: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 191412.7969 - categorical_accuracy: 0.1480 - val_loss: 79774.7578 - val_categorical_accuracy: 0.1483\n",
      "Epoch 82/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 85809.9688 - categorical_accuracy: 0.1480\n",
      "Epoch 00082: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 85809.9688 - categorical_accuracy: 0.1480 - val_loss: 61651.1797 - val_categorical_accuracy: 0.1483\n",
      "Epoch 83/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 84594.0078 - categorical_accuracy: 0.1480\n",
      "Epoch 00083: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 273ms/step - loss: 84594.0078 - categorical_accuracy: 0.1480 - val_loss: 166740.4062 - val_categorical_accuracy: 0.1483\n",
      "Epoch 84/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 175077.7812 - categorical_accuracy: 0.1480\n",
      "Epoch 00084: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 175077.7812 - categorical_accuracy: 0.1480 - val_loss: 266681.7500 - val_categorical_accuracy: 0.1483\n",
      "Epoch 85/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 104701.2734 - categorical_accuracy: 0.1480\n",
      "Epoch 00085: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 104701.2734 - categorical_accuracy: 0.1480 - val_loss: 98246.2031 - val_categorical_accuracy: 0.1483\n",
      "Epoch 86/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 70993.6328 - categorical_accuracy: 0.1480\n",
      "Epoch 00086: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 70993.6328 - categorical_accuracy: 0.1480 - val_loss: 20673.6230 - val_categorical_accuracy: 0.1483\n",
      "Epoch 87/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 131134.4688 - categorical_accuracy: 0.1480\n",
      "Epoch 00087: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 131134.4688 - categorical_accuracy: 0.1480 - val_loss: 31815.4961 - val_categorical_accuracy: 0.1483\n",
      "Epoch 88/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 65114.9102 - categorical_accuracy: 0.1480\n",
      "Epoch 00088: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 65114.9102 - categorical_accuracy: 0.1480 - val_loss: 89728.2891 - val_categorical_accuracy: 0.1483\n",
      "Epoch 89/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 46969.9141 - categorical_accuracy: 0.1480\n",
      "Epoch 00089: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 46969.9141 - categorical_accuracy: 0.1480 - val_loss: 3431.0171 - val_categorical_accuracy: 0.1483\n",
      "Epoch 90/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 50662.8867 - categorical_accuracy: 0.1480\n",
      "Epoch 00090: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 50662.8867 - categorical_accuracy: 0.1480 - val_loss: 125458.7344 - val_categorical_accuracy: 0.1483\n",
      "Epoch 91/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 153813.9688 - categorical_accuracy: 0.1480\n",
      "Epoch 00091: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 153813.9688 - categorical_accuracy: 0.1480 - val_loss: 189359.2188 - val_categorical_accuracy: 0.1483\n",
      "Epoch 92/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 134372.4688 - categorical_accuracy: 0.1480\n",
      "Epoch 00092: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 134372.4688 - categorical_accuracy: 0.1480 - val_loss: 7454.4390 - val_categorical_accuracy: 0.1483\n",
      "Epoch 93/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 88534.1172 - categorical_accuracy: 0.1480\n",
      "Epoch 00093: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 88534.1172 - categorical_accuracy: 0.1480 - val_loss: 12455.8916 - val_categorical_accuracy: 0.1483\n",
      "Epoch 94/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 104185.7812 - categorical_accuracy: 0.1480\n",
      "Epoch 00094: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 276ms/step - loss: 104185.7812 - categorical_accuracy: 0.1480 - val_loss: 186630.8594 - val_categorical_accuracy: 0.1483\n",
      "Epoch 95/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 126519.2891 - categorical_accuracy: 0.1480\n",
      "Epoch 00095: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 303ms/step - loss: 126519.2891 - categorical_accuracy: 0.1480 - val_loss: 187335.5781 - val_categorical_accuracy: 0.1483\n",
      "Epoch 96/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 88243.4062 - categorical_accuracy: 0.1480\n",
      "Epoch 00096: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 273ms/step - loss: 88243.4062 - categorical_accuracy: 0.1480 - val_loss: 53929.0977 - val_categorical_accuracy: 0.1483\n",
      "Epoch 97/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 76495.8281 - categorical_accuracy: 0.1480\n",
      "Epoch 00097: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 76495.8281 - categorical_accuracy: 0.1480 - val_loss: 78492.9141 - val_categorical_accuracy: 0.1483\n",
      "Epoch 98/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 84364.3516 - categorical_accuracy: 0.1480\n",
      "Epoch 00098: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 84364.3516 - categorical_accuracy: 0.1480 - val_loss: 231820.9688 - val_categorical_accuracy: 0.1483\n",
      "Epoch 99/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 66481.4453 - categorical_accuracy: 0.1480\n",
      "Epoch 00099: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 274ms/step - loss: 66481.4453 - categorical_accuracy: 0.1480 - val_loss: 59262.5078 - val_categorical_accuracy: 0.1483\n",
      "Epoch 100/100\n",
      "31/31 [==============================] - ETA: 0s - loss: 130986.8281 - categorical_accuracy: 0.1480\n",
      "Epoch 00100: saving model to BCS_CNN\\BCS_CNN.hdf5\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 130986.8281 - categorical_accuracy: 0.1480 - val_loss: 86243.8750 - val_categorical_accuracy: 0.1483\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit(train_generator, steps_per_epoch=step_size_train, epochs=100, verbose=1,\\\n",
    "                     callbacks=callbacks_list, validation_data=val_generator, validation_steps=step_size_val,\\\n",
    "                     validation_freq=1, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_Xtrain(X_train):\n",
    "    mn = []\n",
    "    mx = []\n",
    "    Xn_train = np.zeros(np.shape(X_train))\n",
    "    X_norm = np.zeros(np.shape(X_train))\n",
    "    for i in range(len(X_train[0, ::])):\n",
    "        mn.append(np.min(X_train[::, i]))\n",
    "        Xn_train[::, i] = X_train[::, i] - mn[i]\n",
    "    for i in range(len(X_train[0, ::])):\n",
    "        mx.append(np.max(Xn_train[::, i]))\n",
    "        if mx[i] == 0:\n",
    "            X_norm[::, i] = 0\n",
    "        if mx[i] != 0:\n",
    "            X_norm[::, i] = Xn_train[::, i]/mx[i]\n",
    "            \n",
    "    return X_norm, mx, mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "The accuracy of the network model2 is:  0.06531531531531531\n",
      "dict_keys(['loss', 'categorical_accuracy', 'val_loss', 'val_categorical_accuracy'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x17d49565640>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import tensorflow_docs as tfdocs\n",
    "# import tensorflow_docs.modeling\n",
    "# import tensorflow_docs.plots\n",
    "\n",
    "# plotter = tfdocs.plots.HistoryPlotter(metric = 'binary_crossentropy', smoothing_std=10)\n",
    "# plotter.plot(history)\n",
    "import sklearn.metrics\n",
    "\n",
    "Y_test_hat=model2.predict(test_generator)\n",
    "y_test_hat=Y_test_hat.argmax(axis=-1)+2\n",
    "\n",
    "print(y_test_hat[:200])\n",
    "\n",
    "\n",
    "y_test = test_df['class']\n",
    "y_test = [int(i) for i in y_test]\n",
    "print(y_test[:200])\n",
    "\n",
    "con_matrix = sklearn.metrics.confusion_matrix(y_test,y_test_hat)\n",
    "acc=np.diag(con_matrix).sum().astype(float)/con_matrix.sum()\n",
    "# acc = tensorflow.keras.metrics.Accuracy()\n",
    "# acc.reset_state()\n",
    "# acc.update_state(y_test, y_test_hat)\n",
    "\n",
    "\n",
    "print('The accuracy of the network model2 is: ', acc)\n",
    "\n",
    "min = np.min(con_matrix)\n",
    "max = np.max(con_matrix)\n",
    "temp_mat = con_matrix - min\n",
    "temp_mat = con_matrix/max\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(normalize_Xtrain(con_matrix)[0], cmap='gray')\n",
    "plt.title('Confusion Matrix for Model2')\n",
    "\n",
    "\n",
    "\n",
    "print(history.history.keys())\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(np.arange(0, len(history.history['categorical_accuracy'])), history.history['categorical_accuracy'])\n",
    "plt.plot(np.arange(0, len(history.history['val_categorical_accuracy'])), history.history['val_categorical_accuracy'])\n",
    "plt.legend(('Training Accuracy', 'Validation Accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model2.fit(train_generator, steps_per_epoch=step_size_train, epochs=10, verbose=1,\\\n",
    "#                      callbacks=callbacks_list, validation_data=val_generator, validation_steps=step_size_val,\\\n",
    "#                      validation_freq=1, class_weight=class_weights, max_queue_size=100, workers=20, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model2.fit(ImageDataGenerator, steps_per_epoch=step_size_train, epochs=10, verbose=1,\\\n",
    "#                      callbacks=callbacks_list, validation_data=val_generator, validation_steps=step_size_val,\\\n",
    "#                      validation_freq=1, class_weight=class_weights,\\\n",
    "#                      max_queue_size=100, workers=16, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(model2.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# model2 = load_model('model.best.hdf5', custom_objects={'categorical_tnr': categorical_tnr,\\\n",
    "#                                                        'categorical_tpr': categorical_tpr,\\\n",
    "#                                                        'categorical_tss': categorical_tss})\n",
    "# model2.compile(loss='categorical_crossentropy', optimizer=adam_opt,\\\n",
    "#                metrics=['categorical_accuracy',categorical_tnr,categorical_tpr,categorical_tss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "# from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "# test_generator = test_datagen.flow_from_directory('VGG_AR_Dataset/TestSet_224',\\\n",
    "#                                                 target_size=(224,224), color_mode='rgb',\\\n",
    "#                                                 batch_size=128, class_mode='categorical',\\\n",
    "#                                                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(model2.evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(test_generator.labels==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(test_generator.labels==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
