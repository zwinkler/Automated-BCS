{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image as pil_image\n",
    "import imageio\n",
    "import tensorflow\n",
    "import sklearn.metrics\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "#plt.rcParams.update({'font.size': 32})\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "#import tensorflow.config.experimental\n",
    "#from tensorflow.config.experimental import list_physical_devices, set_memory_growth\n",
    "physical_devices = tensorflow.config.experimental.list_physical_devices('GPU')\n",
    "tensorflow.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_params(model):\n",
    "    total_params = 0 # initialize counter for total params\n",
    "    trainable_params = 0 # initialize counter for trainable params\n",
    "    print('Layer Name\\t\\tType\\t\\tFilter shape\\t\\t# Parameters\\tTrainable') # print column headings\n",
    "    for layer in model.layers: # loop over layers\n",
    "        lname = layer.name # grab layer name\n",
    "        ltype = type(layer).__name__ # grab layer type\n",
    "        ltype[ltype.find('/'):] # parse for only the last part of the string\n",
    "        if ltype=='Conv2D': # print for convolutional layers\n",
    "            weights = layer.get_weights()\n",
    "            print(lname+'\\t\\t'+ltype+'\\t\\t'+str(weights[0].shape)+'\\t\\t'+\\\n",
    "                  str(layer.count_params())+'\\t'+str(layer.trainable))\n",
    "            if layer.trainable:\n",
    "                trainable_params += layer.count_params()\n",
    "            total_params += layer.count_params() # update number of params\n",
    "        elif ltype=='MaxPooling2D': # print for max pool layers\n",
    "            weights = layer.get_weights()\n",
    "            print(lname+'\\t\\t'+ltype+'\\t---------------\\t\\t---')\n",
    "        elif ltype=='Flatten': # print for flatten layers\n",
    "            print(lname+'\\t\\t'+ltype+'\\t\\t---------------\\t\\t---')\n",
    "        elif ltype=='Dense': # print for dense layers\n",
    "            weights = layer.get_weights()\n",
    "            print(lname+'\\t\\t\\t'+ltype+'\\t\\t'+str(weights[0].shape)+'\\t\\t'+\\\n",
    "                  str(layer.count_params())+'\\t'+str(layer.trainable))\n",
    "            if layer.trainable:\n",
    "                trainable_params += layer.count_params()\n",
    "            total_params += layer.count_params() # update number of params\n",
    "        elif ltype=='Dropout': # print for dropout layers\n",
    "            print(lname+'\\t\\t'+ltype+'\\t\\t------------------\\t---')\n",
    "    print('---------------')\n",
    "    print('Total trainable parameters: '+str(trainable_params)) # print total params\n",
    "    print('Total untrainable parameters: '+str(total_params-trainable_params))\n",
    "    print('Total parameters: '+str(total_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shapes(model):\n",
    "    print('Layer Name\\t\\tType\\t\\tInput Shape\\t\\tOutput Shape\\tTrainable')# print column headings\n",
    "    for layer in model.layers:  # loop over layers\n",
    "        lname = layer.name # grab layer name\n",
    "        ltype = type(layer).__name__ # grab layer type\n",
    "        ltype[ltype.find('/'):] # parse for only the last part of the string\n",
    "        if ltype=='Conv2D': # print for convolutional layers\n",
    "            print(lname+'\\t\\t'+ltype+'\\t\\t'+str(layer.input_shape)+'\\t'+\\\n",
    "                  str(layer.output_shape)+'\\t'+str(layer.trainable))\n",
    "        elif ltype=='MaxPooling2D': # print for maxpool layers\n",
    "            print(lname+'\\t\\t'+ltype+'\\t'+str(layer.input_shape)+'\\t'+\\\n",
    "                  str(layer.output_shape))\n",
    "        elif ltype=='Flatten': # print for flatten layers\n",
    "            print(lname+'\\t\\t'+ltype+'\\t\\t'+str(layer.input_shape)+'\\t'+\\\n",
    "                  str(layer.output_shape))\n",
    "        elif ltype=='Dense': # print for dense layers\n",
    "            print(lname+'\\t\\t\\t'+ltype+'\\t\\t'+str(layer.input_shape)+'\\t\\t'+\\\n",
    "                  str(layer.output_shape)+'\\t'+str(layer.trainable))\n",
    "        elif ltype=='Dropout': # print for dropout layers\n",
    "            print(lname+'\\t\\t'+ltype+'\\t\\t'+str(layer.input_shape)+'\\t'+\\\n",
    "                  str(layer.output_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input\n",
    "from astropy.io import fits\n",
    "import skimage.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input\n",
    "import keras.applications\n",
    "input_tensor = Input(shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Dropout, Conv2D, ReLU, concatenate, MaxPool2D, AvgPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fire_module(x,s1,e1,e3):\n",
    "    s1x = Conv2D(s1,kernel_size = 1, padding = 'same')(x)\n",
    "    s1x = ReLU()(s1x)\n",
    "    e1x = Conv2D(e1,kernel_size = 1, padding = 'same')(s1x)\n",
    "    e3x = Conv2D(e3,kernel_size = 3, padding = 'same')(s1x)\n",
    "    x = concatenate([e1x,e3x])\n",
    "    x = ReLU()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Activation, Concatenate\n",
    "from keras.layers import Flatten, Dropout\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "def SqueezeNet(nb_classes, inputs=(3, 222, 222)):\n",
    "    \"\"\" Keras Implementation of SqueezeNet(arXiv 1602.07360)\n",
    "    @param nb_classes: total number of final categories\n",
    "    Arguments:\n",
    "    inputs -- shape of the input images (channel, cols, rows)\n",
    "    \"\"\"\n",
    "\n",
    "    input_img = Input(shape=inputs)\n",
    "    conv1 = Convolution2D(\n",
    "        96, (7, 7), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        strides=(2, 2), padding='same', name='conv1',\n",
    "        data_format=\"channels_first\")(input_img)\n",
    "    maxpool1 = MaxPooling2D(\n",
    "        pool_size=(3, 3), strides=(2, 2), name='maxpool1',\n",
    "        data_format=\"channels_first\")(conv1)\n",
    "    fire2_squeeze = Convolution2D(\n",
    "        16, (1, 1), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='same', name='fire2_squeeze',\n",
    "        data_format=\"channels_first\")(maxpool1)\n",
    "    fire2_expand1 = Convolution2D(\n",
    "        64, (1, 1), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='same', name='fire2_expand1',\n",
    "        data_format=\"channels_first\")(fire2_squeeze)\n",
    "    fire2_expand2 = Convolution2D(\n",
    "        64, (3, 3), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='same', name='fire2_expand2',\n",
    "        data_format=\"channels_first\")(fire2_squeeze)\n",
    "    merge2 = Concatenate(axis=1)([fire2_expand1, fire2_expand2])\n",
    "\n",
    "    fire3_squeeze = Convolution2D(\n",
    "        16, (1, 1), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='same', name='fire3_squeeze',\n",
    "        data_format=\"channels_first\")(merge2)\n",
    "    fire3_expand1 = Convolution2D(\n",
    "        64, (1, 1), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='same', name='fire3_expand1',\n",
    "        data_format=\"channels_first\")(fire3_squeeze)\n",
    "    fire3_expand2 = Convolution2D(\n",
    "        64, (3, 3), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='same', name='fire3_expand2',\n",
    "        data_format=\"channels_first\")(fire3_squeeze)\n",
    "    merge3 = Concatenate(axis=1)([fire3_expand1, fire3_expand2])\n",
    "\n",
    "    fire4_squeeze = Convolution2D(\n",
    "        32, (1, 1), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='same', name='fire4_squeeze',\n",
    "        data_format=\"channels_first\")(merge3)\n",
    "    fire4_expand1 = Convolution2D(\n",
    "        128, (1, 1), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='same', name='fire4_expand1',\n",
    "        data_format=\"channels_first\")(fire4_squeeze)\n",
    "    fire4_expand2 = Convolution2D(\n",
    "        128, (3, 3), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='same', name='fire4_expand2',\n",
    "        data_format=\"channels_first\")(fire4_squeeze)\n",
    "    merge4 = Concatenate(axis=1)([fire4_expand1, fire4_expand2])\n",
    "    maxpool4 = MaxPooling2D(\n",
    "        pool_size=(3, 3), strides=(2, 2), name='maxpool4',\n",
    "        data_format=\"channels_first\")(merge4)\n",
    "\n",
    "    fire5_squeeze = Convolution2D(\n",
    "        32, (1, 1), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='same', name='fire5_squeeze',\n",
    "        data_format=\"channels_first\")(maxpool4)\n",
    "    fire5_expand1 = Convolution2D(\n",
    "        128, (1, 1), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='same', name='fire5_expand1',\n",
    "        data_format=\"channels_first\")(fire5_squeeze)\n",
    "    fire5_expand2 = Convolution2D(\n",
    "        128, (3, 3), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='same', name='fire5_expand2',\n",
    "        data_format=\"channels_first\")(fire5_squeeze)\n",
    "    merge5 = Concatenate(axis=1)([fire5_expand1, fire5_expand2])\n",
    "\n",
    "    fire6_squeeze = Convolution2D(\n",
    "        48, (1, 1), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='same', name='fire6_squeeze',\n",
    "        data_format=\"channels_first\")(merge5)\n",
    "    fire6_expand1 = Convolution2D(\n",
    "        192, (1, 1), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='same', name='fire6_expand1',\n",
    "        data_format=\"channels_first\")(fire6_squeeze)\n",
    "    fire6_expand2 = Convolution2D(\n",
    "        192, (3, 3), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='same', name='fire6_expand2',\n",
    "        data_format=\"channels_first\")(fire6_squeeze)\n",
    "    merge6 = Concatenate(axis=1)([fire6_expand1, fire6_expand2])\n",
    "\n",
    "    fire7_squeeze = Convolution2D(\n",
    "        48, (1, 1), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='same', name='fire7_squeeze',\n",
    "        data_format=\"channels_first\")(merge6)\n",
    "    fire7_expand1 = Convolution2D(\n",
    "        192, (1, 1), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='same', name='fire7_expand1',\n",
    "        data_format=\"channels_first\")(fire7_squeeze)\n",
    "    fire7_expand2 = Convolution2D(\n",
    "        192, (3, 3), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='same', name='fire7_expand2',\n",
    "        data_format=\"channels_first\")(fire7_squeeze)\n",
    "    merge7 = Concatenate(axis=1)([fire7_expand1, fire7_expand2])\n",
    "\n",
    "    fire8_squeeze = Convolution2D(\n",
    "        64, (1, 1), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='same', name='fire8_squeeze',\n",
    "        data_format=\"channels_first\")(merge7)\n",
    "    fire8_expand1 = Convolution2D(\n",
    "        256, (1, 1), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='same', name='fire8_expand1',\n",
    "        data_format=\"channels_first\")(fire8_squeeze)\n",
    "    fire8_expand2 = Convolution2D(\n",
    "        256, (3, 3), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='same', name='fire8_expand2',\n",
    "        data_format=\"channels_first\")(fire8_squeeze)\n",
    "    merge8 = Concatenate(axis=1)([fire8_expand1, fire8_expand2])\n",
    "\n",
    "    maxpool8 = MaxPooling2D(\n",
    "        pool_size=(3, 3), strides=(2, 2), name='maxpool8',\n",
    "        data_format=\"channels_first\")(merge8)\n",
    "    fire9_squeeze = Convolution2D(\n",
    "        64, (1, 1), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='same', name='fire9_squeeze',\n",
    "        data_format=\"channels_first\")(maxpool8)\n",
    "    fire9_expand1 = Convolution2D(\n",
    "        256, (1, 1), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='same', name='fire9_expand1',\n",
    "        data_format=\"channels_first\")(fire9_squeeze)\n",
    "    fire9_expand2 = Convolution2D(\n",
    "        256, (3, 3), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='same', name='fire9_expand2',\n",
    "        data_format=\"channels_first\")(fire9_squeeze)\n",
    "    merge9 = Concatenate(axis=1)([fire9_expand1, fire9_expand2])\n",
    "\n",
    "    fire9_dropout = Dropout(0.5, name='fire9_dropout')(merge9)\n",
    "    conv10 = Convolution2D(\n",
    "        nb_classes, (1, 1), activation='relu', kernel_initializer='glorot_uniform',\n",
    "        padding='valid', name='conv10',\n",
    "        data_format=\"channels_first\")(fire9_dropout)\n",
    "\n",
    "    global_avgpool10 = GlobalAveragePooling2D(data_format='channels_first')(conv10)\n",
    "    softmax = Activation(\"softmax\", name='softmax')(global_avgpool10)\n",
    "\n",
    "    return Model(inputs=input_img, outputs=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_data.csv',dtype=str)\n",
    "val_df = pd.read_csv('val_data.csv',dtype=str)\n",
    "test_df = pd.read_csv('test_data.csv',dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3893 validated image filenames belonging to 5 classes.\n",
      "Found 971 validated image filenames belonging to 5 classes.\n",
      "Found 444 validated image filenames belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "train_generator = train_datagen.flow_from_dataframe(dataframe=train_df,\\\n",
    "                                                    directory='',\\\n",
    "                                                    xcol='filename',y_col='class',\\\n",
    "                                                    target_size=(222,222), color_mode='rgb',\\\n",
    "                                                    batch_size=128, class_mode='categorical',\\\n",
    "                                                    shuffle=True) #RETURN TO TRUEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "val_generator = val_datagen.flow_from_dataframe(dataframe=val_df,\\\n",
    "                                                directory = '',\\\n",
    "                                                xcol='filename',ycol='class',\\\n",
    "                                                target_size=(222,222), color_mode='rgb',\\\n",
    "                                                batch_size=128, class_mode='categorical',\\\n",
    "                                                shuffle=True)\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_generator = test_datagen.flow_from_dataframe(dataframe=test_df,\\\n",
    "                                                directory = '',\\\n",
    "                                                xcol='filename',ycol='class',\\\n",
    "                                                target_size=(222,222), color_mode='rgb',\\\n",
    "                                                batch_size=128, class_mode='categorical',\\\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading C:\\Users\\zacha\\.tensorflow\\models\\squeezenet_v1_1-1739-48945577.tf.npz.zip from https://github.com/osmr/imgclsmob/releases/download/v0.0.88/squeezenet_v1_1-1739-48945577.tf.npz.zip...\n"
     ]
    }
   ],
   "source": [
    "# import tensorflowcv\n",
    "# from tensorflowcv.model_provider import get_model\n",
    "# from tensorflowcv.models.common import is_channels_first\n",
    "\n",
    "from tensorflowcv.model_provider import get_model as tfcv_get_model\n",
    "from tensorflowcv.model_provider import init_variables_from_state_dict as tfcv_init_variables_from_state_dict\n",
    "\n",
    "# model = tensorflowcv.model_provider.get_model(\"SqueezeNet\", use_pretrained=True)\n",
    "\n",
    "model = tfcv_get_model(\"squeezenet_v1_1\", pretrained=True, data_format=\"channels_first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 5\n",
    "\n",
    "# model = SqueezeNet(nb_classes, inputs=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SqueezeNet' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'SqueezeNet' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_tnr(y_true,y_pred):\n",
    "    import keras.backend as K\n",
    "    y_true = K.argmax(y_true)\n",
    "    y_pred = K.argmax(y_pred)\n",
    "    neg_y_true = 1 - y_true\n",
    "    neg_y_pred = 1 - y_pred\n",
    "    fp = K.cast(K.sum(neg_y_true * y_pred),'float32')\n",
    "    tn = K.cast(K.sum(neg_y_true * neg_y_pred),'float32')\n",
    "    tnr = tn / (tn + fp + K.epsilon())\n",
    "    return tnr\n",
    "\n",
    "def categorical_tpr(y_true,y_pred):\n",
    "    import keras.backend as K\n",
    "    y_true = K.argmax(y_true)\n",
    "    y_pred = K.argmax(y_pred)\n",
    "    neg_y_pred = 1 - y_pred\n",
    "    fn = K.cast(K.sum(y_true * neg_y_pred),'float32')\n",
    "    tp = K.cast(K.sum(y_true * y_pred),'float32')\n",
    "    tpr = tp / (tp + fn + K.epsilon())\n",
    "    return tpr\n",
    "\n",
    "def categorical_tss(y_true,y_pred):\n",
    "    import keras.backend as K\n",
    "    tpr = categorical_tpr(y_true,y_pred)\n",
    "    tnr = categorical_tnr(y_true,y_pred)\n",
    "    tss = tpr + tnr - 1\n",
    "    return tss\n",
    "\n",
    "def save_con_mat(y_true, y_pred):\n",
    "    import tensorflow as tf\n",
    "    print((y_true.shape))\n",
    "    print((y_pred.shape))\n",
    "    con_matrix = tf.math.confusion_matrix(y_true, y_pred)\n",
    "    # acc=np.diag(con_matrix).sum().astype(float)/con_matrix.sum()\n",
    "    return con_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SqueezeNet' object has no attribute 'compile'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-c3f1abe87acd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0madam_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m model.compile(loss='categorical_crossentropy', optimizer=adam_opt,\\\n\u001b[0m\u001b[0;32m      4\u001b[0m                metrics=['categorical_accuracy'])# save_con_mat]) #,categorical_tnr,categorical_tpr,categorical_tss])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SqueezeNet' object has no attribute 'compile'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "adam_opt = Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam_opt,\\\n",
    "               metrics=['categorical_accuracy'])# save_con_mat]) #,categorical_tnr,categorical_tpr,categorical_tss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.0\n",
      "Batch size:  128\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "filepath = 'BCS_CNN\\\\BCS_CNN.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_tss', verbose=1, save_best_only=False, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_categorical_tss', min_delta=0.001, patience=5, verbose=1, mode='max')\n",
    "#callbacks_list = [checkpoint, early_stop]\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "step_size_train = np.ceil(train_generator.n/train_generator.batch_size)\n",
    "print(step_size_train)\n",
    "print('Batch size: ', train_generator.batch_size)\n",
    "#step_size_train = 2373 # to help debug val accuracy issue\n",
    "step_size_val = np.ceil(val_generator.n/val_generator.batch_size)\n",
    "print(step_size_val)\n",
    "#step_size_val = 303 # to help debug val accuracy issue\n",
    "\n",
    "# the following assumes that 0 is the majority class\n",
    "# class_weights = {0: 1.,\n",
    "#                  1: (np.asarray(train_generator.classes)==0).sum()/(np.asarray(train_generator.classes)==1).sum()}\n",
    "# the following taken from https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n",
    "#class_weights = {0: (1/(train_generator.classes==0).sum())*len(train_generator.classes)/2,\n",
    "#                 1: (1/(train_generator.classes==1).sum())*len(train_generator.classes)/2}\n",
    "# class_weights = {0: 1.,\n",
    "#                 1: 6.}\n",
    "\n",
    "from sklearn.utils import class_weight \n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(train_df['class']), y =train_df['class'])\n",
    "class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": " No algorithm worked!\n\t [[node model/conv1/Relu\n (defined at C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\backend.py:4867)\n]] [Op:__inference_train_function_3248]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model/conv1/Relu:\nIn[0] model/conv1/BiasAdd (defined at C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\layers\\convolutional.py:264)\n\nOperation defined at: (most recent call last)\n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\asyncio\\events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n>>>     lambda f: self._run_callback(functools.partial(callback, future))\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n>>>     ret = callback()\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n>>>     self.ctx_run(self.run)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n>>>     yielded = self.gen.send(value)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n>>>     yield gen.maybe_future(dispatch(*args))\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n>>>     yielded = ctx_run(next, result)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n>>>     yield gen.maybe_future(handler(stream, idents, msg))\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n>>>     yielded = ctx_run(next, result)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 543, in execute_request\n>>>     self.do_execute(\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n>>>     yielded = ctx_run(next, result)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2876, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2922, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3145, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3337, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3417, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"<ipython-input-17-5d16005ffb93>\", line 1, in <module>\n>>>     history = model.fit(train_generator, steps_per_epoch=step_size_train, epochs=10, verbose=1,\\\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n>>>     y_pred = self(x, training=True)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\n>>>     return self._run_internal_graph(\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\n>>>     outputs = node.layer(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 273, in call\n>>>     return self.activation(outputs)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\activations.py\", line 311, in relu\n>>>     return backend.relu(x, alpha=alpha, max_value=max_value, threshold=threshold)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\backend.py\", line 4867, in relu\n>>>     x = tf.nn.relu(x)\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-5d16005ffb93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m history = model.fit(train_generator, steps_per_epoch=step_size_train, epochs=10, verbose=1,\\\n\u001b[0m\u001b[0;32m      2\u001b[0m                      \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep_size_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                      validation_freq=1, class_weight=class_weights)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m:  No algorithm worked!\n\t [[node model/conv1/Relu\n (defined at C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\backend.py:4867)\n]] [Op:__inference_train_function_3248]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model/conv1/Relu:\nIn[0] model/conv1/BiasAdd (defined at C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\layers\\convolutional.py:264)\n\nOperation defined at: (most recent call last)\n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\asyncio\\events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n>>>     lambda f: self._run_callback(functools.partial(callback, future))\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n>>>     ret = callback()\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n>>>     self.ctx_run(self.run)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n>>>     yielded = self.gen.send(value)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n>>>     yield gen.maybe_future(dispatch(*args))\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n>>>     yielded = ctx_run(next, result)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n>>>     yield gen.maybe_future(handler(stream, idents, msg))\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n>>>     yielded = ctx_run(next, result)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 543, in execute_request\n>>>     self.do_execute(\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n>>>     yielded = ctx_run(next, result)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2876, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2922, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3145, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3337, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3417, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"<ipython-input-17-5d16005ffb93>\", line 1, in <module>\n>>>     history = model.fit(train_generator, steps_per_epoch=step_size_train, epochs=10, verbose=1,\\\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n>>>     y_pred = self(x, training=True)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\n>>>     return self._run_internal_graph(\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\n>>>     outputs = node.layer(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 273, in call\n>>>     return self.activation(outputs)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\activations.py\", line 311, in relu\n>>>     return backend.relu(x, alpha=alpha, max_value=max_value, threshold=threshold)\n>>> \n>>>   File \"C:\\Users\\zacha\\anaconda3\\envs\\ML_env\\lib\\site-packages\\keras\\backend.py\", line 4867, in relu\n>>>     x = tf.nn.relu(x)\n>>> "
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator, steps_per_epoch=step_size_train, epochs=10, verbose=1,\\\n",
    "                     callbacks=callbacks_list, validation_data=val_generator, validation_steps=step_size_val,\\\n",
    "                     validation_freq=1, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_Xtrain(X_train):\n",
    "    mn = []\n",
    "    mx = []\n",
    "    Xn_train = np.zeros(np.shape(X_train))\n",
    "    X_norm = np.zeros(np.shape(X_train))\n",
    "    for i in range(len(X_train[0, ::])):\n",
    "        mn.append(np.min(X_train[::, i]))\n",
    "        Xn_train[::, i] = X_train[::, i] - mn[i]\n",
    "    for i in range(len(X_train[0, ::])):\n",
    "        mx.append(np.max(Xn_train[::, i]))\n",
    "        if mx[i] == 0:\n",
    "            X_norm[::, i] = 0\n",
    "        if mx[i] != 0:\n",
    "            X_norm[::, i] = Xn_train[::, i]/mx[i]\n",
    "            \n",
    "    return X_norm, mx, mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "Y_test_hat=model.predict(test_generator)\n",
    "y_test_hat=Y_test_hat.argmax(axis=-1)+2\n",
    "\n",
    "print(y_test_hat[:200])\n",
    "\n",
    "\n",
    "y_test = test_df['class']\n",
    "y_test = [int(i) for i in y_test]\n",
    "print(y_test[:200])\n",
    "\n",
    "con_matrix = sklearn.metrics.confusion_matrix(y_test,y_test_hat)\n",
    "acc=np.diag(con_matrix).sum().astype(float)/con_matrix.sum()\n",
    "# acc = tensorflow.keras.metrics.Accuracy()\n",
    "# acc.reset_state()\n",
    "# acc.update_state(y_test, y_test_hat)\n",
    "\n",
    "\n",
    "print('The accuracy of the network model2 is: ', acc)\n",
    "\n",
    "min = np.min(con_matrix)\n",
    "max = np.max(con_matrix)\n",
    "temp_mat = con_matrix - min\n",
    "temp_mat = con_matrix/max\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(normalize_Xtrain(con_matrix)[0], cmap='gray')\n",
    "plt.title('Confusion Matrix for Model2')\n",
    "\n",
    "\n",
    "\n",
    "print(history.history.keys())\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(np.arange(0, len(history.history['categorical_accuracy'])), history.history['categorical_accuracy'])\n",
    "plt.plot(np.arange(0, len(history.history['val_categorical_accuracy'])), history.history['val_categorical_accuracy'])\n",
    "plt.legend(('Training Accuracy', 'Validation Accuracy'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "777203db839fd8862590ccaec2e9b0e8d43d0b0433ee236e71ebf4beaf77dfce"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ML_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
