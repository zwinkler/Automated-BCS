{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zacha\\anaconda3\\envs\\ML_env_pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pyrealsense2.pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import skimage.filters\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BeitFeatureExtractor\n",
    "from torchvision.transforms import Resize, Compose, Normalize, ToTensor\n",
    "import glob\n",
    "import pytorch_lightning as pl\n",
    "# import mercury\n",
    "# import RPi.GPIO as GPIO\n",
    "# import multiprocessing as mp\n",
    "# GPIO.setwarnings(False)\n",
    "# GPIO.setmode(GPIO.BCM)\n",
    "# GPIO.setup(13, GPIO.IN)\n",
    "# from myFunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(depth_img, background):\n",
    "    depth_img = background - depth_img #[:, int((848-640)/2):int(-(848-640)/2)]\n",
    "\n",
    "    otsu_thresh = skimage.filters.threshold_otsu(depth_img)\n",
    "    img_post = np.where(depth_img<otsu_thresh, depth_img, 0)\n",
    "\n",
    "    img_post = np.where(img_post>600, depth_img, 0)\n",
    "\n",
    "    img_mask = np.where(img_post != 0, 1, 0)\n",
    "    img_seg = skimage.measure.label(img_mask, background = 0, connectivity=2)\n",
    "    #assert( img_seg.max() != 0 )\n",
    "    if img_seg.max() != 0:\n",
    "        mask = img_seg == np.argmax(np.bincount(img_seg.flat)[1:])+1\n",
    "    else:\n",
    "        mask = np.zeros(np.shape(depth_img))\n",
    "\n",
    "    depth_img = np.where(mask != 0, depth_img, 0)\n",
    "    depth_img = (depth_img/np.max(depth_img)*255).astype('uint8')\n",
    "\n",
    "    return depth_img, mask\n",
    "\n",
    "    import pytorch_lightning as pl\n",
    "from transformers import BeitForImageClassification, AdamW\n",
    "import torch.nn as nn\n",
    "\n",
    "class ViTLightningModule(pl.LightningModule):\n",
    "    def __init__(self, num_labels=10):\n",
    "        super(ViTLightningModule, self).__init__()\n",
    "        self.vit = BeitForImageClassification.from_pretrained(\"microsoft/beit-base-patch16-224\",\n",
    "                                                              num_labels=5,\n",
    "                                                            #   id2label=id2label,\n",
    "                                                            #   label2id=label2id,\n",
    "                                                              ignore_mismatched_sizes=True)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        return outputs.logits\n",
    "        \n",
    "    # def common_step(self, batch, batch_idx):\n",
    "    #     pixel_values = batch['pixel_values']\n",
    "    #     labels = batch['labels']\n",
    "    #     logits = self(pixel_values)\n",
    "\n",
    "    #     #criterion = nn.CrossEntropyLoss()\n",
    "    #     #criterion = nn.CrossEntropyLoss(weight=torch.cuda.FloatTensor(class_weights))\n",
    "    #     loss = criterion(logits, labels)\n",
    "    #     predictions = logits.argmax(-1)\n",
    "    #     correct = (predictions == labels).sum().item()\n",
    "    #     accuracy = correct/pixel_values.shape[0]\n",
    "\n",
    "    #     return loss, accuracy\n",
    "      \n",
    "    # def training_step(self, batch, batch_idx):\n",
    "    #     loss, accuracy = self.common_step(batch, batch_idx)     \n",
    "    #     # logs metrics for each training_step,\n",
    "    #     # and the average across the epoch\n",
    "    #     self.log(\"training_loss\", loss)\n",
    "    #     self.log(\"training_accuracy\", accuracy)\n",
    "\n",
    "    #     return loss\n",
    "    \n",
    "    # def validation_step(self, batch, batch_idx):\n",
    "    #     loss, accuracy = self.common_step(batch, batch_idx)     \n",
    "    #     self.log(\"validation_loss\", loss, on_epoch=True)\n",
    "    #     self.log(\"validation_accuracy\", accuracy, on_epoch=True)\n",
    "\n",
    "    #     return loss\n",
    "\n",
    "    # def test_step(self, batch, batch_idx):\n",
    "    #     loss, accuracy = self.common_step(batch, batch_idx)\n",
    "    #     self.log(\"test_loss\", loss, on_epoch=True)\n",
    "    #     self.log(\"test_accuracy\", accuracy, on_epoch=True)\n",
    "\n",
    "    #     return loss, accuracy\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # We could make the optimizer more fancy by adding a scheduler and specifying which parameters do\n",
    "        # not require weight_decay but just using AdamW out-of-the-box works fine\n",
    "        return AdamW(self.parameters(), lr=1e-5)\n",
    "\n",
    "    # def train_dataloader(self):\n",
    "    #     return train_dataloader\n",
    "\n",
    "    # def val_dataloader(self):\n",
    "    #     return val_dataloader\n",
    "    \n",
    "    # def test_dataloader(self):\n",
    "    #     return test_dataloader\n",
    "    \n",
    "    # def predict_dataloader(self):\n",
    "    #     return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zacha\\anaconda3\\envs\\ML_env_pytorch\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['February_Data\\\\Cow_10_0014.bag', 'February_Data\\\\Cow_11_0075.bag', 'February_Data\\\\Cow_12_6006.bag', 'February_Data\\\\Cow_13_8001.bag', 'February_Data\\\\Cow_14_1044.bag', 'February_Data\\\\Cow_1_8025.bag', 'February_Data\\\\Cow_2_6016.bag', 'February_Data\\\\Cow_3_6018.bag', 'February_Data\\\\Cow_4_8036.bag', 'February_Data\\\\Cow_5_8041.bag', 'February_Data\\\\Cow_6_5029.bag', 'February_Data\\\\Cow_7_6022.bag', 'February_Data\\\\Cow_8_2023.bag', 'February_Data\\\\Cow_9_NOTG.bag']\n",
      "(480, 640)\n"
     ]
    }
   ],
   "source": [
    "fps = 30\n",
    "width = 848\n",
    "length = 480\n",
    "total_pixels = width*length\n",
    "\n",
    "transformer_name = \"microsoft/beit-base-patch16-224\"\n",
    "model = ViTLightningModule.load_from_checkpoint(checkpoint_path=\"VIT_models\\BEiT_42_keep_test.ckpt\")\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "feature_extractor = BeitFeatureExtractor.from_pretrained(transformer_name)\n",
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "\n",
    "filenames = glob.glob('February_Data\\\\*.bag')\n",
    "print(filenames)\n",
    "# Configure depth and color streams\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_device_from_file(filenames[4]) #4 is a good example\n",
    "# align_to = rs.stream.color\n",
    "# align = rs.align(align_to)\n",
    "#config.enable_stream(rs.stream.depth, width, length, rs.format.z16, fps)\n",
    "#config.enable_stream(rs.stream.color, width, length, rs.format.bgr8, fps)\n",
    "\n",
    "pipe_profile = pipeline.start(config)\n",
    "\n",
    "#decimation = rs.decimation_filter()\n",
    "#decimation.set_option(rs.option.filter_magnitude, 2)\n",
    "\n",
    "spatial = rs.spatial_filter()\n",
    "spatial.set_option(rs.option.filter_magnitude, 5)\n",
    "spatial.set_option(rs.option.filter_smooth_alpha, 1)\n",
    "spatial.set_option(rs.option.filter_smooth_delta, 50)\n",
    "spatial.set_option(rs.option.holes_fill, 2)\n",
    "\n",
    "thresh = rs.threshold_filter(min_dist = 0.1, max_dist = 3.0)\n",
    "\n",
    "# depth_sensor = pipe_profile.get_device().first_depth_sensor()\n",
    "\n",
    "# depth_sensor.set_option(rs.option.enable_auto_exposure, True)\n",
    "\n",
    "#getting the background when the camera is first turned on\n",
    "frames = pipeline.wait_for_frames()\n",
    "#aligned_frames = align.process(frames)\n",
    "depth_frame = frames.get_depth_frame()\n",
    "color_frame = frames.get_color_frame()\n",
    "thresh_frame = thresh.process(depth_frame)\n",
    "filtered_depth = spatial.process(thresh_frame)\n",
    "background_image = np.asanyarray(filtered_depth.get_data())\n",
    "print(np.shape(background_image))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    #getting frames from the camera\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    #aligned_frames = align.process(frames)\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    thresh_frame = thresh.process(depth_frame)\n",
    "    filtered_depth = spatial.process(thresh_frame)\n",
    "    depth_image = np.asanyarray(filtered_depth.get_data())\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "    #import imageio\n",
    "    #background_image = imageio.imread('masks\\\\feb_background.tif')\n",
    "    cropped_depth, mask = image_preprocessing(depth_image, background_image)\n",
    "    upper_pixels = np.sum(np.where(cropped_depth[:100, :] != 0, 1, 0))\n",
    "    lower_pixels = np.sum(np.where(cropped_depth[length-20:length, :] != 0, 1, 0))\n",
    "    total_nonzero_pixels = np.sum(np.where(cropped_depth != 0, 1, 0))\n",
    "    \n",
    "    \n",
    "    display_img = np.zeros(np.shape(cropped_depth))\n",
    "    display_img = np.where(cropped_depth !=0, cropped_depth, 0)\n",
    "    # cv2.line(display_img,(0,100),(848,100),(255,0,0),1)\n",
    "    # cv2.line(display_img,(0, length-20),(848, length-20),(255,0,0),1)\n",
    "    \n",
    "    # cv2.putText(display_img, 'Lower Pixel values: '+str(lower_pixels),(0,400), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(255),2,cv2.LINE_AA)\n",
    "    # cv2.putText(display_img, 'Upper Pixel values: '+str(upper_pixels),(0,150), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(255),2,cv2.LINE_AA)\n",
    "\n",
    "    # cv2.imshow('cropped depth', display_img)\n",
    "    # cv2.waitKey(1)\n",
    "    # print(lower_pixels, upper_pixels, total_nonzero_pixels)\n",
    "\n",
    "    cropped_depth = (cropped_depth/np.max(cropped_depth)*255).astype('uint8')\n",
    "\n",
    "    cropped_color = color_image\n",
    "    cropped_color[:, :, 0] = np.where(mask==True, color_image[:, :, 0], 0)\n",
    "    cropped_color[:, :, 1] = np.where(mask==True, color_image[:, :, 1], 0)\n",
    "    cropped_color[:, :, 2] = np.where(mask==True, color_image[:, :, 2], 0)\n",
    "\n",
    "    img_DGE = np.zeros(np.shape(color_image))\n",
    "    img_DGE[:, :, 0] = cropped_depth\n",
    "    img_DGE[:, :, 1] = skimage.color.rgb2gray(cropped_color)\n",
    "    mask_dilation = skimage.morphology.binary_dilation(mask, skimage.morphology.ball(4, dtype=bool)[::, ::, 0], out=None)\n",
    "    img_DGE[:, :, 2] = skimage.feature.canny(cropped_depth, sigma=1.0).astype('uint8')*255*mask_dilation\n",
    "    #img_DGE= skimage.transform.rescale(img_DGE, (1920, 1080), anti_aliasing=True, channel_axis=2)\n",
    "    # print(np.shape(img_DGE))\n",
    "    transform = Compose([ToTensor(), Resize((224, 224)), normalize])\n",
    "    img_DGE_norm = transform(img_DGE)\n",
    "\n",
    "    outputs=model(img_DGE_norm[None, :].to('cpu').type('torch.FloatTensor'))\n",
    "    prediction=np.argmax(outputs[0].cpu().detach().numpy(), axis=-1)\n",
    "    #print(prediction)\n",
    "\n",
    "    #cv2.putText(img_DGE, 'BCS: '+ str(prediction[0]),(0,400), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0, 0, 255),2,cv2.LINE_AA)\n",
    "    cv2.putText(img_DGE,'Thi'+'c'*(prediction+2),(0,400), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0, 0, 255),2,cv2.LINE_AA)\n",
    "    cv2.imshow('cropped color', cv2.resize(img_DGE, (1920,1080)))\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "\n",
    "    # if (lower_pixels < 400) and (upper_pixels > 15000) and (total_nonzero_pixels > 14000):\n",
    "\n",
    "    #     #cropped_depth = (cropped_depth/np.max(cropped_depth)*255).astype('uint8')\n",
    "\n",
    "    #     cropped_color = color_image\n",
    "    #     cropped_color[:, :, 0] = np.where(mask==True, color_image[:, :, 0], 0)\n",
    "    #     cropped_color[:, :, 1] = np.where(mask==True, color_image[:, :, 1], 0)\n",
    "    #     cropped_color[:, :, 2] = np.where(mask==True, color_image[:, :, 2], 0)\n",
    "\n",
    "    #     #Now convert depth/color images to DGE\n",
    "    #     img_DGE = np.zeros(np.shape(color_image))\n",
    "    #     img_DGE[:, :, 0] = cropped_depth\n",
    "    #     img_DGE[:, :, 1] = skimage.color.rgb2gray(cropped_color)\n",
    "    #     mask_dilation = skimage.morphology.binary_dilation(mask, skimage.morphology.ball(4, dtype=bool)[::, ::, 0], out=None)\n",
    "    #     img_DGE[:, :, 2] = skimage.feature.canny(cropped_depth, sigma=0.5).astype('uint8')*255*mask_dilation\n",
    "    #     # print(np.shape(img_DGE))\n",
    "    #     transform = Compose([ToTensor(), Resize((224, 224)), normalize])\n",
    "    #     img_DGE_norm = transform(img_DGE)\n",
    "\n",
    "    #     outputs=model(img_DGE_norm[None, :].to('cpu').type('torch.FloatTensor'))\n",
    "    #     prediction=np.argmax(outputs[0].cpu().detach().numpy(), axis=-1)\n",
    "    #     print(int(prediction))\n",
    "\n",
    "    #     cv2.putText(img_DGE, 'Thiccness Index: '+'thi'+'c'*int(prediction),(0,400), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0, 0, 255),2,cv2.LINE_AA)\n",
    "    #     cv2.imshow('cropped color', img_DGE)\n",
    "    #     cv2.waitKey(1)\n",
    "    \n",
    "\n",
    "    #cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ML_env_pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2107e4a676a391af164907120f93cd287ed2c419d0c1d2ead0dc1b99f4b7ffc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
