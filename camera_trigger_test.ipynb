{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zacha\\anaconda3\\envs\\ML_env_pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pyrealsense2.pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import skimage.filters\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BeitFeatureExtractor\n",
    "from torchvision.transforms import Resize, Compose, Normalize, ToTensor\n",
    "import glob\n",
    "# import mercury\n",
    "# import RPi.GPIO as GPIO\n",
    "# import multiprocessing as mp\n",
    "# GPIO.setwarnings(False)\n",
    "# GPIO.setmode(GPIO.BCM)\n",
    "# GPIO.setup(13, GPIO.IN)\n",
    "# from myFunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(depth_img, background):\n",
    "    depth_img = background - depth_img #[:, int((848-640)/2):int(-(848-640)/2)]\n",
    "\n",
    "    otsu_thresh = skimage.filters.threshold_otsu(depth_img)\n",
    "    img_post = np.where(depth_img<otsu_thresh, depth_img, 0)\n",
    "\n",
    "    img_post = np.where(img_post>600, depth_img, 0)\n",
    "\n",
    "    img_mask = np.where(img_post != 0, 1, 0)\n",
    "    img_seg = skimage.measure.label(img_mask, background = 0, connectivity=2)\n",
    "    #assert( img_seg.max() != 0 )\n",
    "    if img_seg.max() != 0:\n",
    "        mask = img_seg == np.argmax(np.bincount(img_seg.flat)[1:])+1\n",
    "    else:\n",
    "        mask = np.zeros(np.shape(depth_img))\n",
    "\n",
    "    depth_img = np.where(mask != 0, depth_img, 0)\n",
    "    depth_img = (depth_img/np.max(depth_img)*255).astype('uint8')\n",
    "\n",
    "    return depth_img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['February_Data\\\\20220224_101029.bag', 'February_Data\\\\Background.bag', 'February_Data\\\\Background2.bag', 'February_Data\\\\Cow_10_0014.bag', 'February_Data\\\\Cow_11_0075.bag', 'February_Data\\\\Cow_12_6006.bag', 'February_Data\\\\Cow_13_8001.bag', 'February_Data\\\\Cow_14_1044.bag', 'February_Data\\\\Cow_1_8025.bag', 'February_Data\\\\Cow_2_6016.bag', 'February_Data\\\\Cow_3_6018.bag', 'February_Data\\\\Cow_4_8036.bag', 'February_Data\\\\Cow_5_8041.bag', 'February_Data\\\\Cow_6_5029.bag', 'February_Data\\\\Cow_7_6022.bag', 'February_Data\\\\Cow_8_2023.bag', 'February_Data\\\\Cow_9_NOTG.bag', 'February_Data\\\\shwelemia.bag', 'February_Data\\\\test.bag']\n",
      "(480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-45949cbaef30>:18: RuntimeWarning: invalid value encountered in true_divide\n",
      "  depth_img = (depth_img/np.max(depth_img)*255).astype('uint8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5b88a71c64ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;31m#import imageio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;31m#background_image = imageio.imread('masks\\\\feb_background.tif')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[0mcropped_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_preprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdepth_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackground_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m     \u001b[0mupper_pixels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcropped_depth\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0mlower_pixels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcropped_depth\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-45949cbaef30>\u001b[0m in \u001b[0;36mimage_preprocessing\u001b[1;34m(depth_img, background)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mimage_preprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdepth_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackground\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdepth_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackground\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdepth_img\u001b[0m \u001b[1;31m#[:, int((848-640)/2):int(-(848-640)/2)]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0motsu_thresh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mskimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold_otsu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdepth_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mimg_post\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdepth_img\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0motsu_thresh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fps = 30\n",
    "width = 848\n",
    "length = 480\n",
    "total_pixels = width*length\n",
    "\n",
    "transformer_name = \"microsoft/beit-base-patch16-224\"\n",
    "model = torch.load('VIT_models\\\\BEiT_90.pt')\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "feature_extractor = BeitFeatureExtractor.from_pretrained(transformer_name)\n",
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "\n",
    "filenames = glob.glob('February_Data\\\\*.bag')\n",
    "print(filenames)\n",
    "# Configure depth and color streams\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_device_from_file(filenames[-2]) #4 is a good example\n",
    "# align_to = rs.stream.color\n",
    "# align = rs.align(align_to)\n",
    "#config.enable_stream(rs.stream.depth, width, length, rs.format.z16, fps)\n",
    "#config.enable_stream(rs.stream.color, width, length, rs.format.bgr8, fps)\n",
    "\n",
    "pipe_profile = pipeline.start(config)\n",
    "\n",
    "#decimation = rs.decimation_filter()\n",
    "#decimation.set_option(rs.option.filter_magnitude, 2)\n",
    "\n",
    "spatial = rs.spatial_filter()\n",
    "spatial.set_option(rs.option.filter_magnitude, 5)\n",
    "spatial.set_option(rs.option.filter_smooth_alpha, 1)\n",
    "spatial.set_option(rs.option.filter_smooth_delta, 50)\n",
    "spatial.set_option(rs.option.holes_fill, 2)\n",
    "\n",
    "thresh = rs.threshold_filter(min_dist = 0.1, max_dist = 3.0)\n",
    "\n",
    "# depth_sensor = pipe_profile.get_device().first_depth_sensor()\n",
    "\n",
    "# depth_sensor.set_option(rs.option.enable_auto_exposure, True)\n",
    "\n",
    "#getting the background when the camera is first turned on\n",
    "frames = pipeline.wait_for_frames()\n",
    "#aligned_frames = align.process(frames)\n",
    "depth_frame = frames.get_depth_frame()\n",
    "color_frame = frames.get_color_frame()\n",
    "thresh_frame = thresh.process(depth_frame)\n",
    "filtered_depth = spatial.process(thresh_frame)\n",
    "background_image = np.asanyarray(filtered_depth.get_data())\n",
    "print(np.shape(background_image))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    #getting frames from the camera\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    #aligned_frames = align.process(frames)\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    thresh_frame = thresh.process(depth_frame)\n",
    "    filtered_depth = spatial.process(thresh_frame)\n",
    "    depth_image = np.asanyarray(filtered_depth.get_data())\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "    #import imageio\n",
    "    #background_image = imageio.imread('masks\\\\feb_background.tif')\n",
    "    cropped_depth, mask = image_preprocessing(depth_image, background_image)\n",
    "    upper_pixels = np.sum(np.where(cropped_depth[:100, :] != 0, 1, 0))\n",
    "    lower_pixels = np.sum(np.where(cropped_depth[length-20:length, :] != 0, 1, 0))\n",
    "    total_nonzero_pixels = np.sum(np.where(cropped_depth != 0, 1, 0))\n",
    "    \n",
    "    \n",
    "    display_img = np.zeros(np.shape(cropped_depth))\n",
    "    display_img = np.where(cropped_depth !=0, cropped_depth, 0)\n",
    "    cv2.line(display_img,(0,100),(848,100),(255,0,0),1)\n",
    "    cv2.line(display_img,(0, length-20),(848, length-20),(255,0,0),1)\n",
    "    \n",
    "    cv2.putText(display_img, 'Lower Pixel values: '+str(lower_pixels),(0,400), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(255),2,cv2.LINE_AA)\n",
    "    cv2.putText(display_img, 'Upper Pixel values: '+str(upper_pixels),(0,150), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(255),2,cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('cropped depth', display_img)\n",
    "    cv2.waitKey(1)\n",
    "    # print(lower_pixels, upper_pixels, total_nonzero_pixels)\n",
    "\n",
    "    # cropped_depth = (cropped_depth/np.max(cropped_depth)*255).astype('uint8')\n",
    "\n",
    "    # cropped_color = color_image\n",
    "    # cropped_color[:, :, 0] = np.where(mask==True, color_image[:, :, 0], 0)\n",
    "    # cropped_color[:, :, 1] = np.where(mask==True, color_image[:, :, 1], 0)\n",
    "    # cropped_color[:, :, 2] = np.where(mask==True, color_image[:, :, 2], 0)\n",
    "\n",
    "    # img_DGE = np.zeros(np.shape(color_image))\n",
    "    # img_DGE[:, :, 0] = cropped_depth\n",
    "    # img_DGE[:, :, 1] = skimage.color.rgb2gray(cropped_color)\n",
    "    # mask_dilation = skimage.morphology.binary_dilation(mask, skimage.morphology.ball(4, dtype=bool)[::, ::, 0], out=None)\n",
    "    # img_DGE[:, :, 2] = skimage.feature.canny(cropped_depth, sigma=1.0).astype('uint8')*255*mask_dilation\n",
    "    # # print(np.shape(img_DGE))\n",
    "    # transform = Compose([ToTensor(), Resize((224, 224)), normalize])\n",
    "    # img_DGE_norm = transform(img_DGE)\n",
    "\n",
    "    # outputs=model(img_DGE_norm[None, :].to('cpu').type('torch.FloatTensor'))\n",
    "    # prediction=np.argmax(outputs[0].cpu().detach().numpy(), axis=-1)\n",
    "\n",
    "    # cv2.putText(img_DGE, 'Thiccness Index: '+'thic'+'c'*int(prediction),(0,400), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0, 0, 255),2,cv2.LINE_AA)\n",
    "    # cv2.imshow('cropped color', img_DGE)\n",
    "    # cv2.waitKey(1)\n",
    "\n",
    "\n",
    "    if (lower_pixels < 400) and (upper_pixels > 15000) and (total_nonzero_pixels > 14000):\n",
    "\n",
    "        #cropped_depth = (cropped_depth/np.max(cropped_depth)*255).astype('uint8')\n",
    "\n",
    "        cropped_color = color_image\n",
    "        cropped_color[:, :, 0] = np.where(mask==True, color_image[:, :, 0], 0)\n",
    "        cropped_color[:, :, 1] = np.where(mask==True, color_image[:, :, 1], 0)\n",
    "        cropped_color[:, :, 2] = np.where(mask==True, color_image[:, :, 2], 0)\n",
    "\n",
    "        #Now convert depth/color images to DGE\n",
    "        img_DGE = np.zeros(np.shape(color_image))\n",
    "        img_DGE[:, :, 0] = cropped_depth\n",
    "        img_DGE[:, :, 1] = skimage.color.rgb2gray(cropped_color)\n",
    "        mask_dilation = skimage.morphology.binary_dilation(mask, skimage.morphology.ball(4, dtype=bool)[::, ::, 0], out=None)\n",
    "        img_DGE[:, :, 2] = skimage.feature.canny(cropped_depth, sigma=0.5).astype('uint8')*255*mask_dilation\n",
    "        # print(np.shape(img_DGE))\n",
    "        transform = Compose([ToTensor(), Resize((224, 224)), normalize])\n",
    "        img_DGE_norm = transform(img_DGE)\n",
    "\n",
    "        outputs=model(img_DGE_norm[None, :].to('cpu').type('torch.FloatTensor'))\n",
    "        prediction=np.argmax(outputs[0].cpu().detach().numpy(), axis=-1)\n",
    "        print(int(prediction))\n",
    "\n",
    "        cv2.putText(img_DGE, 'Thiccness Index: '+'thi'+'c'*int(prediction),(0,400), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0, 0, 255),2,cv2.LINE_AA)\n",
    "        cv2.imshow('cropped color', img_DGE)\n",
    "        cv2.waitKey(1)\n",
    "    \n",
    "\n",
    "    #cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ML_env_pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2107e4a676a391af164907120f93cd287ed2c419d0c1d2ead0dc1b99f4b7ffc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
